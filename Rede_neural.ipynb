{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "checkpoint_02_Dl_V2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Estevão Batista Sanchez RM 86649"
      ],
      "metadata": {
        "id": "pH4gCScVFpCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DnFle81bDHJC"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df =pd.read_csv('cardiovascular_diseases_dv3.csv',delimiter=';')"
      ],
      "metadata": {
        "id": "P_TRSrT1DMXU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "_ut_E8WbDgF7",
        "outputId": "72fd9cd8-5a77-4fec-964c-ed502ecfafd5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   AGE  GENDER  HEIGHT  WEIGHT  AP_HIGH  AP_LOW  CHOLESTEROL  GLUCOSE  SMOKE  \\\n",
              "0   50       2     168      62      110      80            1        1      0   \n",
              "1   55       1     156      85      140      90            3        1      0   \n",
              "2   52       1     165      64      130      70            3        1      0   \n",
              "3   48       2     169      82      150     100            1        1      0   \n",
              "4   48       1     156      56      100      60            1        1      0   \n",
              "\n",
              "   ALCOHOL  PHYSICAL_ACTIVITY  CARDIO_DISEASE  \n",
              "0        0                  1               0  \n",
              "1        0                  1               1  \n",
              "2        0                  0               1  \n",
              "3        0                  1               1  \n",
              "4        0                  0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cfa8404-4fb8-464a-a5b3-613b4080de37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AGE</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>HEIGHT</th>\n",
              "      <th>WEIGHT</th>\n",
              "      <th>AP_HIGH</th>\n",
              "      <th>AP_LOW</th>\n",
              "      <th>CHOLESTEROL</th>\n",
              "      <th>GLUCOSE</th>\n",
              "      <th>SMOKE</th>\n",
              "      <th>ALCOHOL</th>\n",
              "      <th>PHYSICAL_ACTIVITY</th>\n",
              "      <th>CARDIO_DISEASE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>2</td>\n",
              "      <td>168</td>\n",
              "      <td>62</td>\n",
              "      <td>110</td>\n",
              "      <td>80</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>156</td>\n",
              "      <td>85</td>\n",
              "      <td>140</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>165</td>\n",
              "      <td>64</td>\n",
              "      <td>130</td>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48</td>\n",
              "      <td>2</td>\n",
              "      <td>169</td>\n",
              "      <td>82</td>\n",
              "      <td>150</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>156</td>\n",
              "      <td>56</td>\n",
              "      <td>100</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cfa8404-4fb8-464a-a5b3-613b4080de37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0cfa8404-4fb8-464a-a5b3-613b4080de37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0cfa8404-4fb8-464a-a5b3-613b4080de37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tratamento dos dados, transformando colunas categóricas em numéricas e normalizando as colunas númericas"
      ],
      "metadata": {
        "id": "MFnkGAgZ-EF_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variavés dummy - similar a função one hot\n",
        "\n",
        "dummies_coleterol = pd.get_dummies(df['CHOLESTEROL'])\n",
        "df['CHOLESTEROL_1'] = dummies_coleterol[1]\n",
        "df['CHOLESTEROL_2'] = dummies_coleterol[2]\n",
        "df['CHOLESTEROL_3'] = dummies_coleterol[3]"
      ],
      "metadata": {
        "id": "GnG3PHFTlnx6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummies_gender = pd.get_dummies(df['GENDER'])\n",
        "df['GENDER_1'] = dummies_gender[1]\n",
        "df['GENDER_2'] = dummies_gender[2]"
      ],
      "metadata": {
        "id": "Z4IqVeULmh97"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummies_glucose = pd.get_dummies(df['GLUCOSE'])\n",
        "df['GLUCOSE_1'] = dummies_coleterol[1]\n",
        "df['GLUCOSE_2'] = dummies_coleterol[2]\n",
        "df['GLUCOSE_3'] = dummies_coleterol[3]"
      ],
      "metadata": {
        "id": "LKG8D-R6nCF0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummies_smoke = pd.get_dummies(df['SMOKE'])\n",
        "df['SMOKE_1'] = dummies_smoke[1]\n",
        "df['SMOKE_0'] = dummies_smoke[0]"
      ],
      "metadata": {
        "id": "dH0k4oSEnWjj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummies_alcool = pd.get_dummies(df['ALCOHOL'])\n",
        "df['ALCOHOL_1'] = dummies_alcool[1]\n",
        "df['ALCOHOL_0'] = dummies_alcool[0]"
      ],
      "metadata": {
        "id": "82f77dvKnhty"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummies_pa = pd.get_dummies(df['PHYSICAL_ACTIVITY'])\n",
        "df['PHYSICAL_ACTIVITY_1'] = dummies_alcool[1]\n",
        "df['PHYSICAL_ACTIVITY_0'] = dummies_alcool[0]"
      ],
      "metadata": {
        "id": "4PRrzY-insXK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "kTu6lqSymTmb",
        "outputId": "b6bf4a86-f747-4890-c283-35072607e30d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   AGE  GENDER  HEIGHT  WEIGHT  AP_HIGH  AP_LOW  CHOLESTEROL  GLUCOSE  SMOKE  \\\n",
              "0   50       2     168      62      110      80            1        1      0   \n",
              "1   55       1     156      85      140      90            3        1      0   \n",
              "2   52       1     165      64      130      70            3        1      0   \n",
              "3   48       2     169      82      150     100            1        1      0   \n",
              "4   48       1     156      56      100      60            1        1      0   \n",
              "\n",
              "   ALCOHOL  ...  GENDER_2  GLUCOSE_1  GLUCOSE_2  GLUCOSE_3  SMOKE_1  SMOKE_0  \\\n",
              "0        0  ...         1          1          0          0        0        1   \n",
              "1        0  ...         0          0          0          1        0        1   \n",
              "2        0  ...         0          0          0          1        0        1   \n",
              "3        0  ...         1          1          0          0        0        1   \n",
              "4        0  ...         0          1          0          0        0        1   \n",
              "\n",
              "   ALCOHOL_1  ALCOHOL_0  PHYSICAL_ACTIVITY_1  PHYSICAL_ACTIVITY_0  \n",
              "0          0          1                    0                    1  \n",
              "1          0          1                    0                    1  \n",
              "2          0          1                    0                    1  \n",
              "3          0          1                    0                    1  \n",
              "4          0          1                    0                    1  \n",
              "\n",
              "[5 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d51f406f-4385-484a-b274-696b2156284e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AGE</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>HEIGHT</th>\n",
              "      <th>WEIGHT</th>\n",
              "      <th>AP_HIGH</th>\n",
              "      <th>AP_LOW</th>\n",
              "      <th>CHOLESTEROL</th>\n",
              "      <th>GLUCOSE</th>\n",
              "      <th>SMOKE</th>\n",
              "      <th>ALCOHOL</th>\n",
              "      <th>...</th>\n",
              "      <th>GENDER_2</th>\n",
              "      <th>GLUCOSE_1</th>\n",
              "      <th>GLUCOSE_2</th>\n",
              "      <th>GLUCOSE_3</th>\n",
              "      <th>SMOKE_1</th>\n",
              "      <th>SMOKE_0</th>\n",
              "      <th>ALCOHOL_1</th>\n",
              "      <th>ALCOHOL_0</th>\n",
              "      <th>PHYSICAL_ACTIVITY_1</th>\n",
              "      <th>PHYSICAL_ACTIVITY_0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>2</td>\n",
              "      <td>168</td>\n",
              "      <td>62</td>\n",
              "      <td>110</td>\n",
              "      <td>80</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>156</td>\n",
              "      <td>85</td>\n",
              "      <td>140</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>165</td>\n",
              "      <td>64</td>\n",
              "      <td>130</td>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48</td>\n",
              "      <td>2</td>\n",
              "      <td>169</td>\n",
              "      <td>82</td>\n",
              "      <td>150</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>156</td>\n",
              "      <td>56</td>\n",
              "      <td>100</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d51f406f-4385-484a-b274-696b2156284e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d51f406f-4385-484a-b274-696b2156284e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d51f406f-4385-484a-b274-696b2156284e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para normalizar (valores entre -1 e 1)\n",
        "\n",
        "def normaliza_media(valores):\n",
        "  media = np.mean(valores)\n",
        "  minimo = np.min(valores)\n",
        "  maximo = np.max(valores)\n",
        "  return (valores - media)/(maximo-minimo)"
      ],
      "metadata": {
        "id": "u5TIGldyIi1H"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando a função e normalizando as colunas selecionadas\n",
        "\n",
        "df.AGE = normaliza_media(df.AGE)\n",
        "df.AP_LOW = normaliza_media(df.AP_LOW)\n",
        "df.AP_HIGH = normaliza_media(df.AP_HIGH)\n",
        "df.WEIGHT = normaliza_media(df.WEIGHT)\n",
        "df.HEIGHT = normaliza_media(df.HEIGHT)\n"
      ],
      "metadata": {
        "id": "wGfdbCtZI6w_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "BRXrq9bjJV3n",
        "outputId": "2a52e914-0406-465e-dcd4-cc4f4663008c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        AGE  GENDER    HEIGHT    WEIGHT   AP_HIGH    AP_LOW  CHOLESTEROL  \\\n",
              "0 -0.095051       2  0.018660 -0.064137 -0.092303 -0.009211            1   \n",
              "1  0.047806       1 -0.042878  0.057556  0.074363  0.057456            3   \n",
              "2 -0.037908       1  0.003276 -0.053555  0.018808 -0.075877            3   \n",
              "3 -0.152194       2  0.023788  0.041683  0.129919  0.124123            1   \n",
              "4 -0.152194       1 -0.042878 -0.095883 -0.147859 -0.142544            1   \n",
              "\n",
              "   GLUCOSE  SMOKE  ALCOHOL  ...  GENDER_2  GLUCOSE_1  GLUCOSE_2  GLUCOSE_3  \\\n",
              "0        1      0        0  ...         1          1          0          0   \n",
              "1        1      0        0  ...         0          0          0          1   \n",
              "2        1      0        0  ...         0          0          0          1   \n",
              "3        1      0        0  ...         1          1          0          0   \n",
              "4        1      0        0  ...         0          1          0          0   \n",
              "\n",
              "   SMOKE_1  SMOKE_0  ALCOHOL_1  ALCOHOL_0  PHYSICAL_ACTIVITY_1  \\\n",
              "0        0        1          0          1                    0   \n",
              "1        0        1          0          1                    0   \n",
              "2        0        1          0          1                    0   \n",
              "3        0        1          0          1                    0   \n",
              "4        0        1          0          1                    0   \n",
              "\n",
              "   PHYSICAL_ACTIVITY_0  \n",
              "0                    1  \n",
              "1                    1  \n",
              "2                    1  \n",
              "3                    1  \n",
              "4                    1  \n",
              "\n",
              "[5 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2857de2-3d38-4f4f-9189-824b9e5d9b69\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AGE</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>HEIGHT</th>\n",
              "      <th>WEIGHT</th>\n",
              "      <th>AP_HIGH</th>\n",
              "      <th>AP_LOW</th>\n",
              "      <th>CHOLESTEROL</th>\n",
              "      <th>GLUCOSE</th>\n",
              "      <th>SMOKE</th>\n",
              "      <th>ALCOHOL</th>\n",
              "      <th>...</th>\n",
              "      <th>GENDER_2</th>\n",
              "      <th>GLUCOSE_1</th>\n",
              "      <th>GLUCOSE_2</th>\n",
              "      <th>GLUCOSE_3</th>\n",
              "      <th>SMOKE_1</th>\n",
              "      <th>SMOKE_0</th>\n",
              "      <th>ALCOHOL_1</th>\n",
              "      <th>ALCOHOL_0</th>\n",
              "      <th>PHYSICAL_ACTIVITY_1</th>\n",
              "      <th>PHYSICAL_ACTIVITY_0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.095051</td>\n",
              "      <td>2</td>\n",
              "      <td>0.018660</td>\n",
              "      <td>-0.064137</td>\n",
              "      <td>-0.092303</td>\n",
              "      <td>-0.009211</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.047806</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.042878</td>\n",
              "      <td>0.057556</td>\n",
              "      <td>0.074363</td>\n",
              "      <td>0.057456</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.037908</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003276</td>\n",
              "      <td>-0.053555</td>\n",
              "      <td>0.018808</td>\n",
              "      <td>-0.075877</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.152194</td>\n",
              "      <td>2</td>\n",
              "      <td>0.023788</td>\n",
              "      <td>0.041683</td>\n",
              "      <td>0.129919</td>\n",
              "      <td>0.124123</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.152194</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.042878</td>\n",
              "      <td>-0.095883</td>\n",
              "      <td>-0.147859</td>\n",
              "      <td>-0.142544</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2857de2-3d38-4f4f-9189-824b9e5d9b69')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2857de2-3d38-4f4f-9189-824b9e5d9b69 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2857de2-3d38-4f4f-9189-824b9e5d9b69');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determinando a coluna de classificação\n",
        "\n",
        "saida = df['CARDIO_DISEASE']"
      ],
      "metadata": {
        "id": "HjuTrmtBEOCI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionando as colunas de entrada\n",
        "\n",
        "entrada = df[['AGE','GENDER_1','GENDER_2','HEIGHT','WEIGHT','AP_HIGH','AP_LOW','CHOLESTEROL_1','CHOLESTEROL_2','CHOLESTEROL_3','GLUCOSE_1','GLUCOSE_2','GLUCOSE_3','SMOKE_1','SMOKE_0','ALCOHOL_1','ALCOHOL_0','PHYSICAL_ACTIVITY_1','PHYSICAL_ACTIVITY_0']]"
      ],
      "metadata": {
        "id": "iFOYT4ALE1V-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando o número de neorônios da camada oculta\n",
        "# Entradas = 11\n",
        "# saidas = 1\n",
        "\n",
        "nco_1 = (19+1)/2\n",
        "nco_2 = (2*19)/3 + 1\n",
        "nco_3 = 2*19\n",
        "\n",
        "print(nco_1,nco_2,nco_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAx-zY7IQczx",
        "outputId": "74e8e44c-3785-4401-ef39-9cc126a6f801"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0 13.666666666666666 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separando a amostra de treino e teste - 80% para treino e 20% para teste\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(entrada, saida, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "XUXgsqDbM1zz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determinando os parametros dos modelos de rede neural\n",
        "\n",
        "# learning_rate_init - taxa de aprendizado inicial usada. Ele controla o tamanho do passo na atualização dos pesos\n",
        "# max_iter - Número máximo de iterações\n",
        "# activation - Função utilizada  - Linear\n",
        "# tol - Tolerância para a otimização\n",
        "# hidden_layer_sizes - Número de camdas ocultas e neuronios\n",
        "\n",
        "redeneural_1 = MLPClassifier(verbose=True,\n",
        "                           max_iter=10000,\n",
        "                           hidden_layer_sizes=(30,30,30),\n",
        "                           tol=0.000001,\n",
        "                           activation='logistic',\n",
        "                           learning_rate_init = 0.0001)\n"
      ],
      "metadata": {
        "id": "uU5UTQEiFP7N"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoide\n",
        "\n",
        "redeneural_2 = MLPClassifier(verbose=True,\n",
        "                           max_iter=10000,\n",
        "                           hidden_layer_sizes=(30,30,30),\n",
        "                           tol=0.000001,\n",
        "                           activation='identity',\n",
        "                           learning_rate_init = 0.0001)"
      ],
      "metadata": {
        "id": "SRlWSyGiqvV1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperbolica\n",
        "\n",
        "redeneural_3 = MLPClassifier(verbose=True,\n",
        "                           max_iter=10000,\n",
        "                           hidden_layer_sizes=(30,30,30),\n",
        "                           tol=0.000001,\n",
        "                           activation='tanh',\n",
        "                           learning_rate_init = 0.0001)"
      ],
      "metadata": {
        "id": "RaqFpA0LrBMT"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear retificada\n",
        "\n",
        "redeneural_4 = MLPClassifier(verbose=True,\n",
        "                           max_iter=10000,\n",
        "                           hidden_layer_sizes=(30,30,30),\n",
        "                           tol=0.000001,\n",
        "                           activation='relu',\n",
        "                           learning_rate_init = 0.0001)"
      ],
      "metadata": {
        "id": "CkKKPJTfrGGT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando os modelos"
      ],
      "metadata": {
        "id": "0L1WbNi6FJiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "redeneural_1.fit(X_train,Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XelpoLtyEG2T",
        "outputId": "ce7baf00-7d2c-48f7-c7b5-986b1aeb72f9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.75464313\n",
            "Iteration 2, loss = 0.69676243\n",
            "Iteration 3, loss = 0.69255737\n",
            "Iteration 4, loss = 0.69210893\n",
            "Iteration 5, loss = 0.69160310\n",
            "Iteration 6, loss = 0.69093579\n",
            "Iteration 7, loss = 0.68996921\n",
            "Iteration 8, loss = 0.68857698\n",
            "Iteration 9, loss = 0.68647969\n",
            "Iteration 10, loss = 0.68332153\n",
            "Iteration 11, loss = 0.67877956\n",
            "Iteration 12, loss = 0.67262399\n",
            "Iteration 13, loss = 0.66551021\n",
            "Iteration 14, loss = 0.65849296\n",
            "Iteration 15, loss = 0.65216707\n",
            "Iteration 16, loss = 0.64635541\n",
            "Iteration 17, loss = 0.64062734\n",
            "Iteration 18, loss = 0.63497653\n",
            "Iteration 19, loss = 0.62937152\n",
            "Iteration 20, loss = 0.62358326\n",
            "Iteration 21, loss = 0.61795481\n",
            "Iteration 22, loss = 0.61252404\n",
            "Iteration 23, loss = 0.60738420\n",
            "Iteration 24, loss = 0.60283189\n",
            "Iteration 25, loss = 0.59878495\n",
            "Iteration 26, loss = 0.59523891\n",
            "Iteration 27, loss = 0.59219908\n",
            "Iteration 28, loss = 0.58949241\n",
            "Iteration 29, loss = 0.58697886\n",
            "Iteration 30, loss = 0.58478611\n",
            "Iteration 31, loss = 0.58267909\n",
            "Iteration 32, loss = 0.58065215\n",
            "Iteration 33, loss = 0.57884965\n",
            "Iteration 34, loss = 0.57693947\n",
            "Iteration 35, loss = 0.57522806\n",
            "Iteration 36, loss = 0.57355527\n",
            "Iteration 37, loss = 0.57211021\n",
            "Iteration 38, loss = 0.57073870\n",
            "Iteration 39, loss = 0.56955144\n",
            "Iteration 40, loss = 0.56851619\n",
            "Iteration 41, loss = 0.56751112\n",
            "Iteration 42, loss = 0.56676462\n",
            "Iteration 43, loss = 0.56598653\n",
            "Iteration 44, loss = 0.56545923\n",
            "Iteration 45, loss = 0.56485580\n",
            "Iteration 46, loss = 0.56435716\n",
            "Iteration 47, loss = 0.56399951\n",
            "Iteration 48, loss = 0.56364415\n",
            "Iteration 49, loss = 0.56332770\n",
            "Iteration 50, loss = 0.56299166\n",
            "Iteration 51, loss = 0.56268425\n",
            "Iteration 52, loss = 0.56244622\n",
            "Iteration 53, loss = 0.56223596\n",
            "Iteration 54, loss = 0.56199602\n",
            "Iteration 55, loss = 0.56175515\n",
            "Iteration 56, loss = 0.56158236\n",
            "Iteration 57, loss = 0.56138395\n",
            "Iteration 58, loss = 0.56129826\n",
            "Iteration 59, loss = 0.56108937\n",
            "Iteration 60, loss = 0.56096392\n",
            "Iteration 61, loss = 0.56081173\n",
            "Iteration 62, loss = 0.56069276\n",
            "Iteration 63, loss = 0.56058562\n",
            "Iteration 64, loss = 0.56048661\n",
            "Iteration 65, loss = 0.56030498\n",
            "Iteration 66, loss = 0.56028252\n",
            "Iteration 67, loss = 0.56019211\n",
            "Iteration 68, loss = 0.56010285\n",
            "Iteration 69, loss = 0.55996285\n",
            "Iteration 70, loss = 0.55990969\n",
            "Iteration 71, loss = 0.55984678\n",
            "Iteration 72, loss = 0.55978400\n",
            "Iteration 73, loss = 0.55969675\n",
            "Iteration 74, loss = 0.55963427\n",
            "Iteration 75, loss = 0.55951101\n",
            "Iteration 76, loss = 0.55945599\n",
            "Iteration 77, loss = 0.55938608\n",
            "Iteration 78, loss = 0.55936102\n",
            "Iteration 79, loss = 0.55929772\n",
            "Iteration 80, loss = 0.55927187\n",
            "Iteration 81, loss = 0.55917442\n",
            "Iteration 82, loss = 0.55917127\n",
            "Iteration 83, loss = 0.55911630\n",
            "Iteration 84, loss = 0.55902921\n",
            "Iteration 85, loss = 0.55902299\n",
            "Iteration 86, loss = 0.55902593\n",
            "Iteration 87, loss = 0.55889705\n",
            "Iteration 88, loss = 0.55886435\n",
            "Iteration 89, loss = 0.55883102\n",
            "Iteration 90, loss = 0.55880865\n",
            "Iteration 91, loss = 0.55881128\n",
            "Iteration 92, loss = 0.55873556\n",
            "Iteration 93, loss = 0.55872694\n",
            "Iteration 94, loss = 0.55865088\n",
            "Iteration 95, loss = 0.55870907\n",
            "Iteration 96, loss = 0.55862348\n",
            "Iteration 97, loss = 0.55858952\n",
            "Iteration 98, loss = 0.55851314\n",
            "Iteration 99, loss = 0.55851593\n",
            "Iteration 100, loss = 0.55850261\n",
            "Iteration 101, loss = 0.55849276\n",
            "Iteration 102, loss = 0.55847218\n",
            "Iteration 103, loss = 0.55840108\n",
            "Iteration 104, loss = 0.55842293\n",
            "Iteration 105, loss = 0.55838635\n",
            "Iteration 106, loss = 0.55831826\n",
            "Iteration 107, loss = 0.55832689\n",
            "Iteration 108, loss = 0.55826406\n",
            "Iteration 109, loss = 0.55827019\n",
            "Iteration 110, loss = 0.55822410\n",
            "Iteration 111, loss = 0.55825067\n",
            "Iteration 112, loss = 0.55819827\n",
            "Iteration 113, loss = 0.55818751\n",
            "Iteration 114, loss = 0.55823693\n",
            "Iteration 115, loss = 0.55818696\n",
            "Iteration 116, loss = 0.55816525\n",
            "Iteration 117, loss = 0.55806604\n",
            "Iteration 118, loss = 0.55813032\n",
            "Iteration 119, loss = 0.55805901\n",
            "Iteration 120, loss = 0.55804801\n",
            "Iteration 121, loss = 0.55811057\n",
            "Iteration 122, loss = 0.55803317\n",
            "Iteration 123, loss = 0.55802933\n",
            "Iteration 124, loss = 0.55805605\n",
            "Iteration 125, loss = 0.55792226\n",
            "Iteration 126, loss = 0.55791267\n",
            "Iteration 127, loss = 0.55805507\n",
            "Iteration 128, loss = 0.55791375\n",
            "Iteration 129, loss = 0.55788661\n",
            "Iteration 130, loss = 0.55792057\n",
            "Iteration 131, loss = 0.55787819\n",
            "Iteration 132, loss = 0.55789049\n",
            "Iteration 133, loss = 0.55785396\n",
            "Iteration 134, loss = 0.55787897\n",
            "Iteration 135, loss = 0.55782760\n",
            "Iteration 136, loss = 0.55780514\n",
            "Iteration 137, loss = 0.55780818\n",
            "Iteration 138, loss = 0.55783233\n",
            "Iteration 139, loss = 0.55774528\n",
            "Iteration 140, loss = 0.55779099\n",
            "Iteration 141, loss = 0.55775275\n",
            "Iteration 142, loss = 0.55774281\n",
            "Iteration 143, loss = 0.55768813\n",
            "Iteration 144, loss = 0.55770903\n",
            "Iteration 145, loss = 0.55773680\n",
            "Iteration 146, loss = 0.55765597\n",
            "Iteration 147, loss = 0.55763329\n",
            "Iteration 148, loss = 0.55761504\n",
            "Iteration 149, loss = 0.55759630\n",
            "Iteration 150, loss = 0.55755945\n",
            "Iteration 151, loss = 0.55764623\n",
            "Iteration 152, loss = 0.55762318\n",
            "Iteration 153, loss = 0.55754389\n",
            "Iteration 154, loss = 0.55755788\n",
            "Iteration 155, loss = 0.55765623\n",
            "Iteration 156, loss = 0.55755898\n",
            "Iteration 157, loss = 0.55751547\n",
            "Iteration 158, loss = 0.55753161\n",
            "Iteration 159, loss = 0.55747844\n",
            "Iteration 160, loss = 0.55752919\n",
            "Iteration 161, loss = 0.55753374\n",
            "Iteration 162, loss = 0.55745756\n",
            "Iteration 163, loss = 0.55742612\n",
            "Iteration 164, loss = 0.55746353\n",
            "Iteration 165, loss = 0.55743234\n",
            "Iteration 166, loss = 0.55741260\n",
            "Iteration 167, loss = 0.55743069\n",
            "Iteration 168, loss = 0.55736076\n",
            "Iteration 169, loss = 0.55739232\n",
            "Iteration 170, loss = 0.55737746\n",
            "Iteration 171, loss = 0.55735605\n",
            "Iteration 172, loss = 0.55734341\n",
            "Iteration 173, loss = 0.55735506\n",
            "Iteration 174, loss = 0.55737195\n",
            "Iteration 175, loss = 0.55734992\n",
            "Iteration 176, loss = 0.55730526\n",
            "Iteration 177, loss = 0.55735146\n",
            "Iteration 178, loss = 0.55729722\n",
            "Iteration 179, loss = 0.55734718\n",
            "Iteration 180, loss = 0.55730929\n",
            "Iteration 181, loss = 0.55713620\n",
            "Iteration 182, loss = 0.55719405\n",
            "Iteration 183, loss = 0.55722812\n",
            "Iteration 184, loss = 0.55724130\n",
            "Iteration 185, loss = 0.55719505\n",
            "Iteration 186, loss = 0.55717975\n",
            "Iteration 187, loss = 0.55716136\n",
            "Iteration 188, loss = 0.55718052\n",
            "Iteration 189, loss = 0.55718452\n",
            "Iteration 190, loss = 0.55713835\n",
            "Iteration 191, loss = 0.55715194\n",
            "Iteration 192, loss = 0.55715620\n",
            "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', hidden_layer_sizes=(30, 30, 30),\n",
              "              learning_rate_init=0.0001, max_iter=10000, tol=1e-06,\n",
              "              verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "redeneural_2.fit(X_train,Y_train)\n",
        "redeneural_3.fit(X_train,Y_train)\n",
        "redeneural_4.fit(X_train,Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaIraYHPFb5X",
        "outputId": "c704a1c5-a149-4f36-bc2e-fe3d7343891d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.65630927\n",
            "Iteration 2, loss = 0.62178685\n",
            "Iteration 3, loss = 0.59735175\n",
            "Iteration 4, loss = 0.58057642\n",
            "Iteration 5, loss = 0.57286901\n",
            "Iteration 6, loss = 0.56939936\n",
            "Iteration 7, loss = 0.56780358\n",
            "Iteration 8, loss = 0.56704153\n",
            "Iteration 9, loss = 0.56661395\n",
            "Iteration 10, loss = 0.56606519\n",
            "Iteration 11, loss = 0.56566214\n",
            "Iteration 12, loss = 0.56560795\n",
            "Iteration 13, loss = 0.56552165\n",
            "Iteration 14, loss = 0.56544419\n",
            "Iteration 15, loss = 0.56497703\n",
            "Iteration 16, loss = 0.56504910\n",
            "Iteration 17, loss = 0.56489768\n",
            "Iteration 18, loss = 0.56478110\n",
            "Iteration 19, loss = 0.56466627\n",
            "Iteration 20, loss = 0.56466190\n",
            "Iteration 21, loss = 0.56460789\n",
            "Iteration 22, loss = 0.56460382\n",
            "Iteration 23, loss = 0.56465079\n",
            "Iteration 24, loss = 0.56452338\n",
            "Iteration 25, loss = 0.56455991\n",
            "Iteration 26, loss = 0.56458446\n",
            "Iteration 27, loss = 0.56454464\n",
            "Iteration 28, loss = 0.56440660\n",
            "Iteration 29, loss = 0.56436812\n",
            "Iteration 30, loss = 0.56446207\n",
            "Iteration 31, loss = 0.56447216\n",
            "Iteration 32, loss = 0.56440993\n",
            "Iteration 33, loss = 0.56452116\n",
            "Iteration 34, loss = 0.56424054\n",
            "Iteration 35, loss = 0.56452668\n",
            "Iteration 36, loss = 0.56439933\n",
            "Iteration 37, loss = 0.56446498\n",
            "Iteration 38, loss = 0.56448129\n",
            "Iteration 39, loss = 0.56453032\n",
            "Iteration 40, loss = 0.56434681\n",
            "Iteration 41, loss = 0.56458729\n",
            "Iteration 42, loss = 0.56447079\n",
            "Iteration 43, loss = 0.56459335\n",
            "Iteration 44, loss = 0.56446908\n",
            "Iteration 45, loss = 0.56429507\n",
            "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.66548613\n",
            "Iteration 2, loss = 0.63228576\n",
            "Iteration 3, loss = 0.60595876\n",
            "Iteration 4, loss = 0.58614994\n",
            "Iteration 5, loss = 0.57405145\n",
            "Iteration 6, loss = 0.56760930\n",
            "Iteration 7, loss = 0.56474480\n",
            "Iteration 8, loss = 0.56345882\n",
            "Iteration 9, loss = 0.56260196\n",
            "Iteration 10, loss = 0.56190872\n",
            "Iteration 11, loss = 0.56166150\n",
            "Iteration 12, loss = 0.56094058\n",
            "Iteration 13, loss = 0.56076780\n",
            "Iteration 14, loss = 0.56030060\n",
            "Iteration 15, loss = 0.55977000\n",
            "Iteration 16, loss = 0.55935484\n",
            "Iteration 17, loss = 0.55915069\n",
            "Iteration 18, loss = 0.55890778\n",
            "Iteration 19, loss = 0.55851596\n",
            "Iteration 20, loss = 0.55845404\n",
            "Iteration 21, loss = 0.55814773\n",
            "Iteration 22, loss = 0.55789996\n",
            "Iteration 23, loss = 0.55789452\n",
            "Iteration 24, loss = 0.55764771\n",
            "Iteration 25, loss = 0.55746175\n",
            "Iteration 26, loss = 0.55747282\n",
            "Iteration 27, loss = 0.55715770\n",
            "Iteration 28, loss = 0.55711726\n",
            "Iteration 29, loss = 0.55690538\n",
            "Iteration 30, loss = 0.55684173\n",
            "Iteration 31, loss = 0.55668608\n",
            "Iteration 32, loss = 0.55667000\n",
            "Iteration 33, loss = 0.55645686\n",
            "Iteration 34, loss = 0.55644613\n",
            "Iteration 35, loss = 0.55624237\n",
            "Iteration 36, loss = 0.55619107\n",
            "Iteration 37, loss = 0.55619079\n",
            "Iteration 38, loss = 0.55595587\n",
            "Iteration 39, loss = 0.55591853\n",
            "Iteration 40, loss = 0.55573255\n",
            "Iteration 41, loss = 0.55570636\n",
            "Iteration 42, loss = 0.55561593\n",
            "Iteration 43, loss = 0.55539335\n",
            "Iteration 44, loss = 0.55536608\n",
            "Iteration 45, loss = 0.55520838\n",
            "Iteration 46, loss = 0.55526043\n",
            "Iteration 47, loss = 0.55503028\n",
            "Iteration 48, loss = 0.55500226\n",
            "Iteration 49, loss = 0.55487206\n",
            "Iteration 50, loss = 0.55476040\n",
            "Iteration 51, loss = 0.55472995\n",
            "Iteration 52, loss = 0.55460904\n",
            "Iteration 53, loss = 0.55445100\n",
            "Iteration 54, loss = 0.55432081\n",
            "Iteration 55, loss = 0.55423355\n",
            "Iteration 56, loss = 0.55415114\n",
            "Iteration 57, loss = 0.55406440\n",
            "Iteration 58, loss = 0.55393454\n",
            "Iteration 59, loss = 0.55381964\n",
            "Iteration 60, loss = 0.55371075\n",
            "Iteration 61, loss = 0.55362566\n",
            "Iteration 62, loss = 0.55338807\n",
            "Iteration 63, loss = 0.55341036\n",
            "Iteration 64, loss = 0.55314290\n",
            "Iteration 65, loss = 0.55310222\n",
            "Iteration 66, loss = 0.55300313\n",
            "Iteration 67, loss = 0.55287413\n",
            "Iteration 68, loss = 0.55263918\n",
            "Iteration 69, loss = 0.55253575\n",
            "Iteration 70, loss = 0.55249541\n",
            "Iteration 71, loss = 0.55238797\n",
            "Iteration 72, loss = 0.55223316\n",
            "Iteration 73, loss = 0.55214014\n",
            "Iteration 74, loss = 0.55187475\n",
            "Iteration 75, loss = 0.55182172\n",
            "Iteration 76, loss = 0.55170541\n",
            "Iteration 77, loss = 0.55153234\n",
            "Iteration 78, loss = 0.55133375\n",
            "Iteration 79, loss = 0.55118540\n",
            "Iteration 80, loss = 0.55101762\n",
            "Iteration 81, loss = 0.55082390\n",
            "Iteration 82, loss = 0.55073157\n",
            "Iteration 83, loss = 0.55055134\n",
            "Iteration 84, loss = 0.55027865\n",
            "Iteration 85, loss = 0.55010476\n",
            "Iteration 86, loss = 0.55002104\n",
            "Iteration 87, loss = 0.54979270\n",
            "Iteration 88, loss = 0.54965313\n",
            "Iteration 89, loss = 0.54944183\n",
            "Iteration 90, loss = 0.54924284\n",
            "Iteration 91, loss = 0.54916746\n",
            "Iteration 92, loss = 0.54889473\n",
            "Iteration 93, loss = 0.54872203\n",
            "Iteration 94, loss = 0.54859039\n",
            "Iteration 95, loss = 0.54839490\n",
            "Iteration 96, loss = 0.54827690\n",
            "Iteration 97, loss = 0.54803302\n",
            "Iteration 98, loss = 0.54791826\n",
            "Iteration 99, loss = 0.54786121\n",
            "Iteration 100, loss = 0.54770766\n",
            "Iteration 101, loss = 0.54748395\n",
            "Iteration 102, loss = 0.54745206\n",
            "Iteration 103, loss = 0.54742350\n",
            "Iteration 104, loss = 0.54719641\n",
            "Iteration 105, loss = 0.54703296\n",
            "Iteration 106, loss = 0.54706195\n",
            "Iteration 107, loss = 0.54696362\n",
            "Iteration 108, loss = 0.54697032\n",
            "Iteration 109, loss = 0.54687877\n",
            "Iteration 110, loss = 0.54674998\n",
            "Iteration 111, loss = 0.54664003\n",
            "Iteration 112, loss = 0.54664566\n",
            "Iteration 113, loss = 0.54658962\n",
            "Iteration 114, loss = 0.54645798\n",
            "Iteration 115, loss = 0.54647260\n",
            "Iteration 116, loss = 0.54635601\n",
            "Iteration 117, loss = 0.54635348\n",
            "Iteration 118, loss = 0.54638758\n",
            "Iteration 119, loss = 0.54639234\n",
            "Iteration 120, loss = 0.54621305\n",
            "Iteration 121, loss = 0.54620231\n",
            "Iteration 122, loss = 0.54619625\n",
            "Iteration 123, loss = 0.54611231\n",
            "Iteration 124, loss = 0.54606640\n",
            "Iteration 125, loss = 0.54610884\n",
            "Iteration 126, loss = 0.54604885\n",
            "Iteration 127, loss = 0.54601631\n",
            "Iteration 128, loss = 0.54600501\n",
            "Iteration 129, loss = 0.54600242\n",
            "Iteration 130, loss = 0.54593104\n",
            "Iteration 131, loss = 0.54593038\n",
            "Iteration 132, loss = 0.54583565\n",
            "Iteration 133, loss = 0.54597197\n",
            "Iteration 134, loss = 0.54581120\n",
            "Iteration 135, loss = 0.54582241\n",
            "Iteration 136, loss = 0.54577064\n",
            "Iteration 137, loss = 0.54571867\n",
            "Iteration 138, loss = 0.54586009\n",
            "Iteration 139, loss = 0.54578472\n",
            "Iteration 140, loss = 0.54569522\n",
            "Iteration 141, loss = 0.54568077\n",
            "Iteration 142, loss = 0.54569650\n",
            "Iteration 143, loss = 0.54567547\n",
            "Iteration 144, loss = 0.54565147\n",
            "Iteration 145, loss = 0.54573401\n",
            "Iteration 146, loss = 0.54560201\n",
            "Iteration 147, loss = 0.54561558\n",
            "Iteration 148, loss = 0.54555902\n",
            "Iteration 149, loss = 0.54555689\n",
            "Iteration 150, loss = 0.54555023\n",
            "Iteration 151, loss = 0.54562912\n",
            "Iteration 152, loss = 0.54553287\n",
            "Iteration 153, loss = 0.54545404\n",
            "Iteration 154, loss = 0.54545234\n",
            "Iteration 155, loss = 0.54542386\n",
            "Iteration 156, loss = 0.54539195\n",
            "Iteration 157, loss = 0.54545201\n",
            "Iteration 158, loss = 0.54542515\n",
            "Iteration 159, loss = 0.54540792\n",
            "Iteration 160, loss = 0.54541230\n",
            "Iteration 161, loss = 0.54543721\n",
            "Iteration 162, loss = 0.54541454\n",
            "Iteration 163, loss = 0.54541321\n",
            "Iteration 164, loss = 0.54540318\n",
            "Iteration 165, loss = 0.54527558\n",
            "Iteration 166, loss = 0.54529513\n",
            "Iteration 167, loss = 0.54536492\n",
            "Iteration 168, loss = 0.54541604\n",
            "Iteration 169, loss = 0.54537456\n",
            "Iteration 170, loss = 0.54532383\n",
            "Iteration 171, loss = 0.54528788\n",
            "Iteration 172, loss = 0.54530113\n",
            "Iteration 173, loss = 0.54533538\n",
            "Iteration 174, loss = 0.54528926\n",
            "Iteration 175, loss = 0.54530570\n",
            "Iteration 176, loss = 0.54527660\n",
            "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.70963527\n",
            "Iteration 2, loss = 0.67582490\n",
            "Iteration 3, loss = 0.64636266\n",
            "Iteration 4, loss = 0.61687005\n",
            "Iteration 5, loss = 0.59320085\n",
            "Iteration 6, loss = 0.57932239\n",
            "Iteration 7, loss = 0.57086492\n",
            "Iteration 8, loss = 0.56563368\n",
            "Iteration 9, loss = 0.56290441\n",
            "Iteration 10, loss = 0.56141175\n",
            "Iteration 11, loss = 0.56023505\n",
            "Iteration 12, loss = 0.55936387\n",
            "Iteration 13, loss = 0.55890706\n",
            "Iteration 14, loss = 0.55824516\n",
            "Iteration 15, loss = 0.55780362\n",
            "Iteration 16, loss = 0.55736954\n",
            "Iteration 17, loss = 0.55693515\n",
            "Iteration 18, loss = 0.55652888\n",
            "Iteration 19, loss = 0.55628376\n",
            "Iteration 20, loss = 0.55597726\n",
            "Iteration 21, loss = 0.55565949\n",
            "Iteration 22, loss = 0.55534057\n",
            "Iteration 23, loss = 0.55509326\n",
            "Iteration 24, loss = 0.55470578\n",
            "Iteration 25, loss = 0.55438644\n",
            "Iteration 26, loss = 0.55406346\n",
            "Iteration 27, loss = 0.55377008\n",
            "Iteration 28, loss = 0.55361513\n",
            "Iteration 29, loss = 0.55335028\n",
            "Iteration 30, loss = 0.55305898\n",
            "Iteration 31, loss = 0.55300653\n",
            "Iteration 32, loss = 0.55272746\n",
            "Iteration 33, loss = 0.55246291\n",
            "Iteration 34, loss = 0.55219376\n",
            "Iteration 35, loss = 0.55206668\n",
            "Iteration 36, loss = 0.55182136\n",
            "Iteration 37, loss = 0.55169925\n",
            "Iteration 38, loss = 0.55142521\n",
            "Iteration 39, loss = 0.55118921\n",
            "Iteration 40, loss = 0.55102874\n",
            "Iteration 41, loss = 0.55082461\n",
            "Iteration 42, loss = 0.55071283\n",
            "Iteration 43, loss = 0.55054978\n",
            "Iteration 44, loss = 0.55033138\n",
            "Iteration 45, loss = 0.55021207\n",
            "Iteration 46, loss = 0.55004580\n",
            "Iteration 47, loss = 0.55005191\n",
            "Iteration 48, loss = 0.54968900\n",
            "Iteration 49, loss = 0.54955978\n",
            "Iteration 50, loss = 0.54969226\n",
            "Iteration 51, loss = 0.54935830\n",
            "Iteration 52, loss = 0.54942793\n",
            "Iteration 53, loss = 0.54912665\n",
            "Iteration 54, loss = 0.54895012\n",
            "Iteration 55, loss = 0.54903473\n",
            "Iteration 56, loss = 0.54888036\n",
            "Iteration 57, loss = 0.54872763\n",
            "Iteration 58, loss = 0.54869602\n",
            "Iteration 59, loss = 0.54859665\n",
            "Iteration 60, loss = 0.54837648\n",
            "Iteration 61, loss = 0.54826623\n",
            "Iteration 62, loss = 0.54829061\n",
            "Iteration 63, loss = 0.54816889\n",
            "Iteration 64, loss = 0.54795563\n",
            "Iteration 65, loss = 0.54795500\n",
            "Iteration 66, loss = 0.54790475\n",
            "Iteration 67, loss = 0.54779202\n",
            "Iteration 68, loss = 0.54769996\n",
            "Iteration 69, loss = 0.54777541\n",
            "Iteration 70, loss = 0.54744919\n",
            "Iteration 71, loss = 0.54750152\n",
            "Iteration 72, loss = 0.54742129\n",
            "Iteration 73, loss = 0.54735619\n",
            "Iteration 74, loss = 0.54728338\n",
            "Iteration 75, loss = 0.54719171\n",
            "Iteration 76, loss = 0.54720325\n",
            "Iteration 77, loss = 0.54702694\n",
            "Iteration 78, loss = 0.54700439\n",
            "Iteration 79, loss = 0.54702451\n",
            "Iteration 80, loss = 0.54692107\n",
            "Iteration 81, loss = 0.54696394\n",
            "Iteration 82, loss = 0.54688062\n",
            "Iteration 83, loss = 0.54679179\n",
            "Iteration 84, loss = 0.54670263\n",
            "Iteration 85, loss = 0.54666900\n",
            "Iteration 86, loss = 0.54663429\n",
            "Iteration 87, loss = 0.54659781\n",
            "Iteration 88, loss = 0.54652124\n",
            "Iteration 89, loss = 0.54645008\n",
            "Iteration 90, loss = 0.54643314\n",
            "Iteration 91, loss = 0.54644977\n",
            "Iteration 92, loss = 0.54634289\n",
            "Iteration 93, loss = 0.54627595\n",
            "Iteration 94, loss = 0.54625653\n",
            "Iteration 95, loss = 0.54612238\n",
            "Iteration 96, loss = 0.54619937\n",
            "Iteration 97, loss = 0.54627800\n",
            "Iteration 98, loss = 0.54613275\n",
            "Iteration 99, loss = 0.54624988\n",
            "Iteration 100, loss = 0.54603179\n",
            "Iteration 101, loss = 0.54606051\n",
            "Iteration 102, loss = 0.54599546\n",
            "Iteration 103, loss = 0.54601066\n",
            "Iteration 104, loss = 0.54591213\n",
            "Iteration 105, loss = 0.54596376\n",
            "Iteration 106, loss = 0.54587321\n",
            "Iteration 107, loss = 0.54578283\n",
            "Iteration 108, loss = 0.54586239\n",
            "Iteration 109, loss = 0.54569165\n",
            "Iteration 110, loss = 0.54579230\n",
            "Iteration 111, loss = 0.54574385\n",
            "Iteration 112, loss = 0.54569585\n",
            "Iteration 113, loss = 0.54566839\n",
            "Iteration 114, loss = 0.54564703\n",
            "Iteration 115, loss = 0.54564782\n",
            "Iteration 116, loss = 0.54558444\n",
            "Iteration 117, loss = 0.54556069\n",
            "Iteration 118, loss = 0.54560328\n",
            "Iteration 119, loss = 0.54556644\n",
            "Iteration 120, loss = 0.54543856\n",
            "Iteration 121, loss = 0.54549635\n",
            "Iteration 122, loss = 0.54541521\n",
            "Iteration 123, loss = 0.54537389\n",
            "Iteration 124, loss = 0.54539709\n",
            "Iteration 125, loss = 0.54535516\n",
            "Iteration 126, loss = 0.54538776\n",
            "Iteration 127, loss = 0.54540654\n",
            "Iteration 128, loss = 0.54537514\n",
            "Iteration 129, loss = 0.54533157\n",
            "Iteration 130, loss = 0.54531189\n",
            "Iteration 131, loss = 0.54529384\n",
            "Iteration 132, loss = 0.54535191\n",
            "Iteration 133, loss = 0.54518328\n",
            "Iteration 134, loss = 0.54525449\n",
            "Iteration 135, loss = 0.54525588\n",
            "Iteration 136, loss = 0.54521351\n",
            "Iteration 137, loss = 0.54526564\n",
            "Iteration 138, loss = 0.54516114\n",
            "Iteration 139, loss = 0.54514170\n",
            "Iteration 140, loss = 0.54509163\n",
            "Iteration 141, loss = 0.54512817\n",
            "Iteration 142, loss = 0.54513798\n",
            "Iteration 143, loss = 0.54516306\n",
            "Iteration 144, loss = 0.54515005\n",
            "Iteration 145, loss = 0.54507954\n",
            "Iteration 146, loss = 0.54495982\n",
            "Iteration 147, loss = 0.54501239\n",
            "Iteration 148, loss = 0.54511042\n",
            "Iteration 149, loss = 0.54490076\n",
            "Iteration 150, loss = 0.54486534\n",
            "Iteration 151, loss = 0.54500154\n",
            "Iteration 152, loss = 0.54488600\n",
            "Iteration 153, loss = 0.54496331\n",
            "Iteration 154, loss = 0.54493456\n",
            "Iteration 155, loss = 0.54495452\n",
            "Iteration 156, loss = 0.54480866\n",
            "Iteration 157, loss = 0.54481501\n",
            "Iteration 158, loss = 0.54485138\n",
            "Iteration 159, loss = 0.54483028\n",
            "Iteration 160, loss = 0.54492665\n",
            "Iteration 161, loss = 0.54478866\n",
            "Iteration 162, loss = 0.54482745\n",
            "Iteration 163, loss = 0.54472514\n",
            "Iteration 164, loss = 0.54487428\n",
            "Iteration 165, loss = 0.54473387\n",
            "Iteration 166, loss = 0.54475397\n",
            "Iteration 167, loss = 0.54476015\n",
            "Iteration 168, loss = 0.54480394\n",
            "Iteration 169, loss = 0.54471889\n",
            "Iteration 170, loss = 0.54473778\n",
            "Iteration 171, loss = 0.54459560\n",
            "Iteration 172, loss = 0.54471368\n",
            "Iteration 173, loss = 0.54470570\n",
            "Iteration 174, loss = 0.54466697\n",
            "Iteration 175, loss = 0.54456804\n",
            "Iteration 176, loss = 0.54469902\n",
            "Iteration 177, loss = 0.54461782\n",
            "Iteration 178, loss = 0.54467357\n",
            "Iteration 179, loss = 0.54460700\n",
            "Iteration 180, loss = 0.54467937\n",
            "Iteration 181, loss = 0.54450949\n",
            "Iteration 182, loss = 0.54454212\n",
            "Iteration 183, loss = 0.54451644\n",
            "Iteration 184, loss = 0.54451988\n",
            "Iteration 185, loss = 0.54457221\n",
            "Iteration 186, loss = 0.54455174\n",
            "Iteration 187, loss = 0.54455017\n",
            "Iteration 188, loss = 0.54453173\n",
            "Iteration 189, loss = 0.54439005\n",
            "Iteration 190, loss = 0.54463197\n",
            "Iteration 191, loss = 0.54440453\n",
            "Iteration 192, loss = 0.54445531\n",
            "Iteration 193, loss = 0.54447907\n",
            "Iteration 194, loss = 0.54431075\n",
            "Iteration 195, loss = 0.54434736\n",
            "Iteration 196, loss = 0.54428866\n",
            "Iteration 197, loss = 0.54429170\n",
            "Iteration 198, loss = 0.54432804\n",
            "Iteration 199, loss = 0.54447946\n",
            "Iteration 200, loss = 0.54422914\n",
            "Iteration 201, loss = 0.54431149\n",
            "Iteration 202, loss = 0.54433009\n",
            "Iteration 203, loss = 0.54439698\n",
            "Iteration 204, loss = 0.54430719\n",
            "Iteration 205, loss = 0.54423625\n",
            "Iteration 206, loss = 0.54411943\n",
            "Iteration 207, loss = 0.54419860\n",
            "Iteration 208, loss = 0.54414094\n",
            "Iteration 209, loss = 0.54426395\n",
            "Iteration 210, loss = 0.54428012\n",
            "Iteration 211, loss = 0.54416811\n",
            "Iteration 212, loss = 0.54417101\n",
            "Iteration 213, loss = 0.54424570\n",
            "Iteration 214, loss = 0.54415555\n",
            "Iteration 215, loss = 0.54419241\n",
            "Iteration 216, loss = 0.54418557\n",
            "Iteration 217, loss = 0.54404900\n",
            "Iteration 218, loss = 0.54419923\n",
            "Iteration 219, loss = 0.54420485\n",
            "Iteration 220, loss = 0.54408799\n",
            "Iteration 221, loss = 0.54413180\n",
            "Iteration 222, loss = 0.54417530\n",
            "Iteration 223, loss = 0.54406992\n",
            "Iteration 224, loss = 0.54404012\n",
            "Iteration 225, loss = 0.54403241\n",
            "Iteration 226, loss = 0.54408444\n",
            "Iteration 227, loss = 0.54406655\n",
            "Iteration 228, loss = 0.54399855\n",
            "Iteration 229, loss = 0.54407306\n",
            "Iteration 230, loss = 0.54402120\n",
            "Iteration 231, loss = 0.54403182\n",
            "Iteration 232, loss = 0.54408716\n",
            "Iteration 233, loss = 0.54409121\n",
            "Iteration 234, loss = 0.54401780\n",
            "Iteration 235, loss = 0.54401155\n",
            "Iteration 236, loss = 0.54405118\n",
            "Iteration 237, loss = 0.54392413\n",
            "Iteration 238, loss = 0.54395092\n",
            "Iteration 239, loss = 0.54398060\n",
            "Iteration 240, loss = 0.54398827\n",
            "Iteration 241, loss = 0.54387209\n",
            "Iteration 242, loss = 0.54392365\n",
            "Iteration 243, loss = 0.54379486\n",
            "Iteration 244, loss = 0.54388397\n",
            "Iteration 245, loss = 0.54387932\n",
            "Iteration 246, loss = 0.54386110\n",
            "Iteration 247, loss = 0.54384553\n",
            "Iteration 248, loss = 0.54390225\n",
            "Iteration 249, loss = 0.54386710\n",
            "Iteration 250, loss = 0.54385950\n",
            "Iteration 251, loss = 0.54378120\n",
            "Iteration 252, loss = 0.54372597\n",
            "Iteration 253, loss = 0.54387417\n",
            "Iteration 254, loss = 0.54385727\n",
            "Iteration 255, loss = 0.54383345\n",
            "Iteration 256, loss = 0.54381412\n",
            "Iteration 257, loss = 0.54378234\n",
            "Iteration 258, loss = 0.54369023\n",
            "Iteration 259, loss = 0.54382438\n",
            "Iteration 260, loss = 0.54363995\n",
            "Iteration 261, loss = 0.54364413\n",
            "Iteration 262, loss = 0.54374623\n",
            "Iteration 263, loss = 0.54370229\n",
            "Iteration 264, loss = 0.54380186\n",
            "Iteration 265, loss = 0.54365947\n",
            "Iteration 266, loss = 0.54366428\n",
            "Iteration 267, loss = 0.54365711\n",
            "Iteration 268, loss = 0.54376868\n",
            "Iteration 269, loss = 0.54363063\n",
            "Iteration 270, loss = 0.54364899\n",
            "Iteration 271, loss = 0.54368996\n",
            "Iteration 272, loss = 0.54379565\n",
            "Iteration 273, loss = 0.54367502\n",
            "Iteration 274, loss = 0.54365233\n",
            "Iteration 275, loss = 0.54366779\n",
            "Iteration 276, loss = 0.54359100\n",
            "Iteration 277, loss = 0.54359138\n",
            "Iteration 278, loss = 0.54363328\n",
            "Iteration 279, loss = 0.54359263\n",
            "Iteration 280, loss = 0.54355963\n",
            "Iteration 281, loss = 0.54364427\n",
            "Iteration 282, loss = 0.54358705\n",
            "Iteration 283, loss = 0.54357589\n",
            "Iteration 284, loss = 0.54358062\n",
            "Iteration 285, loss = 0.54356159\n",
            "Iteration 286, loss = 0.54349871\n",
            "Iteration 287, loss = 0.54353259\n",
            "Iteration 288, loss = 0.54358865\n",
            "Iteration 289, loss = 0.54354544\n",
            "Iteration 290, loss = 0.54360723\n",
            "Iteration 291, loss = 0.54346436\n",
            "Iteration 292, loss = 0.54348533\n",
            "Iteration 293, loss = 0.54355796\n",
            "Iteration 294, loss = 0.54353670\n",
            "Iteration 295, loss = 0.54346222\n",
            "Iteration 296, loss = 0.54359640\n",
            "Iteration 297, loss = 0.54331082\n",
            "Iteration 298, loss = 0.54357166\n",
            "Iteration 299, loss = 0.54350913\n",
            "Iteration 300, loss = 0.54353041\n",
            "Iteration 301, loss = 0.54334817\n",
            "Iteration 302, loss = 0.54341118\n",
            "Iteration 303, loss = 0.54342505\n",
            "Iteration 304, loss = 0.54337737\n",
            "Iteration 305, loss = 0.54340855\n",
            "Iteration 306, loss = 0.54334796\n",
            "Iteration 307, loss = 0.54335283\n",
            "Iteration 308, loss = 0.54342822\n",
            "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(30, 30, 30), learning_rate_init=0.0001,\n",
              "              max_iter=10000, tol=1e-06, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resultados d acurácia\n",
        "\n",
        "redeneural_1.score(X_test,Y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxPUfHa5FgBT",
        "outputId": "30b4e3e2-4bc1-42ea-c45a-563d555078ce"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7337355528094788"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "redeneural_2.score(X_test,Y_test)"
      ],
      "metadata": {
        "id": "RLyX_NQ3G4s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c59c17d-0382-48eb-a07e-2bc28f4a5c05"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7323544377407865"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "redeneural_3.score(X_test,Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ECA6KNwsYDK",
        "outputId": "ef792147-c41f-41b8-bb0a-30ef172a50a9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7359889510794505"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "redeneural_4.score(X_test,Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXjNS8tlsZGc",
        "outputId": "6ae04610-24e4-4ed2-8472-f68a98b99546"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.737951588282329"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NIzTzOyusb_B"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}