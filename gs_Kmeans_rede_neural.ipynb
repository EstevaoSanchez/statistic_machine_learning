{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gs_Kmeans_rede_neural.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Estevão Batista Sanchez"
      ],
      "metadata": {
        "id": "1of8_ERyI4wQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import seaborn as sb\n",
        "import statsmodels.api as sm\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "DebjNa09tZWh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ab1171-7bcf-4675-a172-8f645e1d2607"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DjCrHQZUskLw"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('DATASET.csv',sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7EG4WV84tuH5",
        "outputId": "bdfff269-a831-4935-dfc7-f3588790b631"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Estado  idade  qtd_item_compra  total_compras tipo_pgto\n",
              "0     MG     22                2          502.6    CARTÃO\n",
              "1     RJ     33                4          100.6    BOLETO\n",
              "2     RJ     21                7           40.0    BOLETO\n",
              "3     SP     40                1           45.0    BOLETO\n",
              "4     ES     21                3           78.9       PIX"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3231def0-067d-4843-8e68-7b4ce01d4808\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Estado</th>\n",
              "      <th>idade</th>\n",
              "      <th>qtd_item_compra</th>\n",
              "      <th>total_compras</th>\n",
              "      <th>tipo_pgto</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MG</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>502.6</td>\n",
              "      <td>CARTÃO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RJ</td>\n",
              "      <td>33</td>\n",
              "      <td>4</td>\n",
              "      <td>100.6</td>\n",
              "      <td>BOLETO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RJ</td>\n",
              "      <td>21</td>\n",
              "      <td>7</td>\n",
              "      <td>40.0</td>\n",
              "      <td>BOLETO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SP</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>45.0</td>\n",
              "      <td>BOLETO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ES</td>\n",
              "      <td>21</td>\n",
              "      <td>3</td>\n",
              "      <td>78.9</td>\n",
              "      <td>PIX</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3231def0-067d-4843-8e68-7b4ce01d4808')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3231def0-067d-4843-8e68-7b4ce01d4808 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3231def0-067d-4843-8e68-7b4ce01d4808');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar os valores Nan\n",
        "# O dataset não possui valors Nan\n",
        "print('Estado',df.loc[pd.isna(df['Estado']),'Estado'].shape)\n",
        "print('idade',df.loc[pd.isna(df['idade']),'idade'].shape)\n",
        "print('qtd_item_compra',df.loc[pd.isna(df['qtd_item_compra']),'qtd_item_compra'].shape)\n",
        "print('total_compras',df.loc[pd.isna(df['total_compras']),'total_compras'].shape)\n",
        "print('tipo_pgto',df.loc[pd.isna(df['tipo_pgto']),'tipo_pgto'].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVxIlsUtt7PR",
        "outputId": "544219a1-5cbd-48fc-fab0-7fe298419834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estado (0,)\n",
            "idade (0,)\n",
            "qtd_item_compra (0,)\n",
            "total_compras (0,)\n",
            "tipo_pgto (0,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando valores únicos por coluna\n",
        "# Definimos as colunas que serão normalizadas e categorizados (função one hot ou variável dummy)\n",
        "\n",
        "print('Estado',len(df.Estado.unique()),df.Estado.unique())\n",
        "print('idade',len(df.idade.unique()),df.idade.unique())\n",
        "print('qtd_item_compra',len(df.qtd_item_compra.unique()),df.qtd_item_compra.unique())\n",
        "print('total_compras',len(df.total_compras.unique()),df.total_compras.unique())\n",
        "print('tipo_pgto',len(df.tipo_pgto.unique()),df.tipo_pgto.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDGq7Z3YurU8",
        "outputId": "a2234223-f890-42f5-eebe-37da7c7557ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estado 23 ['MG' 'RJ' 'SP' 'ES' 'GO' 'MT' 'MS' 'DF' 'PE' 'PI' 'BA' 'PA' 'PR' 'AM'\n",
            " 'AC' 'RR' 'RO' 'SE' 'SC' 'RS' 'PB' 'AL' 'TO']\n",
            "idade 24 [22 33 21 40 35 60 20 18 19 23 31 29 28 32 44 39 34 25 36 24 41 37 50 26]\n",
            "qtd_item_compra 10 [ 2  4  7  1  3  5  6  8  9 16]\n",
            "total_compras 76 [ 502.6   100.6    40.     45.     78.9    89.     87.     68.     65.\n",
            "  424.     57.     52.    242.    985.    224.    222.    509.77  630.\n",
            "   25.     48.    669.     53.    120.    123.    600.     59.    412.\n",
            "   90.     80.     55.     36.    336.    124.    568.     66.    687.\n",
            "   78.     56.    569.     46.     99.    790.   1800.9   879.    698.\n",
            "   63.    158.    198.    789.    122.    980.    740.    782.    965.\n",
            "  187.    256.    655.    302.    235.    968.    756.    899.36  456.\n",
            "   86.    150.    155.    204.    690.    146.    870.   1250.    486.\n",
            "  189.    999.    250.    245.  ]\n",
            "tipo_pgto 3 ['CARTÃO' 'BOLETO' 'PIX']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colunas normalizadas pela média, idade, qtd_item_compra , total_compras \n",
        "\n",
        "def normaliza_media(valores):\n",
        "  media = np.mean(valores)\n",
        "  minimo = np.min(valores)\n",
        "  maximo = np.max(valores)\n",
        "  return (valores - media)/(maximo-minimo)\n",
        "\n",
        "df.idade = normaliza_media(df.idade)\n",
        "df.qtd_item_compra = normaliza_media(df.qtd_item_compra)\n",
        "df.total_compras = normaliza_media(df.total_compras)\n"
      ],
      "metadata": {
        "id": "Q9de_A7cuStw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "4_fA5U2awhGR",
        "outputId": "914736da-137a-416f-9668-951ccce05194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Estado     idade  qtd_item_compra  total_compras tipo_pgto\n",
              "0     MG -0.112731        -0.130612       0.096121    CARTÃO"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-858e647f-0b70-4598-9206-e1f6fc1b925c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Estado</th>\n",
              "      <th>idade</th>\n",
              "      <th>qtd_item_compra</th>\n",
              "      <th>total_compras</th>\n",
              "      <th>tipo_pgto</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MG</td>\n",
              "      <td>-0.112731</td>\n",
              "      <td>-0.130612</td>\n",
              "      <td>0.096121</td>\n",
              "      <td>CARTÃO</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-858e647f-0b70-4598-9206-e1f6fc1b925c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-858e647f-0b70-4598-9206-e1f6fc1b925c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-858e647f-0b70-4598-9206-e1f6fc1b925c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colunas Categóricas, tranformadas em númericas como variável dummy\n",
        "\n",
        "dummies_estado = pd.get_dummies(df['Estado'])\n",
        "df['Estado_MG'] = dummies_estado['MG']\n",
        "df['Estado_RJ'] = dummies_estado['RJ']\n",
        "df['Estado_SP'] = dummies_estado['SP']\n",
        "df['Estado_ES'] = dummies_estado['ES']\n",
        "df['Estado_GO'] = dummies_estado['GO']\n",
        "df['Estado_MT'] = dummies_estado['MT']\n",
        "df['Estado_MS'] = dummies_estado['MS']\n",
        "df['Estado_DF'] = dummies_estado['DF']\n",
        "df['Estado_PE'] = dummies_estado['PE']\n",
        "df['Estado_PI'] = dummies_estado['PI']\n",
        "df['Estado_BA'] = dummies_estado['BA']\n",
        "df['Estado_PA'] = dummies_estado['PA']\n",
        "df['Estado_PR'] = dummies_estado['PR']\n",
        "df['Estado_AM'] = dummies_estado['AM']\n",
        "df['Estado_AC'] = dummies_estado['AC']\n",
        "df['Estado_RR'] = dummies_estado['RR']\n",
        "df['Estado_RO'] = dummies_estado['RO']\n",
        "df['Estado_SE'] = dummies_estado['SE']\n",
        "df['Estado_SC'] = dummies_estado['SC']\n",
        "df['Estado_RS'] = dummies_estado['RS']\n",
        "df['Estado_PB'] = dummies_estado['PB']\n",
        "df['Estado_AL'] = dummies_estado['AL']\n",
        "df['Estado_TO'] = dummies_estado['TO']\n",
        "\n",
        "dummies_tipo_pagamento = pd.get_dummies(df['tipo_pgto'])\n",
        "df['tipo_pgto_cartao'] = dummies_tipo_pagamento['CARTÃO']\n",
        "df['tipo_pgto_boleto'] = dummies_tipo_pagamento['BOLETO']\n",
        "df['tipo_pgto_pix'] = dummies_tipo_pagamento['PIX']\n",
        "\n"
      ],
      "metadata": {
        "id": "7fRhUSRMv6aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQSrBlMv0MsN",
        "outputId": "3c15e782-6cf6-4594-f7bc-a438ae06cb66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Estado', 'idade', 'qtd_item_compra', 'total_compras', 'tipo_pgto',\n",
              "       'Estado_MG', 'Estado_RJ', 'Estado_SP', 'Estado_ES', 'Estado_GO',\n",
              "       'Estado_MT', 'Estado_MS', 'Estado_DF', 'Estado_PE', 'Estado_PI',\n",
              "       'Estado_BA', 'Estado_PA', 'Estado_PR', 'Estado_AM', 'Estado_AC',\n",
              "       'Estado_RR', 'Estado_RO', 'Estado_SE', 'Estado_SC', 'Estado_RS',\n",
              "       'Estado_PB', 'Estado_AL', 'Estado_TO', 'tipo_pgto_cartao',\n",
              "       'tipo_pgto_boleto', 'tipo_pgto_pix'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dropando as colunas que não serão utilizadas para análise\n",
        "\n",
        "df.drop('Estado', axis=1, inplace=True)\n",
        "df.drop('tipo_pgto', axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "wGKtfMYI0H-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercicio 1\n",
        "\n",
        "# Aplicando a técnica do cotovelo para determinar o número de clusters\n",
        "\n",
        "# Chute inicial sempre de 2 clusters até 21\n",
        "\n",
        "def soma_quadrados_intra_clusters(dataset):\n",
        "\n",
        "  wcss = []\n",
        "  for n in range(2,21):\n",
        "    kmeans = KMeans(n_clusters=n)\n",
        "    kmeans.fit(dataset)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "    \n",
        "  return wcss"
      ],
      "metadata": {
        "id": "PhE9XHHi1HC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo as colunas que serão analisadas, no caso todas, pois já dropamos as que não iteressavam\n",
        "\n",
        "caracteristicas = np.array(df)"
      ],
      "metadata": {
        "id": "9_iGwo-21kiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determinando a soma dos quadrados\n",
        "soma = soma_quadrados_intra_clusters(caracteristicas)"
      ],
      "metadata": {
        "id": "wkX4M9Dl1HBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determinando o range para gerar o gráfico de análise\n",
        "n_clusters = list(range(2,21))"
      ],
      "metadata": {
        "id": "gL5Nvo-r2KJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(n_clusters, soma, '-o', color=\"red\")\n",
        "plt.plot([2,20],[soma[0],soma[-1]])\n",
        "plt.xlabel(\"Número de Cluesters\")\n",
        "plt.ylabel(\"Soma dos quadrados ontra-clusters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "KTRwQK-z1G-x",
        "outputId": "f4bbdd12-bdfd-41b3-de2b-b328d056f754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Soma dos quadrados ontra-clusters')"
            ]
          },
          "metadata": {},
          "execution_count": 241
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para determinar o número ótimo de clusters\n",
        "\n",
        "def numero_otimo_clusters(wcss):\n",
        "  import math\n",
        "  x0=2\n",
        "  y0=wcss[0]\n",
        "  x1=20\n",
        "  y1=wcss[-1]\n",
        "  distancia = []\n",
        "\n",
        "  for i in range(len(wcss)):\n",
        "    x =i+2\n",
        "    y=wcss[i]\n",
        "    numerador = abs((y1-y0)*x - (x1-x0)*y+x1*y0-y1*x0)\n",
        "    denominador = math.sqrt((y1-y0)**2+(x1-x0)**2)\n",
        "    distancia.append(numerador/denominador)\n",
        "\n",
        "  return distancia.index(max(distancia))+2"
      ],
      "metadata": {
        "id": "EEDatfkd1G4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_otimo = numero_otimo_clusters(soma)"
      ],
      "metadata": {
        "id": "7kX61Fiy1G13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Número ótimo de clusters: \",k_otimo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xnxFk0s1GuP",
        "outputId": "fa01bc0a-b3f7-433c-e413-5cae5d954264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número ótimo de clusters:  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#treinando o modelo\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, random_state=0)\n"
      ],
      "metadata": {
        "id": "oqXAxLKL359l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.fit(caracteristicas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pezZ4m0c3no1",
        "outputId": "6d363659-dd36-4a8c-fd53-31e8430d48e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(n_clusters=5, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.labels_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXbhkJS7Bkjc",
        "outputId": "8ba1e5c3-5ae1-4658-88ed-3ebb8d0da781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 2, 4, 1, 1, 3, 1, 1, 3, 2, 4, 4, 1, 0, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 1, 1, 0, 2, 2, 2, 1, 1, 1, 1, 3, 0, 3, 3, 1, 0, 1, 1, 3, 1,\n",
              "       2, 3, 0, 0, 3, 1, 4, 3, 1, 3, 1, 3, 0, 3, 3, 3, 3, 1, 0, 2, 2, 2,\n",
              "       2, 1, 3, 3, 3, 0, 1, 1, 3, 1, 3, 3, 3, 3, 1, 2, 1, 4, 0, 1, 1, 0,\n",
              "       1, 0, 2, 0, 3, 1, 3, 3, 3, 3], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KMeans = KMeans(n_clusters=3, random_state=0)"
      ],
      "metadata": {
        "id": "xhkV5z1Ky2qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determinando o resultado da classificação\n",
        "\n",
        "y_classificacao = kmeans.predict(caracteristicas)"
      ],
      "metadata": {
        "id": "hhUkhv1Y5zX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# colocando a coluna com os resultados\n",
        "df['Classe'] = y_classificacao"
      ],
      "metadata": {
        "id": "Ld8z8kEmCA00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "vHHH6NwYCV0i",
        "outputId": "72767231-359b-4281-d78c-c381b8a36648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      idade  qtd_item_compra  total_compras  Estado_MG  Estado_RJ  Estado_SP  \\\n",
              "0 -0.112731        -0.130612       0.096121          1          0          0   \n",
              "1  0.149174         0.002721      -0.130243          0          1          0   \n",
              "2 -0.136540         0.202721      -0.164367          0          1          0   \n",
              "3  0.315841        -0.197279      -0.161551          0          0          1   \n",
              "4 -0.136540        -0.063946      -0.142463          0          0          0   \n",
              "\n",
              "   Estado_ES  Estado_GO  Estado_MT  Estado_MS  ...  Estado_SE  Estado_SC  \\\n",
              "0          0          0          0          0  ...          0          0   \n",
              "1          0          0          0          0  ...          0          0   \n",
              "2          0          0          0          0  ...          0          0   \n",
              "3          0          0          0          0  ...          0          0   \n",
              "4          1          0          0          0  ...          0          0   \n",
              "\n",
              "   Estado_RS  Estado_PB  Estado_AL  Estado_TO  tipo_pgto_cartao  \\\n",
              "0          0          0          0          0                 1   \n",
              "1          0          0          0          0                 0   \n",
              "2          0          0          0          0                 0   \n",
              "3          0          0          0          0                 0   \n",
              "4          0          0          0          0                 0   \n",
              "\n",
              "   tipo_pgto_boleto  tipo_pgto_pix  Classe  \n",
              "0                 0              0       3  \n",
              "1                 1              0       2  \n",
              "2                 1              0       2  \n",
              "3                 1              0       4  \n",
              "4                 0              1       1  \n",
              "\n",
              "[5 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6907d46f-d4b1-4963-a5ce-a4c7b7035747\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idade</th>\n",
              "      <th>qtd_item_compra</th>\n",
              "      <th>total_compras</th>\n",
              "      <th>Estado_MG</th>\n",
              "      <th>Estado_RJ</th>\n",
              "      <th>Estado_SP</th>\n",
              "      <th>Estado_ES</th>\n",
              "      <th>Estado_GO</th>\n",
              "      <th>Estado_MT</th>\n",
              "      <th>Estado_MS</th>\n",
              "      <th>...</th>\n",
              "      <th>Estado_SE</th>\n",
              "      <th>Estado_SC</th>\n",
              "      <th>Estado_RS</th>\n",
              "      <th>Estado_PB</th>\n",
              "      <th>Estado_AL</th>\n",
              "      <th>Estado_TO</th>\n",
              "      <th>tipo_pgto_cartao</th>\n",
              "      <th>tipo_pgto_boleto</th>\n",
              "      <th>tipo_pgto_pix</th>\n",
              "      <th>Classe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.112731</td>\n",
              "      <td>-0.130612</td>\n",
              "      <td>0.096121</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.149174</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>-0.130243</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.136540</td>\n",
              "      <td>0.202721</td>\n",
              "      <td>-0.164367</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.315841</td>\n",
              "      <td>-0.197279</td>\n",
              "      <td>-0.161551</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.136540</td>\n",
              "      <td>-0.063946</td>\n",
              "      <td>-0.142463</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6907d46f-d4b1-4963-a5ce-a4c7b7035747')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6907d46f-d4b1-4963-a5ce-a4c7b7035747 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6907d46f-d4b1-4963-a5ce-a4c7b7035747');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercício 2\n",
        "# Selecionando as colunas de saída \n",
        "\n",
        "saida = df[['tipo_pgto_cartao','tipo_pgto_boleto','tipo_pgto_pix']]\n",
        "print(saida)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSfezjTxzWtb",
        "outputId": "6539e613-c0b7-4f1f-99e6-c9ebd182ddd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    tipo_pgto_cartao  tipo_pgto_boleto  tipo_pgto_pix\n",
            "0                  1                 0              0\n",
            "1                  0                 1              0\n",
            "2                  0                 1              0\n",
            "3                  0                 1              0\n",
            "4                  0                 0              1\n",
            "..               ...               ...            ...\n",
            "93                 0                 0              1\n",
            "94                 1                 0              0\n",
            "95                 1                 0              0\n",
            "96                 1                 0              0\n",
            "97                 1                 0              0\n",
            "\n",
            "[98 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionando as colunas de entrada \n",
        "\n",
        "entradas = df[['idade','qtd_item_compra','total_compras','Estado_MG','Estado_RJ','Estado_SP','Estado_ES','Estado_GO'\\\n",
        "               ,'Estado_MT','Estado_MS','Estado_DF', 'Estado_PE', 'Estado_PI', 'Estado_BA', 'Estado_PA','Estado_PR', 'Estado_AM', 'Estado_AC', 'Estado_RR', 'Estado_RO',\n",
        "       'Estado_SE', 'Estado_SC', 'Estado_RS', 'Estado_PB', 'Estado_AL','Estado_TO']]\n"
      ],
      "metadata": {
        "id": "u5OrmexM6MyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separando o data frame para treino e teste, 20% teste e 80% treino\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(entradas, saida, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "-Snn5Cyx6B3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando o número de neorônios da camada oculta\n",
        "# Entradas = 26\n",
        "# saidas = 3\n",
        "\n",
        "# Range de neuronios entre 15 e 52\n",
        "\n",
        "nco_1 = (26+3)/2\n",
        "nco_2 = (2*26)/3 + 3\n",
        "nco_3 = 2*26\n",
        "\n",
        "print(nco_1,nco_2,nco_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qOauj6B-QBV",
        "outputId": "2c0e3751-3222-4ef8-f863-aeb25e1ccb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.5 20.333333333333332 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determinando os parametros dos modelos de rede neural\n",
        "\n",
        "# learning_rate_init - taxa de aprendizado inicial usada. Ele controla o tamanho do passo na atualização dos pesos\n",
        "# max_iter - Número máximo de iterações\n",
        "# activation - Função utilizada  - Linear\n",
        "# tol - Tolerância para a otimização\n",
        "# hidden_layer_sizes - Número de camdas ocultas e neuronios\n",
        "\n",
        "# 2 camadas ocultas com 40 neuronios\n",
        "\n",
        "redeneural_1 = MLPClassifier(verbose=True,\n",
        "                           max_iter=10000,\n",
        "                           hidden_layer_sizes=(2,40),\n",
        "                           tol=0.000001,\n",
        "                           activation='tanh',\n",
        "                           learning_rate_init = 0.0001)"
      ],
      "metadata": {
        "id": "MeGsdQpv6D2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# treinando o modelo\n",
        "\n",
        "redeneural_1.fit(X_train,Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6SoZDcF9Mtl",
        "outputId": "257061b8-9a69-46f4-ee32-363afc7fce03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Iteration 5001, loss = 0.97443139\n",
            "Iteration 5002, loss = 0.97435050\n",
            "Iteration 5003, loss = 0.97426964\n",
            "Iteration 5004, loss = 0.97418879\n",
            "Iteration 5005, loss = 0.97410798\n",
            "Iteration 5006, loss = 0.97402718\n",
            "Iteration 5007, loss = 0.97394642\n",
            "Iteration 5008, loss = 0.97386567\n",
            "Iteration 5009, loss = 0.97378495\n",
            "Iteration 5010, loss = 0.97370426\n",
            "Iteration 5011, loss = 0.97362358\n",
            "Iteration 5012, loss = 0.97354294\n",
            "Iteration 5013, loss = 0.97346231\n",
            "Iteration 5014, loss = 0.97338172\n",
            "Iteration 5015, loss = 0.97330114\n",
            "Iteration 5016, loss = 0.97322059\n",
            "Iteration 5017, loss = 0.97314006\n",
            "Iteration 5018, loss = 0.97305956\n",
            "Iteration 5019, loss = 0.97297908\n",
            "Iteration 5020, loss = 0.97289863\n",
            "Iteration 5021, loss = 0.97281820\n",
            "Iteration 5022, loss = 0.97273779\n",
            "Iteration 5023, loss = 0.97265741\n",
            "Iteration 5024, loss = 0.97257705\n",
            "Iteration 5025, loss = 0.97249672\n",
            "Iteration 5026, loss = 0.97241641\n",
            "Iteration 5027, loss = 0.97233612\n",
            "Iteration 5028, loss = 0.97225586\n",
            "Iteration 5029, loss = 0.97217562\n",
            "Iteration 5030, loss = 0.97209540\n",
            "Iteration 5031, loss = 0.97201521\n",
            "Iteration 5032, loss = 0.97193504\n",
            "Iteration 5033, loss = 0.97185490\n",
            "Iteration 5034, loss = 0.97177478\n",
            "Iteration 5035, loss = 0.97169468\n",
            "Iteration 5036, loss = 0.97161461\n",
            "Iteration 5037, loss = 0.97153456\n",
            "Iteration 5038, loss = 0.97145453\n",
            "Iteration 5039, loss = 0.97137453\n",
            "Iteration 5040, loss = 0.97129455\n",
            "Iteration 5041, loss = 0.97121459\n",
            "Iteration 5042, loss = 0.97113466\n",
            "Iteration 5043, loss = 0.97105475\n",
            "Iteration 5044, loss = 0.97097486\n",
            "Iteration 5045, loss = 0.97089500\n",
            "Iteration 5046, loss = 0.97081516\n",
            "Iteration 5047, loss = 0.97073535\n",
            "Iteration 5048, loss = 0.97065556\n",
            "Iteration 5049, loss = 0.97057579\n",
            "Iteration 5050, loss = 0.97049604\n",
            "Iteration 5051, loss = 0.97041632\n",
            "Iteration 5052, loss = 0.97033662\n",
            "Iteration 5053, loss = 0.97025694\n",
            "Iteration 5054, loss = 0.97017729\n",
            "Iteration 5055, loss = 0.97009766\n",
            "Iteration 5056, loss = 0.97001805\n",
            "Iteration 5057, loss = 0.96993847\n",
            "Iteration 5058, loss = 0.96985891\n",
            "Iteration 5059, loss = 0.96977937\n",
            "Iteration 5060, loss = 0.96969986\n",
            "Iteration 5061, loss = 0.96962037\n",
            "Iteration 5062, loss = 0.96954090\n",
            "Iteration 5063, loss = 0.96946145\n",
            "Iteration 5064, loss = 0.96938203\n",
            "Iteration 5065, loss = 0.96930263\n",
            "Iteration 5066, loss = 0.96922325\n",
            "Iteration 5067, loss = 0.96914390\n",
            "Iteration 5068, loss = 0.96906457\n",
            "Iteration 5069, loss = 0.96898526\n",
            "Iteration 5070, loss = 0.96890597\n",
            "Iteration 5071, loss = 0.96882671\n",
            "Iteration 5072, loss = 0.96874747\n",
            "Iteration 5073, loss = 0.96866825\n",
            "Iteration 5074, loss = 0.96858906\n",
            "Iteration 5075, loss = 0.96850989\n",
            "Iteration 5076, loss = 0.96843074\n",
            "Iteration 5077, loss = 0.96835161\n",
            "Iteration 5078, loss = 0.96827251\n",
            "Iteration 5079, loss = 0.96819343\n",
            "Iteration 5080, loss = 0.96811437\n",
            "Iteration 5081, loss = 0.96803533\n",
            "Iteration 5082, loss = 0.96795632\n",
            "Iteration 5083, loss = 0.96787732\n",
            "Iteration 5084, loss = 0.96779835\n",
            "Iteration 5085, loss = 0.96771941\n",
            "Iteration 5086, loss = 0.96764048\n",
            "Iteration 5087, loss = 0.96756158\n",
            "Iteration 5088, loss = 0.96748270\n",
            "Iteration 5089, loss = 0.96740384\n",
            "Iteration 5090, loss = 0.96732501\n",
            "Iteration 5091, loss = 0.96724619\n",
            "Iteration 5092, loss = 0.96716740\n",
            "Iteration 5093, loss = 0.96708864\n",
            "Iteration 5094, loss = 0.96700989\n",
            "Iteration 5095, loss = 0.96693116\n",
            "Iteration 5096, loss = 0.96685246\n",
            "Iteration 5097, loss = 0.96677378\n",
            "Iteration 5098, loss = 0.96669512\n",
            "Iteration 5099, loss = 0.96661649\n",
            "Iteration 5100, loss = 0.96653787\n",
            "Iteration 5101, loss = 0.96645928\n",
            "Iteration 5102, loss = 0.96638071\n",
            "Iteration 5103, loss = 0.96630217\n",
            "Iteration 5104, loss = 0.96622364\n",
            "Iteration 5105, loss = 0.96614514\n",
            "Iteration 5106, loss = 0.96606665\n",
            "Iteration 5107, loss = 0.96598819\n",
            "Iteration 5108, loss = 0.96590976\n",
            "Iteration 5109, loss = 0.96583134\n",
            "Iteration 5110, loss = 0.96575294\n",
            "Iteration 5111, loss = 0.96567457\n",
            "Iteration 5112, loss = 0.96559622\n",
            "Iteration 5113, loss = 0.96551789\n",
            "Iteration 5114, loss = 0.96543958\n",
            "Iteration 5115, loss = 0.96536130\n",
            "Iteration 5116, loss = 0.96528303\n",
            "Iteration 5117, loss = 0.96520479\n",
            "Iteration 5118, loss = 0.96512657\n",
            "Iteration 5119, loss = 0.96504837\n",
            "Iteration 5120, loss = 0.96497019\n",
            "Iteration 5121, loss = 0.96489204\n",
            "Iteration 5122, loss = 0.96481390\n",
            "Iteration 5123, loss = 0.96473579\n",
            "Iteration 5124, loss = 0.96465769\n",
            "Iteration 5125, loss = 0.96457962\n",
            "Iteration 5126, loss = 0.96450158\n",
            "Iteration 5127, loss = 0.96442355\n",
            "Iteration 5128, loss = 0.96434554\n",
            "Iteration 5129, loss = 0.96426756\n",
            "Iteration 5130, loss = 0.96418959\n",
            "Iteration 5131, loss = 0.96411165\n",
            "Iteration 5132, loss = 0.96403373\n",
            "Iteration 5133, loss = 0.96395583\n",
            "Iteration 5134, loss = 0.96387795\n",
            "Iteration 5135, loss = 0.96380010\n",
            "Iteration 5136, loss = 0.96372226\n",
            "Iteration 5137, loss = 0.96364444\n",
            "Iteration 5138, loss = 0.96356665\n",
            "Iteration 5139, loss = 0.96348888\n",
            "Iteration 5140, loss = 0.96341113\n",
            "Iteration 5141, loss = 0.96333340\n",
            "Iteration 5142, loss = 0.96325569\n",
            "Iteration 5143, loss = 0.96317800\n",
            "Iteration 5144, loss = 0.96310033\n",
            "Iteration 5145, loss = 0.96302269\n",
            "Iteration 5146, loss = 0.96294506\n",
            "Iteration 5147, loss = 0.96286746\n",
            "Iteration 5148, loss = 0.96278987\n",
            "Iteration 5149, loss = 0.96271231\n",
            "Iteration 5150, loss = 0.96263477\n",
            "Iteration 5151, loss = 0.96255725\n",
            "Iteration 5152, loss = 0.96247975\n",
            "Iteration 5153, loss = 0.96240227\n",
            "Iteration 5154, loss = 0.96232481\n",
            "Iteration 5155, loss = 0.96224737\n",
            "Iteration 5156, loss = 0.96216996\n",
            "Iteration 5157, loss = 0.96209256\n",
            "Iteration 5158, loss = 0.96201518\n",
            "Iteration 5159, loss = 0.96193783\n",
            "Iteration 5160, loss = 0.96186049\n",
            "Iteration 5161, loss = 0.96178318\n",
            "Iteration 5162, loss = 0.96170589\n",
            "Iteration 5163, loss = 0.96162862\n",
            "Iteration 5164, loss = 0.96155136\n",
            "Iteration 5165, loss = 0.96147413\n",
            "Iteration 5166, loss = 0.96139692\n",
            "Iteration 5167, loss = 0.96131973\n",
            "Iteration 5168, loss = 0.96124256\n",
            "Iteration 5169, loss = 0.96116541\n",
            "Iteration 5170, loss = 0.96108828\n",
            "Iteration 5171, loss = 0.96101118\n",
            "Iteration 5172, loss = 0.96093409\n",
            "Iteration 5173, loss = 0.96085702\n",
            "Iteration 5174, loss = 0.96077997\n",
            "Iteration 5175, loss = 0.96070295\n",
            "Iteration 5176, loss = 0.96062594\n",
            "Iteration 5177, loss = 0.96054895\n",
            "Iteration 5178, loss = 0.96047199\n",
            "Iteration 5179, loss = 0.96039504\n",
            "Iteration 5180, loss = 0.96031811\n",
            "Iteration 5181, loss = 0.96024121\n",
            "Iteration 5182, loss = 0.96016432\n",
            "Iteration 5183, loss = 0.96008746\n",
            "Iteration 5184, loss = 0.96001061\n",
            "Iteration 5185, loss = 0.95993379\n",
            "Iteration 5186, loss = 0.95985698\n",
            "Iteration 5187, loss = 0.95978020\n",
            "Iteration 5188, loss = 0.95970343\n",
            "Iteration 5189, loss = 0.95962669\n",
            "Iteration 5190, loss = 0.95954996\n",
            "Iteration 5191, loss = 0.95947326\n",
            "Iteration 5192, loss = 0.95939657\n",
            "Iteration 5193, loss = 0.95931991\n",
            "Iteration 5194, loss = 0.95924326\n",
            "Iteration 5195, loss = 0.95916664\n",
            "Iteration 5196, loss = 0.95909003\n",
            "Iteration 5197, loss = 0.95901344\n",
            "Iteration 5198, loss = 0.95893688\n",
            "Iteration 5199, loss = 0.95886033\n",
            "Iteration 5200, loss = 0.95878381\n",
            "Iteration 5201, loss = 0.95870730\n",
            "Iteration 5202, loss = 0.95863081\n",
            "Iteration 5203, loss = 0.95855435\n",
            "Iteration 5204, loss = 0.95847790\n",
            "Iteration 5205, loss = 0.95840147\n",
            "Iteration 5206, loss = 0.95832506\n",
            "Iteration 5207, loss = 0.95824867\n",
            "Iteration 5208, loss = 0.95817231\n",
            "Iteration 5209, loss = 0.95809596\n",
            "Iteration 5210, loss = 0.95801963\n",
            "Iteration 5211, loss = 0.95794332\n",
            "Iteration 5212, loss = 0.95786703\n",
            "Iteration 5213, loss = 0.95779075\n",
            "Iteration 5214, loss = 0.95771450\n",
            "Iteration 5215, loss = 0.95763827\n",
            "Iteration 5216, loss = 0.95756206\n",
            "Iteration 5217, loss = 0.95748586\n",
            "Iteration 5218, loss = 0.95740969\n",
            "Iteration 5219, loss = 0.95733354\n",
            "Iteration 5220, loss = 0.95725740\n",
            "Iteration 5221, loss = 0.95718128\n",
            "Iteration 5222, loss = 0.95710519\n",
            "Iteration 5223, loss = 0.95702911\n",
            "Iteration 5224, loss = 0.95695305\n",
            "Iteration 5225, loss = 0.95687701\n",
            "Iteration 5226, loss = 0.95680099\n",
            "Iteration 5227, loss = 0.95672499\n",
            "Iteration 5228, loss = 0.95664901\n",
            "Iteration 5229, loss = 0.95657305\n",
            "Iteration 5230, loss = 0.95649711\n",
            "Iteration 5231, loss = 0.95642118\n",
            "Iteration 5232, loss = 0.95634528\n",
            "Iteration 5233, loss = 0.95626939\n",
            "Iteration 5234, loss = 0.95619353\n",
            "Iteration 5235, loss = 0.95611768\n",
            "Iteration 5236, loss = 0.95604185\n",
            "Iteration 5237, loss = 0.95596604\n",
            "Iteration 5238, loss = 0.95589025\n",
            "Iteration 5239, loss = 0.95581448\n",
            "Iteration 5240, loss = 0.95573873\n",
            "Iteration 5241, loss = 0.95566299\n",
            "Iteration 5242, loss = 0.95558728\n",
            "Iteration 5243, loss = 0.95551158\n",
            "Iteration 5244, loss = 0.95543591\n",
            "Iteration 5245, loss = 0.95536025\n",
            "Iteration 5246, loss = 0.95528461\n",
            "Iteration 5247, loss = 0.95520899\n",
            "Iteration 5248, loss = 0.95513339\n",
            "Iteration 5249, loss = 0.95505780\n",
            "Iteration 5250, loss = 0.95498224\n",
            "Iteration 5251, loss = 0.95490669\n",
            "Iteration 5252, loss = 0.95483117\n",
            "Iteration 5253, loss = 0.95475566\n",
            "Iteration 5254, loss = 0.95468017\n",
            "Iteration 5255, loss = 0.95460470\n",
            "Iteration 5256, loss = 0.95452925\n",
            "Iteration 5257, loss = 0.95445381\n",
            "Iteration 5258, loss = 0.95437840\n",
            "Iteration 5259, loss = 0.95430300\n",
            "Iteration 5260, loss = 0.95422762\n",
            "Iteration 5261, loss = 0.95415226\n",
            "Iteration 5262, loss = 0.95407692\n",
            "Iteration 5263, loss = 0.95400160\n",
            "Iteration 5264, loss = 0.95392630\n",
            "Iteration 5265, loss = 0.95385101\n",
            "Iteration 5266, loss = 0.95377574\n",
            "Iteration 5267, loss = 0.95370050\n",
            "Iteration 5268, loss = 0.95362527\n",
            "Iteration 5269, loss = 0.95355005\n",
            "Iteration 5270, loss = 0.95347486\n",
            "Iteration 5271, loss = 0.95339969\n",
            "Iteration 5272, loss = 0.95332453\n",
            "Iteration 5273, loss = 0.95324939\n",
            "Iteration 5274, loss = 0.95317427\n",
            "Iteration 5275, loss = 0.95309917\n",
            "Iteration 5276, loss = 0.95302408\n",
            "Iteration 5277, loss = 0.95294902\n",
            "Iteration 5278, loss = 0.95287397\n",
            "Iteration 5279, loss = 0.95279894\n",
            "Iteration 5280, loss = 0.95272393\n",
            "Iteration 5281, loss = 0.95264894\n",
            "Iteration 5282, loss = 0.95257396\n",
            "Iteration 5283, loss = 0.95249901\n",
            "Iteration 5284, loss = 0.95242407\n",
            "Iteration 5285, loss = 0.95234915\n",
            "Iteration 5286, loss = 0.95227425\n",
            "Iteration 5287, loss = 0.95219936\n",
            "Iteration 5288, loss = 0.95212450\n",
            "Iteration 5289, loss = 0.95204965\n",
            "Iteration 5290, loss = 0.95197482\n",
            "Iteration 5291, loss = 0.95190001\n",
            "Iteration 5292, loss = 0.95182521\n",
            "Iteration 5293, loss = 0.95175044\n",
            "Iteration 5294, loss = 0.95167568\n",
            "Iteration 5295, loss = 0.95160094\n",
            "Iteration 5296, loss = 0.95152621\n",
            "Iteration 5297, loss = 0.95145151\n",
            "Iteration 5298, loss = 0.95137682\n",
            "Iteration 5299, loss = 0.95130215\n",
            "Iteration 5300, loss = 0.95122750\n",
            "Iteration 5301, loss = 0.95115287\n",
            "Iteration 5302, loss = 0.95107825\n",
            "Iteration 5303, loss = 0.95100365\n",
            "Iteration 5304, loss = 0.95092907\n",
            "Iteration 5305, loss = 0.95085451\n",
            "Iteration 5306, loss = 0.95077996\n",
            "Iteration 5307, loss = 0.95070544\n",
            "Iteration 5308, loss = 0.95063093\n",
            "Iteration 5309, loss = 0.95055644\n",
            "Iteration 5310, loss = 0.95048196\n",
            "Iteration 5311, loss = 0.95040750\n",
            "Iteration 5312, loss = 0.95033307\n",
            "Iteration 5313, loss = 0.95025864\n",
            "Iteration 5314, loss = 0.95018424\n",
            "Iteration 5315, loss = 0.95010985\n",
            "Iteration 5316, loss = 0.95003548\n",
            "Iteration 5317, loss = 0.94996113\n",
            "Iteration 5318, loss = 0.94988680\n",
            "Iteration 5319, loss = 0.94981248\n",
            "Iteration 5320, loss = 0.94973818\n",
            "Iteration 5321, loss = 0.94966390\n",
            "Iteration 5322, loss = 0.94958964\n",
            "Iteration 5323, loss = 0.94951539\n",
            "Iteration 5324, loss = 0.94944116\n",
            "Iteration 5325, loss = 0.94936695\n",
            "Iteration 5326, loss = 0.94929275\n",
            "Iteration 5327, loss = 0.94921857\n",
            "Iteration 5328, loss = 0.94914441\n",
            "Iteration 5329, loss = 0.94907027\n",
            "Iteration 5330, loss = 0.94899614\n",
            "Iteration 5331, loss = 0.94892204\n",
            "Iteration 5332, loss = 0.94884795\n",
            "Iteration 5333, loss = 0.94877387\n",
            "Iteration 5334, loss = 0.94869981\n",
            "Iteration 5335, loss = 0.94862577\n",
            "Iteration 5336, loss = 0.94855175\n",
            "Iteration 5337, loss = 0.94847775\n",
            "Iteration 5338, loss = 0.94840376\n",
            "Iteration 5339, loss = 0.94832979\n",
            "Iteration 5340, loss = 0.94825583\n",
            "Iteration 5341, loss = 0.94818190\n",
            "Iteration 5342, loss = 0.94810798\n",
            "Iteration 5343, loss = 0.94803407\n",
            "Iteration 5344, loss = 0.94796019\n",
            "Iteration 5345, loss = 0.94788632\n",
            "Iteration 5346, loss = 0.94781247\n",
            "Iteration 5347, loss = 0.94773863\n",
            "Iteration 5348, loss = 0.94766482\n",
            "Iteration 5349, loss = 0.94759101\n",
            "Iteration 5350, loss = 0.94751723\n",
            "Iteration 5351, loss = 0.94744346\n",
            "Iteration 5352, loss = 0.94736971\n",
            "Iteration 5353, loss = 0.94729598\n",
            "Iteration 5354, loss = 0.94722226\n",
            "Iteration 5355, loss = 0.94714856\n",
            "Iteration 5356, loss = 0.94707488\n",
            "Iteration 5357, loss = 0.94700122\n",
            "Iteration 5358, loss = 0.94692757\n",
            "Iteration 5359, loss = 0.94685393\n",
            "Iteration 5360, loss = 0.94678032\n",
            "Iteration 5361, loss = 0.94670672\n",
            "Iteration 5362, loss = 0.94663314\n",
            "Iteration 5363, loss = 0.94655957\n",
            "Iteration 5364, loss = 0.94648603\n",
            "Iteration 5365, loss = 0.94641249\n",
            "Iteration 5366, loss = 0.94633898\n",
            "Iteration 5367, loss = 0.94626548\n",
            "Iteration 5368, loss = 0.94619200\n",
            "Iteration 5369, loss = 0.94611853\n",
            "Iteration 5370, loss = 0.94604508\n",
            "Iteration 5371, loss = 0.94597165\n",
            "Iteration 5372, loss = 0.94589824\n",
            "Iteration 5373, loss = 0.94582484\n",
            "Iteration 5374, loss = 0.94575146\n",
            "Iteration 5375, loss = 0.94567809\n",
            "Iteration 5376, loss = 0.94560474\n",
            "Iteration 5377, loss = 0.94553141\n",
            "Iteration 5378, loss = 0.94545809\n",
            "Iteration 5379, loss = 0.94538479\n",
            "Iteration 5380, loss = 0.94531151\n",
            "Iteration 5381, loss = 0.94523824\n",
            "Iteration 5382, loss = 0.94516499\n",
            "Iteration 5383, loss = 0.94509176\n",
            "Iteration 5384, loss = 0.94501854\n",
            "Iteration 5385, loss = 0.94494534\n",
            "Iteration 5386, loss = 0.94487215\n",
            "Iteration 5387, loss = 0.94479899\n",
            "Iteration 5388, loss = 0.94472583\n",
            "Iteration 5389, loss = 0.94465270\n",
            "Iteration 5390, loss = 0.94457958\n",
            "Iteration 5391, loss = 0.94450648\n",
            "Iteration 5392, loss = 0.94443339\n",
            "Iteration 5393, loss = 0.94436032\n",
            "Iteration 5394, loss = 0.94428726\n",
            "Iteration 5395, loss = 0.94421423\n",
            "Iteration 5396, loss = 0.94414120\n",
            "Iteration 5397, loss = 0.94406820\n",
            "Iteration 5398, loss = 0.94399521\n",
            "Iteration 5399, loss = 0.94392223\n",
            "Iteration 5400, loss = 0.94384928\n",
            "Iteration 5401, loss = 0.94377634\n",
            "Iteration 5402, loss = 0.94370341\n",
            "Iteration 5403, loss = 0.94363050\n",
            "Iteration 5404, loss = 0.94355761\n",
            "Iteration 5405, loss = 0.94348473\n",
            "Iteration 5406, loss = 0.94341187\n",
            "Iteration 5407, loss = 0.94333903\n",
            "Iteration 5408, loss = 0.94326620\n",
            "Iteration 5409, loss = 0.94319339\n",
            "Iteration 5410, loss = 0.94312059\n",
            "Iteration 5411, loss = 0.94304781\n",
            "Iteration 5412, loss = 0.94297505\n",
            "Iteration 5413, loss = 0.94290230\n",
            "Iteration 5414, loss = 0.94282957\n",
            "Iteration 5415, loss = 0.94275685\n",
            "Iteration 5416, loss = 0.94268415\n",
            "Iteration 5417, loss = 0.94261146\n",
            "Iteration 5418, loss = 0.94253879\n",
            "Iteration 5419, loss = 0.94246614\n",
            "Iteration 5420, loss = 0.94239350\n",
            "Iteration 5421, loss = 0.94232088\n",
            "Iteration 5422, loss = 0.94224828\n",
            "Iteration 5423, loss = 0.94217569\n",
            "Iteration 5424, loss = 0.94210311\n",
            "Iteration 5425, loss = 0.94203056\n",
            "Iteration 5426, loss = 0.94195801\n",
            "Iteration 5427, loss = 0.94188549\n",
            "Iteration 5428, loss = 0.94181298\n",
            "Iteration 5429, loss = 0.94174048\n",
            "Iteration 5430, loss = 0.94166800\n",
            "Iteration 5431, loss = 0.94159554\n",
            "Iteration 5432, loss = 0.94152309\n",
            "Iteration 5433, loss = 0.94145066\n",
            "Iteration 5434, loss = 0.94137824\n",
            "Iteration 5435, loss = 0.94130584\n",
            "Iteration 5436, loss = 0.94123346\n",
            "Iteration 5437, loss = 0.94116109\n",
            "Iteration 5438, loss = 0.94108874\n",
            "Iteration 5439, loss = 0.94101640\n",
            "Iteration 5440, loss = 0.94094408\n",
            "Iteration 5441, loss = 0.94087177\n",
            "Iteration 5442, loss = 0.94079948\n",
            "Iteration 5443, loss = 0.94072720\n",
            "Iteration 5444, loss = 0.94065494\n",
            "Iteration 5445, loss = 0.94058270\n",
            "Iteration 5446, loss = 0.94051047\n",
            "Iteration 5447, loss = 0.94043825\n",
            "Iteration 5448, loss = 0.94036606\n",
            "Iteration 5449, loss = 0.94029387\n",
            "Iteration 5450, loss = 0.94022171\n",
            "Iteration 5451, loss = 0.94014955\n",
            "Iteration 5452, loss = 0.94007742\n",
            "Iteration 5453, loss = 0.94000530\n",
            "Iteration 5454, loss = 0.93993319\n",
            "Iteration 5455, loss = 0.93986110\n",
            "Iteration 5456, loss = 0.93978903\n",
            "Iteration 5457, loss = 0.93971697\n",
            "Iteration 5458, loss = 0.93964492\n",
            "Iteration 5459, loss = 0.93957290\n",
            "Iteration 5460, loss = 0.93950088\n",
            "Iteration 5461, loss = 0.93942888\n",
            "Iteration 5462, loss = 0.93935690\n",
            "Iteration 5463, loss = 0.93928493\n",
            "Iteration 5464, loss = 0.93921298\n",
            "Iteration 5465, loss = 0.93914105\n",
            "Iteration 5466, loss = 0.93906912\n",
            "Iteration 5467, loss = 0.93899722\n",
            "Iteration 5468, loss = 0.93892533\n",
            "Iteration 5469, loss = 0.93885345\n",
            "Iteration 5470, loss = 0.93878159\n",
            "Iteration 5471, loss = 0.93870975\n",
            "Iteration 5472, loss = 0.93863792\n",
            "Iteration 5473, loss = 0.93856610\n",
            "Iteration 5474, loss = 0.93849430\n",
            "Iteration 5475, loss = 0.93842252\n",
            "Iteration 5476, loss = 0.93835075\n",
            "Iteration 5477, loss = 0.93827899\n",
            "Iteration 5478, loss = 0.93820725\n",
            "Iteration 5479, loss = 0.93813553\n",
            "Iteration 5480, loss = 0.93806382\n",
            "Iteration 5481, loss = 0.93799213\n",
            "Iteration 5482, loss = 0.93792045\n",
            "Iteration 5483, loss = 0.93784878\n",
            "Iteration 5484, loss = 0.93777714\n",
            "Iteration 5485, loss = 0.93770550\n",
            "Iteration 5486, loss = 0.93763388\n",
            "Iteration 5487, loss = 0.93756228\n",
            "Iteration 5488, loss = 0.93749069\n",
            "Iteration 5489, loss = 0.93741912\n",
            "Iteration 5490, loss = 0.93734756\n",
            "Iteration 5491, loss = 0.93727601\n",
            "Iteration 5492, loss = 0.93720449\n",
            "Iteration 5493, loss = 0.93713297\n",
            "Iteration 5494, loss = 0.93706147\n",
            "Iteration 5495, loss = 0.93698999\n",
            "Iteration 5496, loss = 0.93691852\n",
            "Iteration 5497, loss = 0.93684707\n",
            "Iteration 5498, loss = 0.93677563\n",
            "Iteration 5499, loss = 0.93670420\n",
            "Iteration 5500, loss = 0.93663279\n",
            "Iteration 5501, loss = 0.93656140\n",
            "Iteration 5502, loss = 0.93649002\n",
            "Iteration 5503, loss = 0.93641865\n",
            "Iteration 5504, loss = 0.93634730\n",
            "Iteration 5505, loss = 0.93627597\n",
            "Iteration 5506, loss = 0.93620465\n",
            "Iteration 5507, loss = 0.93613334\n",
            "Iteration 5508, loss = 0.93606205\n",
            "Iteration 5509, loss = 0.93599077\n",
            "Iteration 5510, loss = 0.93591951\n",
            "Iteration 5511, loss = 0.93584826\n",
            "Iteration 5512, loss = 0.93577703\n",
            "Iteration 5513, loss = 0.93570582\n",
            "Iteration 5514, loss = 0.93563461\n",
            "Iteration 5515, loss = 0.93556342\n",
            "Iteration 5516, loss = 0.93549225\n",
            "Iteration 5517, loss = 0.93542109\n",
            "Iteration 5518, loss = 0.93534995\n",
            "Iteration 5519, loss = 0.93527882\n",
            "Iteration 5520, loss = 0.93520770\n",
            "Iteration 5521, loss = 0.93513660\n",
            "Iteration 5522, loss = 0.93506552\n",
            "Iteration 5523, loss = 0.93499445\n",
            "Iteration 5524, loss = 0.93492339\n",
            "Iteration 5525, loss = 0.93485235\n",
            "Iteration 5526, loss = 0.93478132\n",
            "Iteration 5527, loss = 0.93471031\n",
            "Iteration 5528, loss = 0.93463931\n",
            "Iteration 5529, loss = 0.93456833\n",
            "Iteration 5530, loss = 0.93449736\n",
            "Iteration 5531, loss = 0.93442641\n",
            "Iteration 5532, loss = 0.93435547\n",
            "Iteration 5533, loss = 0.93428454\n",
            "Iteration 5534, loss = 0.93421363\n",
            "Iteration 5535, loss = 0.93414273\n",
            "Iteration 5536, loss = 0.93407185\n",
            "Iteration 5537, loss = 0.93400098\n",
            "Iteration 5538, loss = 0.93393013\n",
            "Iteration 5539, loss = 0.93385929\n",
            "Iteration 5540, loss = 0.93378847\n",
            "Iteration 5541, loss = 0.93371766\n",
            "Iteration 5542, loss = 0.93364686\n",
            "Iteration 5543, loss = 0.93357608\n",
            "Iteration 5544, loss = 0.93350532\n",
            "Iteration 5545, loss = 0.93343456\n",
            "Iteration 5546, loss = 0.93336383\n",
            "Iteration 5547, loss = 0.93329310\n",
            "Iteration 5548, loss = 0.93322239\n",
            "Iteration 5549, loss = 0.93315170\n",
            "Iteration 5550, loss = 0.93308102\n",
            "Iteration 5551, loss = 0.93301035\n",
            "Iteration 5552, loss = 0.93293970\n",
            "Iteration 5553, loss = 0.93286906\n",
            "Iteration 5554, loss = 0.93279844\n",
            "Iteration 5555, loss = 0.93272783\n",
            "Iteration 5556, loss = 0.93265724\n",
            "Iteration 5557, loss = 0.93258666\n",
            "Iteration 5558, loss = 0.93251609\n",
            "Iteration 5559, loss = 0.93244554\n",
            "Iteration 5560, loss = 0.93237500\n",
            "Iteration 5561, loss = 0.93230448\n",
            "Iteration 5562, loss = 0.93223397\n",
            "Iteration 5563, loss = 0.93216347\n",
            "Iteration 5564, loss = 0.93209299\n",
            "Iteration 5565, loss = 0.93202253\n",
            "Iteration 5566, loss = 0.93195207\n",
            "Iteration 5567, loss = 0.93188163\n",
            "Iteration 5568, loss = 0.93181121\n",
            "Iteration 5569, loss = 0.93174080\n",
            "Iteration 5570, loss = 0.93167040\n",
            "Iteration 5571, loss = 0.93160002\n",
            "Iteration 5572, loss = 0.93152966\n",
            "Iteration 5573, loss = 0.93145930\n",
            "Iteration 5574, loss = 0.93138896\n",
            "Iteration 5575, loss = 0.93131864\n",
            "Iteration 5576, loss = 0.93124833\n",
            "Iteration 5577, loss = 0.93117803\n",
            "Iteration 5578, loss = 0.93110775\n",
            "Iteration 5579, loss = 0.93103748\n",
            "Iteration 5580, loss = 0.93096722\n",
            "Iteration 5581, loss = 0.93089698\n",
            "Iteration 5582, loss = 0.93082675\n",
            "Iteration 5583, loss = 0.93075654\n",
            "Iteration 5584, loss = 0.93068634\n",
            "Iteration 5585, loss = 0.93061616\n",
            "Iteration 5586, loss = 0.93054599\n",
            "Iteration 5587, loss = 0.93047583\n",
            "Iteration 5588, loss = 0.93040569\n",
            "Iteration 5589, loss = 0.93033556\n",
            "Iteration 5590, loss = 0.93026544\n",
            "Iteration 5591, loss = 0.93019534\n",
            "Iteration 5592, loss = 0.93012526\n",
            "Iteration 5593, loss = 0.93005518\n",
            "Iteration 5594, loss = 0.92998512\n",
            "Iteration 5595, loss = 0.92991508\n",
            "Iteration 5596, loss = 0.92984505\n",
            "Iteration 5597, loss = 0.92977503\n",
            "Iteration 5598, loss = 0.92970502\n",
            "Iteration 5599, loss = 0.92963504\n",
            "Iteration 5600, loss = 0.92956506\n",
            "Iteration 5601, loss = 0.92949510\n",
            "Iteration 5602, loss = 0.92942515\n",
            "Iteration 5603, loss = 0.92935521\n",
            "Iteration 5604, loss = 0.92928529\n",
            "Iteration 5605, loss = 0.92921539\n",
            "Iteration 5606, loss = 0.92914550\n",
            "Iteration 5607, loss = 0.92907562\n",
            "Iteration 5608, loss = 0.92900575\n",
            "Iteration 5609, loss = 0.92893590\n",
            "Iteration 5610, loss = 0.92886606\n",
            "Iteration 5611, loss = 0.92879624\n",
            "Iteration 5612, loss = 0.92872643\n",
            "Iteration 5613, loss = 0.92865663\n",
            "Iteration 5614, loss = 0.92858685\n",
            "Iteration 5615, loss = 0.92851708\n",
            "Iteration 5616, loss = 0.92844733\n",
            "Iteration 5617, loss = 0.92837758\n",
            "Iteration 5618, loss = 0.92830786\n",
            "Iteration 5619, loss = 0.92823814\n",
            "Iteration 5620, loss = 0.92816844\n",
            "Iteration 5621, loss = 0.92809876\n",
            "Iteration 5622, loss = 0.92802908\n",
            "Iteration 5623, loss = 0.92795942\n",
            "Iteration 5624, loss = 0.92788978\n",
            "Iteration 5625, loss = 0.92782015\n",
            "Iteration 5626, loss = 0.92775053\n",
            "Iteration 5627, loss = 0.92768093\n",
            "Iteration 5628, loss = 0.92761133\n",
            "Iteration 5629, loss = 0.92754176\n",
            "Iteration 5630, loss = 0.92747219\n",
            "Iteration 5631, loss = 0.92740264\n",
            "Iteration 5632, loss = 0.92733311\n",
            "Iteration 5633, loss = 0.92726359\n",
            "Iteration 5634, loss = 0.92719408\n",
            "Iteration 5635, loss = 0.92712458\n",
            "Iteration 5636, loss = 0.92705510\n",
            "Iteration 5637, loss = 0.92698563\n",
            "Iteration 5638, loss = 0.92691618\n",
            "Iteration 5639, loss = 0.92684673\n",
            "Iteration 5640, loss = 0.92677731\n",
            "Iteration 5641, loss = 0.92670789\n",
            "Iteration 5642, loss = 0.92663849\n",
            "Iteration 5643, loss = 0.92656911\n",
            "Iteration 5644, loss = 0.92649973\n",
            "Iteration 5645, loss = 0.92643037\n",
            "Iteration 5646, loss = 0.92636103\n",
            "Iteration 5647, loss = 0.92629169\n",
            "Iteration 5648, loss = 0.92622237\n",
            "Iteration 5649, loss = 0.92615307\n",
            "Iteration 5650, loss = 0.92608378\n",
            "Iteration 5651, loss = 0.92601450\n",
            "Iteration 5652, loss = 0.92594523\n",
            "Iteration 5653, loss = 0.92587598\n",
            "Iteration 5654, loss = 0.92580674\n",
            "Iteration 5655, loss = 0.92573751\n",
            "Iteration 5656, loss = 0.92566830\n",
            "Iteration 5657, loss = 0.92559910\n",
            "Iteration 5658, loss = 0.92552992\n",
            "Iteration 5659, loss = 0.92546075\n",
            "Iteration 5660, loss = 0.92539159\n",
            "Iteration 5661, loss = 0.92532245\n",
            "Iteration 5662, loss = 0.92525331\n",
            "Iteration 5663, loss = 0.92518420\n",
            "Iteration 5664, loss = 0.92511509\n",
            "Iteration 5665, loss = 0.92504600\n",
            "Iteration 5666, loss = 0.92497692\n",
            "Iteration 5667, loss = 0.92490786\n",
            "Iteration 5668, loss = 0.92483881\n",
            "Iteration 5669, loss = 0.92476977\n",
            "Iteration 5670, loss = 0.92470075\n",
            "Iteration 5671, loss = 0.92463174\n",
            "Iteration 5672, loss = 0.92456274\n",
            "Iteration 5673, loss = 0.92449375\n",
            "Iteration 5674, loss = 0.92442478\n",
            "Iteration 5675, loss = 0.92435583\n",
            "Iteration 5676, loss = 0.92428688\n",
            "Iteration 5677, loss = 0.92421795\n",
            "Iteration 5678, loss = 0.92414903\n",
            "Iteration 5679, loss = 0.92408013\n",
            "Iteration 5680, loss = 0.92401124\n",
            "Iteration 5681, loss = 0.92394236\n",
            "Iteration 5682, loss = 0.92387349\n",
            "Iteration 5683, loss = 0.92380464\n",
            "Iteration 5684, loss = 0.92373580\n",
            "Iteration 5685, loss = 0.92366698\n",
            "Iteration 5686, loss = 0.92359817\n",
            "Iteration 5687, loss = 0.92352937\n",
            "Iteration 5688, loss = 0.92346058\n",
            "Iteration 5689, loss = 0.92339181\n",
            "Iteration 5690, loss = 0.92332305\n",
            "Iteration 5691, loss = 0.92325431\n",
            "Iteration 5692, loss = 0.92318558\n",
            "Iteration 5693, loss = 0.92311686\n",
            "Iteration 5694, loss = 0.92304815\n",
            "Iteration 5695, loss = 0.92297946\n",
            "Iteration 5696, loss = 0.92291078\n",
            "Iteration 5697, loss = 0.92284211\n",
            "Iteration 5698, loss = 0.92277346\n",
            "Iteration 5699, loss = 0.92270482\n",
            "Iteration 5700, loss = 0.92263619\n",
            "Iteration 5701, loss = 0.92256758\n",
            "Iteration 5702, loss = 0.92249898\n",
            "Iteration 5703, loss = 0.92243039\n",
            "Iteration 5704, loss = 0.92236182\n",
            "Iteration 5705, loss = 0.92229325\n",
            "Iteration 5706, loss = 0.92222471\n",
            "Iteration 5707, loss = 0.92215617\n",
            "Iteration 5708, loss = 0.92208765\n",
            "Iteration 5709, loss = 0.92201914\n",
            "Iteration 5710, loss = 0.92195064\n",
            "Iteration 5711, loss = 0.92188216\n",
            "Iteration 5712, loss = 0.92181369\n",
            "Iteration 5713, loss = 0.92174524\n",
            "Iteration 5714, loss = 0.92167679\n",
            "Iteration 5715, loss = 0.92160836\n",
            "Iteration 5716, loss = 0.92153994\n",
            "Iteration 5717, loss = 0.92147154\n",
            "Iteration 5718, loss = 0.92140315\n",
            "Iteration 5719, loss = 0.92133477\n",
            "Iteration 5720, loss = 0.92126641\n",
            "Iteration 5721, loss = 0.92119805\n",
            "Iteration 5722, loss = 0.92112971\n",
            "Iteration 5723, loss = 0.92106139\n",
            "Iteration 5724, loss = 0.92099308\n",
            "Iteration 5725, loss = 0.92092478\n",
            "Iteration 5726, loss = 0.92085649\n",
            "Iteration 5727, loss = 0.92078821\n",
            "Iteration 5728, loss = 0.92071995\n",
            "Iteration 5729, loss = 0.92065171\n",
            "Iteration 5730, loss = 0.92058347\n",
            "Iteration 5731, loss = 0.92051525\n",
            "Iteration 5732, loss = 0.92044704\n",
            "Iteration 5733, loss = 0.92037884\n",
            "Iteration 5734, loss = 0.92031066\n",
            "Iteration 5735, loss = 0.92024249\n",
            "Iteration 5736, loss = 0.92017433\n",
            "Iteration 5737, loss = 0.92010619\n",
            "Iteration 5738, loss = 0.92003806\n",
            "Iteration 5739, loss = 0.91996994\n",
            "Iteration 5740, loss = 0.91990183\n",
            "Iteration 5741, loss = 0.91983374\n",
            "Iteration 5742, loss = 0.91976566\n",
            "Iteration 5743, loss = 0.91969759\n",
            "Iteration 5744, loss = 0.91962954\n",
            "Iteration 5745, loss = 0.91956150\n",
            "Iteration 5746, loss = 0.91949347\n",
            "Iteration 5747, loss = 0.91942545\n",
            "Iteration 5748, loss = 0.91935745\n",
            "Iteration 5749, loss = 0.91928946\n",
            "Iteration 5750, loss = 0.91922149\n",
            "Iteration 5751, loss = 0.91915352\n",
            "Iteration 5752, loss = 0.91908557\n",
            "Iteration 5753, loss = 0.91901763\n",
            "Iteration 5754, loss = 0.91894971\n",
            "Iteration 5755, loss = 0.91888180\n",
            "Iteration 5756, loss = 0.91881390\n",
            "Iteration 5757, loss = 0.91874601\n",
            "Iteration 5758, loss = 0.91867814\n",
            "Iteration 5759, loss = 0.91861028\n",
            "Iteration 5760, loss = 0.91854243\n",
            "Iteration 5761, loss = 0.91847459\n",
            "Iteration 5762, loss = 0.91840677\n",
            "Iteration 5763, loss = 0.91833896\n",
            "Iteration 5764, loss = 0.91827116\n",
            "Iteration 5765, loss = 0.91820338\n",
            "Iteration 5766, loss = 0.91813561\n",
            "Iteration 5767, loss = 0.91806785\n",
            "Iteration 5768, loss = 0.91800010\n",
            "Iteration 5769, loss = 0.91793237\n",
            "Iteration 5770, loss = 0.91786465\n",
            "Iteration 5771, loss = 0.91779694\n",
            "Iteration 5772, loss = 0.91772925\n",
            "Iteration 5773, loss = 0.91766157\n",
            "Iteration 5774, loss = 0.91759390\n",
            "Iteration 5775, loss = 0.91752624\n",
            "Iteration 5776, loss = 0.91745860\n",
            "Iteration 5777, loss = 0.91739097\n",
            "Iteration 5778, loss = 0.91732335\n",
            "Iteration 5779, loss = 0.91725575\n",
            "Iteration 5780, loss = 0.91718815\n",
            "Iteration 5781, loss = 0.91712057\n",
            "Iteration 5782, loss = 0.91705301\n",
            "Iteration 5783, loss = 0.91698545\n",
            "Iteration 5784, loss = 0.91691791\n",
            "Iteration 5785, loss = 0.91685038\n",
            "Iteration 5786, loss = 0.91678287\n",
            "Iteration 5787, loss = 0.91671536\n",
            "Iteration 5788, loss = 0.91664787\n",
            "Iteration 5789, loss = 0.91658039\n",
            "Iteration 5790, loss = 0.91651293\n",
            "Iteration 5791, loss = 0.91644547\n",
            "Iteration 5792, loss = 0.91637803\n",
            "Iteration 5793, loss = 0.91631061\n",
            "Iteration 5794, loss = 0.91624319\n",
            "Iteration 5795, loss = 0.91617579\n",
            "Iteration 5796, loss = 0.91610840\n",
            "Iteration 5797, loss = 0.91604102\n",
            "Iteration 5798, loss = 0.91597366\n",
            "Iteration 5799, loss = 0.91590631\n",
            "Iteration 5800, loss = 0.91583897\n",
            "Iteration 5801, loss = 0.91577164\n",
            "Iteration 5802, loss = 0.91570433\n",
            "Iteration 5803, loss = 0.91563703\n",
            "Iteration 5804, loss = 0.91556974\n",
            "Iteration 5805, loss = 0.91550247\n",
            "Iteration 5806, loss = 0.91543520\n",
            "Iteration 5807, loss = 0.91536795\n",
            "Iteration 5808, loss = 0.91530071\n",
            "Iteration 5809, loss = 0.91523349\n",
            "Iteration 5810, loss = 0.91516628\n",
            "Iteration 5811, loss = 0.91509908\n",
            "Iteration 5812, loss = 0.91503189\n",
            "Iteration 5813, loss = 0.91496472\n",
            "Iteration 5814, loss = 0.91489755\n",
            "Iteration 5815, loss = 0.91483040\n",
            "Iteration 5816, loss = 0.91476327\n",
            "Iteration 5817, loss = 0.91469614\n",
            "Iteration 5818, loss = 0.91462903\n",
            "Iteration 5819, loss = 0.91456193\n",
            "Iteration 5820, loss = 0.91449484\n",
            "Iteration 5821, loss = 0.91442777\n",
            "Iteration 5822, loss = 0.91436071\n",
            "Iteration 5823, loss = 0.91429366\n",
            "Iteration 5824, loss = 0.91422662\n",
            "Iteration 5825, loss = 0.91415960\n",
            "Iteration 5826, loss = 0.91409259\n",
            "Iteration 5827, loss = 0.91402559\n",
            "Iteration 5828, loss = 0.91395860\n",
            "Iteration 5829, loss = 0.91389163\n",
            "Iteration 5830, loss = 0.91382467\n",
            "Iteration 5831, loss = 0.91375772\n",
            "Iteration 5832, loss = 0.91369078\n",
            "Iteration 5833, loss = 0.91362386\n",
            "Iteration 5834, loss = 0.91355695\n",
            "Iteration 5835, loss = 0.91349005\n",
            "Iteration 5836, loss = 0.91342316\n",
            "Iteration 5837, loss = 0.91335629\n",
            "Iteration 5838, loss = 0.91328943\n",
            "Iteration 5839, loss = 0.91322258\n",
            "Iteration 5840, loss = 0.91315574\n",
            "Iteration 5841, loss = 0.91308892\n",
            "Iteration 5842, loss = 0.91302211\n",
            "Iteration 5843, loss = 0.91295531\n",
            "Iteration 5844, loss = 0.91288852\n",
            "Iteration 5845, loss = 0.91282175\n",
            "Iteration 5846, loss = 0.91275499\n",
            "Iteration 5847, loss = 0.91268824\n",
            "Iteration 5848, loss = 0.91262150\n",
            "Iteration 5849, loss = 0.91255478\n",
            "Iteration 5850, loss = 0.91248807\n",
            "Iteration 5851, loss = 0.91242137\n",
            "Iteration 5852, loss = 0.91235468\n",
            "Iteration 5853, loss = 0.91228801\n",
            "Iteration 5854, loss = 0.91222135\n",
            "Iteration 5855, loss = 0.91215470\n",
            "Iteration 5856, loss = 0.91208806\n",
            "Iteration 5857, loss = 0.91202144\n",
            "Iteration 5858, loss = 0.91195482\n",
            "Iteration 5859, loss = 0.91188823\n",
            "Iteration 5860, loss = 0.91182164\n",
            "Iteration 5861, loss = 0.91175506\n",
            "Iteration 5862, loss = 0.91168850\n",
            "Iteration 5863, loss = 0.91162195\n",
            "Iteration 5864, loss = 0.91155541\n",
            "Iteration 5865, loss = 0.91148889\n",
            "Iteration 5866, loss = 0.91142238\n",
            "Iteration 5867, loss = 0.91135588\n",
            "Iteration 5868, loss = 0.91128939\n",
            "Iteration 5869, loss = 0.91122291\n",
            "Iteration 5870, loss = 0.91115645\n",
            "Iteration 5871, loss = 0.91109000\n",
            "Iteration 5872, loss = 0.91102356\n",
            "Iteration 5873, loss = 0.91095714\n",
            "Iteration 5874, loss = 0.91089072\n",
            "Iteration 5875, loss = 0.91082432\n",
            "Iteration 5876, loss = 0.91075793\n",
            "Iteration 5877, loss = 0.91069156\n",
            "Iteration 5878, loss = 0.91062519\n",
            "Iteration 5879, loss = 0.91055884\n",
            "Iteration 5880, loss = 0.91049250\n",
            "Iteration 5881, loss = 0.91042617\n",
            "Iteration 5882, loss = 0.91035986\n",
            "Iteration 5883, loss = 0.91029356\n",
            "Iteration 5884, loss = 0.91022727\n",
            "Iteration 5885, loss = 0.91016099\n",
            "Iteration 5886, loss = 0.91009472\n",
            "Iteration 5887, loss = 0.91002847\n",
            "Iteration 5888, loss = 0.90996223\n",
            "Iteration 5889, loss = 0.90989600\n",
            "Iteration 5890, loss = 0.90982979\n",
            "Iteration 5891, loss = 0.90976358\n",
            "Iteration 5892, loss = 0.90969739\n",
            "Iteration 5893, loss = 0.90963121\n",
            "Iteration 5894, loss = 0.90956505\n",
            "Iteration 5895, loss = 0.90949889\n",
            "Iteration 5896, loss = 0.90943275\n",
            "Iteration 5897, loss = 0.90936662\n",
            "Iteration 5898, loss = 0.90930050\n",
            "Iteration 5899, loss = 0.90923440\n",
            "Iteration 5900, loss = 0.90916830\n",
            "Iteration 5901, loss = 0.90910222\n",
            "Iteration 5902, loss = 0.90903615\n",
            "Iteration 5903, loss = 0.90897010\n",
            "Iteration 5904, loss = 0.90890406\n",
            "Iteration 5905, loss = 0.90883802\n",
            "Iteration 5906, loss = 0.90877200\n",
            "Iteration 5907, loss = 0.90870600\n",
            "Iteration 5908, loss = 0.90864000\n",
            "Iteration 5909, loss = 0.90857402\n",
            "Iteration 5910, loss = 0.90850805\n",
            "Iteration 5911, loss = 0.90844209\n",
            "Iteration 5912, loss = 0.90837615\n",
            "Iteration 5913, loss = 0.90831021\n",
            "Iteration 5914, loss = 0.90824429\n",
            "Iteration 5915, loss = 0.90817838\n",
            "Iteration 5916, loss = 0.90811249\n",
            "Iteration 5917, loss = 0.90804660\n",
            "Iteration 5918, loss = 0.90798073\n",
            "Iteration 5919, loss = 0.90791487\n",
            "Iteration 5920, loss = 0.90784902\n",
            "Iteration 5921, loss = 0.90778319\n",
            "Iteration 5922, loss = 0.90771737\n",
            "Iteration 5923, loss = 0.90765156\n",
            "Iteration 5924, loss = 0.90758576\n",
            "Iteration 5925, loss = 0.90751997\n",
            "Iteration 5926, loss = 0.90745420\n",
            "Iteration 5927, loss = 0.90738843\n",
            "Iteration 5928, loss = 0.90732268\n",
            "Iteration 5929, loss = 0.90725695\n",
            "Iteration 5930, loss = 0.90719122\n",
            "Iteration 5931, loss = 0.90712551\n",
            "Iteration 5932, loss = 0.90705981\n",
            "Iteration 5933, loss = 0.90699412\n",
            "Iteration 5934, loss = 0.90692844\n",
            "Iteration 5935, loss = 0.90686278\n",
            "Iteration 5936, loss = 0.90679713\n",
            "Iteration 5937, loss = 0.90673149\n",
            "Iteration 5938, loss = 0.90666586\n",
            "Iteration 5939, loss = 0.90660024\n",
            "Iteration 5940, loss = 0.90653464\n",
            "Iteration 5941, loss = 0.90646905\n",
            "Iteration 5942, loss = 0.90640347\n",
            "Iteration 5943, loss = 0.90633790\n",
            "Iteration 5944, loss = 0.90627235\n",
            "Iteration 5945, loss = 0.90620681\n",
            "Iteration 5946, loss = 0.90614128\n",
            "Iteration 5947, loss = 0.90607576\n",
            "Iteration 5948, loss = 0.90601025\n",
            "Iteration 5949, loss = 0.90594476\n",
            "Iteration 5950, loss = 0.90587928\n",
            "Iteration 5951, loss = 0.90581381\n",
            "Iteration 5952, loss = 0.90574835\n",
            "Iteration 5953, loss = 0.90568290\n",
            "Iteration 5954, loss = 0.90561747\n",
            "Iteration 5955, loss = 0.90555205\n",
            "Iteration 5956, loss = 0.90548664\n",
            "Iteration 5957, loss = 0.90542125\n",
            "Iteration 5958, loss = 0.90535586\n",
            "Iteration 5959, loss = 0.90529049\n",
            "Iteration 5960, loss = 0.90522513\n",
            "Iteration 5961, loss = 0.90515978\n",
            "Iteration 5962, loss = 0.90509444\n",
            "Iteration 5963, loss = 0.90502912\n",
            "Iteration 5964, loss = 0.90496381\n",
            "Iteration 5965, loss = 0.90489851\n",
            "Iteration 5966, loss = 0.90483322\n",
            "Iteration 5967, loss = 0.90476795\n",
            "Iteration 5968, loss = 0.90470268\n",
            "Iteration 5969, loss = 0.90463743\n",
            "Iteration 5970, loss = 0.90457219\n",
            "Iteration 5971, loss = 0.90450697\n",
            "Iteration 5972, loss = 0.90444175\n",
            "Iteration 5973, loss = 0.90437655\n",
            "Iteration 5974, loss = 0.90431136\n",
            "Iteration 5975, loss = 0.90424618\n",
            "Iteration 5976, loss = 0.90418102\n",
            "Iteration 5977, loss = 0.90411586\n",
            "Iteration 5978, loss = 0.90405072\n",
            "Iteration 5979, loss = 0.90398559\n",
            "Iteration 5980, loss = 0.90392047\n",
            "Iteration 5981, loss = 0.90385537\n",
            "Iteration 5982, loss = 0.90379027\n",
            "Iteration 5983, loss = 0.90372519\n",
            "Iteration 5984, loss = 0.90366012\n",
            "Iteration 5985, loss = 0.90359506\n",
            "Iteration 5986, loss = 0.90353002\n",
            "Iteration 5987, loss = 0.90346498\n",
            "Iteration 5988, loss = 0.90339996\n",
            "Iteration 5989, loss = 0.90333495\n",
            "Iteration 5990, loss = 0.90326996\n",
            "Iteration 5991, loss = 0.90320497\n",
            "Iteration 5992, loss = 0.90314000\n",
            "Iteration 5993, loss = 0.90307504\n",
            "Iteration 5994, loss = 0.90301009\n",
            "Iteration 5995, loss = 0.90294515\n",
            "Iteration 5996, loss = 0.90288023\n",
            "Iteration 5997, loss = 0.90281531\n",
            "Iteration 5998, loss = 0.90275041\n",
            "Iteration 5999, loss = 0.90268552\n",
            "Iteration 6000, loss = 0.90262065\n",
            "Iteration 6001, loss = 0.90255578\n",
            "Iteration 6002, loss = 0.90249093\n",
            "Iteration 6003, loss = 0.90242609\n",
            "Iteration 6004, loss = 0.90236126\n",
            "Iteration 6005, loss = 0.90229644\n",
            "Iteration 6006, loss = 0.90223164\n",
            "Iteration 6007, loss = 0.90216685\n",
            "Iteration 6008, loss = 0.90210207\n",
            "Iteration 6009, loss = 0.90203730\n",
            "Iteration 6010, loss = 0.90197254\n",
            "Iteration 6011, loss = 0.90190780\n",
            "Iteration 6012, loss = 0.90184307\n",
            "Iteration 6013, loss = 0.90177835\n",
            "Iteration 6014, loss = 0.90171364\n",
            "Iteration 6015, loss = 0.90164894\n",
            "Iteration 6016, loss = 0.90158426\n",
            "Iteration 6017, loss = 0.90151958\n",
            "Iteration 6018, loss = 0.90145492\n",
            "Iteration 6019, loss = 0.90139028\n",
            "Iteration 6020, loss = 0.90132564\n",
            "Iteration 6021, loss = 0.90126101\n",
            "Iteration 6022, loss = 0.90119640\n",
            "Iteration 6023, loss = 0.90113180\n",
            "Iteration 6024, loss = 0.90106721\n",
            "Iteration 6025, loss = 0.90100264\n",
            "Iteration 6026, loss = 0.90093807\n",
            "Iteration 6027, loss = 0.90087352\n",
            "Iteration 6028, loss = 0.90080898\n",
            "Iteration 6029, loss = 0.90074445\n",
            "Iteration 6030, loss = 0.90067994\n",
            "Iteration 6031, loss = 0.90061543\n",
            "Iteration 6032, loss = 0.90055094\n",
            "Iteration 6033, loss = 0.90048646\n",
            "Iteration 6034, loss = 0.90042199\n",
            "Iteration 6035, loss = 0.90035753\n",
            "Iteration 6036, loss = 0.90029309\n",
            "Iteration 6037, loss = 0.90022866\n",
            "Iteration 6038, loss = 0.90016423\n",
            "Iteration 6039, loss = 0.90009983\n",
            "Iteration 6040, loss = 0.90003543\n",
            "Iteration 6041, loss = 0.89997104\n",
            "Iteration 6042, loss = 0.89990667\n",
            "Iteration 6043, loss = 0.89984231\n",
            "Iteration 6044, loss = 0.89977796\n",
            "Iteration 6045, loss = 0.89971362\n",
            "Iteration 6046, loss = 0.89964930\n",
            "Iteration 6047, loss = 0.89958499\n",
            "Iteration 6048, loss = 0.89952069\n",
            "Iteration 6049, loss = 0.89945640\n",
            "Iteration 6050, loss = 0.89939212\n",
            "Iteration 6051, loss = 0.89932785\n",
            "Iteration 6052, loss = 0.89926360\n",
            "Iteration 6053, loss = 0.89919936\n",
            "Iteration 6054, loss = 0.89913513\n",
            "Iteration 6055, loss = 0.89907091\n",
            "Iteration 6056, loss = 0.89900670\n",
            "Iteration 6057, loss = 0.89894251\n",
            "Iteration 6058, loss = 0.89887833\n",
            "Iteration 6059, loss = 0.89881416\n",
            "Iteration 6060, loss = 0.89875000\n",
            "Iteration 6061, loss = 0.89868585\n",
            "Iteration 6062, loss = 0.89862172\n",
            "Iteration 6063, loss = 0.89855760\n",
            "Iteration 6064, loss = 0.89849349\n",
            "Iteration 6065, loss = 0.89842939\n",
            "Iteration 6066, loss = 0.89836530\n",
            "Iteration 6067, loss = 0.89830123\n",
            "Iteration 6068, loss = 0.89823716\n",
            "Iteration 6069, loss = 0.89817311\n",
            "Iteration 6070, loss = 0.89810907\n",
            "Iteration 6071, loss = 0.89804505\n",
            "Iteration 6072, loss = 0.89798103\n",
            "Iteration 6073, loss = 0.89791703\n",
            "Iteration 6074, loss = 0.89785304\n",
            "Iteration 6075, loss = 0.89778906\n",
            "Iteration 6076, loss = 0.89772509\n",
            "Iteration 6077, loss = 0.89766113\n",
            "Iteration 6078, loss = 0.89759719\n",
            "Iteration 6079, loss = 0.89753326\n",
            "Iteration 6080, loss = 0.89746934\n",
            "Iteration 6081, loss = 0.89740543\n",
            "Iteration 6082, loss = 0.89734153\n",
            "Iteration 6083, loss = 0.89727765\n",
            "Iteration 6084, loss = 0.89721377\n",
            "Iteration 6085, loss = 0.89714991\n",
            "Iteration 6086, loss = 0.89708606\n",
            "Iteration 6087, loss = 0.89702223\n",
            "Iteration 6088, loss = 0.89695840\n",
            "Iteration 6089, loss = 0.89689459\n",
            "Iteration 6090, loss = 0.89683078\n",
            "Iteration 6091, loss = 0.89676700\n",
            "Iteration 6092, loss = 0.89670322\n",
            "Iteration 6093, loss = 0.89663945\n",
            "Iteration 6094, loss = 0.89657570\n",
            "Iteration 6095, loss = 0.89651195\n",
            "Iteration 6096, loss = 0.89644822\n",
            "Iteration 6097, loss = 0.89638450\n",
            "Iteration 6098, loss = 0.89632080\n",
            "Iteration 6099, loss = 0.89625710\n",
            "Iteration 6100, loss = 0.89619342\n",
            "Iteration 6101, loss = 0.89612975\n",
            "Iteration 6102, loss = 0.89606609\n",
            "Iteration 6103, loss = 0.89600244\n",
            "Iteration 6104, loss = 0.89593880\n",
            "Iteration 6105, loss = 0.89587518\n",
            "Iteration 6106, loss = 0.89581157\n",
            "Iteration 6107, loss = 0.89574797\n",
            "Iteration 6108, loss = 0.89568438\n",
            "Iteration 6109, loss = 0.89562080\n",
            "Iteration 6110, loss = 0.89555724\n",
            "Iteration 6111, loss = 0.89549368\n",
            "Iteration 6112, loss = 0.89543014\n",
            "Iteration 6113, loss = 0.89536661\n",
            "Iteration 6114, loss = 0.89530309\n",
            "Iteration 6115, loss = 0.89523959\n",
            "Iteration 6116, loss = 0.89517609\n",
            "Iteration 6117, loss = 0.89511261\n",
            "Iteration 6118, loss = 0.89504914\n",
            "Iteration 6119, loss = 0.89498568\n",
            "Iteration 6120, loss = 0.89492223\n",
            "Iteration 6121, loss = 0.89485880\n",
            "Iteration 6122, loss = 0.89479537\n",
            "Iteration 6123, loss = 0.89473196\n",
            "Iteration 6124, loss = 0.89466856\n",
            "Iteration 6125, loss = 0.89460517\n",
            "Iteration 6126, loss = 0.89454180\n",
            "Iteration 6127, loss = 0.89447843\n",
            "Iteration 6128, loss = 0.89441508\n",
            "Iteration 6129, loss = 0.89435174\n",
            "Iteration 6130, loss = 0.89428841\n",
            "Iteration 6131, loss = 0.89422509\n",
            "Iteration 6132, loss = 0.89416178\n",
            "Iteration 6133, loss = 0.89409849\n",
            "Iteration 6134, loss = 0.89403521\n",
            "Iteration 6135, loss = 0.89397194\n",
            "Iteration 6136, loss = 0.89390868\n",
            "Iteration 6137, loss = 0.89384543\n",
            "Iteration 6138, loss = 0.89378220\n",
            "Iteration 6139, loss = 0.89371897\n",
            "Iteration 6140, loss = 0.89365576\n",
            "Iteration 6141, loss = 0.89359256\n",
            "Iteration 6142, loss = 0.89352937\n",
            "Iteration 6143, loss = 0.89346619\n",
            "Iteration 6144, loss = 0.89340303\n",
            "Iteration 6145, loss = 0.89333988\n",
            "Iteration 6146, loss = 0.89327674\n",
            "Iteration 6147, loss = 0.89321361\n",
            "Iteration 6148, loss = 0.89315049\n",
            "Iteration 6149, loss = 0.89308738\n",
            "Iteration 6150, loss = 0.89302429\n",
            "Iteration 6151, loss = 0.89296120\n",
            "Iteration 6152, loss = 0.89289813\n",
            "Iteration 6153, loss = 0.89283507\n",
            "Iteration 6154, loss = 0.89277203\n",
            "Iteration 6155, loss = 0.89270899\n",
            "Iteration 6156, loss = 0.89264597\n",
            "Iteration 6157, loss = 0.89258295\n",
            "Iteration 6158, loss = 0.89251995\n",
            "Iteration 6159, loss = 0.89245696\n",
            "Iteration 6160, loss = 0.89239398\n",
            "Iteration 6161, loss = 0.89233102\n",
            "Iteration 6162, loss = 0.89226807\n",
            "Iteration 6163, loss = 0.89220512\n",
            "Iteration 6164, loss = 0.89214219\n",
            "Iteration 6165, loss = 0.89207927\n",
            "Iteration 6166, loss = 0.89201637\n",
            "Iteration 6167, loss = 0.89195347\n",
            "Iteration 6168, loss = 0.89189059\n",
            "Iteration 6169, loss = 0.89182771\n",
            "Iteration 6170, loss = 0.89176485\n",
            "Iteration 6171, loss = 0.89170200\n",
            "Iteration 6172, loss = 0.89163917\n",
            "Iteration 6173, loss = 0.89157634\n",
            "Iteration 6174, loss = 0.89151353\n",
            "Iteration 6175, loss = 0.89145073\n",
            "Iteration 6176, loss = 0.89138793\n",
            "Iteration 6177, loss = 0.89132516\n",
            "Iteration 6178, loss = 0.89126239\n",
            "Iteration 6179, loss = 0.89119963\n",
            "Iteration 6180, loss = 0.89113689\n",
            "Iteration 6181, loss = 0.89107416\n",
            "Iteration 6182, loss = 0.89101144\n",
            "Iteration 6183, loss = 0.89094873\n",
            "Iteration 6184, loss = 0.89088603\n",
            "Iteration 6185, loss = 0.89082334\n",
            "Iteration 6186, loss = 0.89076067\n",
            "Iteration 6187, loss = 0.89069801\n",
            "Iteration 6188, loss = 0.89063536\n",
            "Iteration 6189, loss = 0.89057272\n",
            "Iteration 6190, loss = 0.89051009\n",
            "Iteration 6191, loss = 0.89044747\n",
            "Iteration 6192, loss = 0.89038487\n",
            "Iteration 6193, loss = 0.89032228\n",
            "Iteration 6194, loss = 0.89025970\n",
            "Iteration 6195, loss = 0.89019713\n",
            "Iteration 6196, loss = 0.89013457\n",
            "Iteration 6197, loss = 0.89007202\n",
            "Iteration 6198, loss = 0.89000949\n",
            "Iteration 6199, loss = 0.88994697\n",
            "Iteration 6200, loss = 0.88988446\n",
            "Iteration 6201, loss = 0.88982196\n",
            "Iteration 6202, loss = 0.88975947\n",
            "Iteration 6203, loss = 0.88969699\n",
            "Iteration 6204, loss = 0.88963453\n",
            "Iteration 6205, loss = 0.88957207\n",
            "Iteration 6206, loss = 0.88950963\n",
            "Iteration 6207, loss = 0.88944720\n",
            "Iteration 6208, loss = 0.88938478\n",
            "Iteration 6209, loss = 0.88932238\n",
            "Iteration 6210, loss = 0.88925998\n",
            "Iteration 6211, loss = 0.88919760\n",
            "Iteration 6212, loss = 0.88913522\n",
            "Iteration 6213, loss = 0.88907286\n",
            "Iteration 6214, loss = 0.88901052\n",
            "Iteration 6215, loss = 0.88894818\n",
            "Iteration 6216, loss = 0.88888585\n",
            "Iteration 6217, loss = 0.88882354\n",
            "Iteration 6218, loss = 0.88876124\n",
            "Iteration 6219, loss = 0.88869894\n",
            "Iteration 6220, loss = 0.88863666\n",
            "Iteration 6221, loss = 0.88857440\n",
            "Iteration 6222, loss = 0.88851214\n",
            "Iteration 6223, loss = 0.88844989\n",
            "Iteration 6224, loss = 0.88838766\n",
            "Iteration 6225, loss = 0.88832544\n",
            "Iteration 6226, loss = 0.88826323\n",
            "Iteration 6227, loss = 0.88820103\n",
            "Iteration 6228, loss = 0.88813884\n",
            "Iteration 6229, loss = 0.88807667\n",
            "Iteration 6230, loss = 0.88801450\n",
            "Iteration 6231, loss = 0.88795235\n",
            "Iteration 6232, loss = 0.88789021\n",
            "Iteration 6233, loss = 0.88782808\n",
            "Iteration 6234, loss = 0.88776596\n",
            "Iteration 6235, loss = 0.88770386\n",
            "Iteration 6236, loss = 0.88764176\n",
            "Iteration 6237, loss = 0.88757968\n",
            "Iteration 6238, loss = 0.88751761\n",
            "Iteration 6239, loss = 0.88745555\n",
            "Iteration 6240, loss = 0.88739350\n",
            "Iteration 6241, loss = 0.88733146\n",
            "Iteration 6242, loss = 0.88726943\n",
            "Iteration 6243, loss = 0.88720742\n",
            "Iteration 6244, loss = 0.88714542\n",
            "Iteration 6245, loss = 0.88708343\n",
            "Iteration 6246, loss = 0.88702145\n",
            "Iteration 6247, loss = 0.88695948\n",
            "Iteration 6248, loss = 0.88689752\n",
            "Iteration 6249, loss = 0.88683558\n",
            "Iteration 6250, loss = 0.88677364\n",
            "Iteration 6251, loss = 0.88671172\n",
            "Iteration 6252, loss = 0.88664981\n",
            "Iteration 6253, loss = 0.88658791\n",
            "Iteration 6254, loss = 0.88652602\n",
            "Iteration 6255, loss = 0.88646414\n",
            "Iteration 6256, loss = 0.88640228\n",
            "Iteration 6257, loss = 0.88634043\n",
            "Iteration 6258, loss = 0.88627858\n",
            "Iteration 6259, loss = 0.88621675\n",
            "Iteration 6260, loss = 0.88615493\n",
            "Iteration 6261, loss = 0.88609313\n",
            "Iteration 6262, loss = 0.88603133\n",
            "Iteration 6263, loss = 0.88596954\n",
            "Iteration 6264, loss = 0.88590777\n",
            "Iteration 6265, loss = 0.88584601\n",
            "Iteration 6266, loss = 0.88578426\n",
            "Iteration 6267, loss = 0.88572252\n",
            "Iteration 6268, loss = 0.88566079\n",
            "Iteration 6269, loss = 0.88559908\n",
            "Iteration 6270, loss = 0.88553737\n",
            "Iteration 6271, loss = 0.88547568\n",
            "Iteration 6272, loss = 0.88541400\n",
            "Iteration 6273, loss = 0.88535233\n",
            "Iteration 6274, loss = 0.88529067\n",
            "Iteration 6275, loss = 0.88522902\n",
            "Iteration 6276, loss = 0.88516738\n",
            "Iteration 6277, loss = 0.88510576\n",
            "Iteration 6278, loss = 0.88504414\n",
            "Iteration 6279, loss = 0.88498254\n",
            "Iteration 6280, loss = 0.88492095\n",
            "Iteration 6281, loss = 0.88485937\n",
            "Iteration 6282, loss = 0.88479781\n",
            "Iteration 6283, loss = 0.88473625\n",
            "Iteration 6284, loss = 0.88467470\n",
            "Iteration 6285, loss = 0.88461317\n",
            "Iteration 6286, loss = 0.88455165\n",
            "Iteration 6287, loss = 0.88449014\n",
            "Iteration 6288, loss = 0.88442864\n",
            "Iteration 6289, loss = 0.88436715\n",
            "Iteration 6290, loss = 0.88430567\n",
            "Iteration 6291, loss = 0.88424421\n",
            "Iteration 6292, loss = 0.88418275\n",
            "Iteration 6293, loss = 0.88412131\n",
            "Iteration 6294, loss = 0.88405988\n",
            "Iteration 6295, loss = 0.88399846\n",
            "Iteration 6296, loss = 0.88393705\n",
            "Iteration 6297, loss = 0.88387565\n",
            "Iteration 6298, loss = 0.88381427\n",
            "Iteration 6299, loss = 0.88375289\n",
            "Iteration 6300, loss = 0.88369153\n",
            "Iteration 6301, loss = 0.88363018\n",
            "Iteration 6302, loss = 0.88356884\n",
            "Iteration 6303, loss = 0.88350751\n",
            "Iteration 6304, loss = 0.88344619\n",
            "Iteration 6305, loss = 0.88338489\n",
            "Iteration 6306, loss = 0.88332359\n",
            "Iteration 6307, loss = 0.88326231\n",
            "Iteration 6308, loss = 0.88320103\n",
            "Iteration 6309, loss = 0.88313977\n",
            "Iteration 6310, loss = 0.88307852\n",
            "Iteration 6311, loss = 0.88301729\n",
            "Iteration 6312, loss = 0.88295606\n",
            "Iteration 6313, loss = 0.88289484\n",
            "Iteration 6314, loss = 0.88283364\n",
            "Iteration 6315, loss = 0.88277244\n",
            "Iteration 6316, loss = 0.88271126\n",
            "Iteration 6317, loss = 0.88265009\n",
            "Iteration 6318, loss = 0.88258893\n",
            "Iteration 6319, loss = 0.88252778\n",
            "Iteration 6320, loss = 0.88246665\n",
            "Iteration 6321, loss = 0.88240552\n",
            "Iteration 6322, loss = 0.88234441\n",
            "Iteration 6323, loss = 0.88228331\n",
            "Iteration 6324, loss = 0.88222221\n",
            "Iteration 6325, loss = 0.88216113\n",
            "Iteration 6326, loss = 0.88210006\n",
            "Iteration 6327, loss = 0.88203901\n",
            "Iteration 6328, loss = 0.88197796\n",
            "Iteration 6329, loss = 0.88191692\n",
            "Iteration 6330, loss = 0.88185590\n",
            "Iteration 6331, loss = 0.88179489\n",
            "Iteration 6332, loss = 0.88173389\n",
            "Iteration 6333, loss = 0.88167290\n",
            "Iteration 6334, loss = 0.88161192\n",
            "Iteration 6335, loss = 0.88155095\n",
            "Iteration 6336, loss = 0.88148999\n",
            "Iteration 6337, loss = 0.88142905\n",
            "Iteration 6338, loss = 0.88136811\n",
            "Iteration 6339, loss = 0.88130719\n",
            "Iteration 6340, loss = 0.88124628\n",
            "Iteration 6341, loss = 0.88118538\n",
            "Iteration 6342, loss = 0.88112449\n",
            "Iteration 6343, loss = 0.88106361\n",
            "Iteration 6344, loss = 0.88100274\n",
            "Iteration 6345, loss = 0.88094189\n",
            "Iteration 6346, loss = 0.88088104\n",
            "Iteration 6347, loss = 0.88082021\n",
            "Iteration 6348, loss = 0.88075939\n",
            "Iteration 6349, loss = 0.88069858\n",
            "Iteration 6350, loss = 0.88063778\n",
            "Iteration 6351, loss = 0.88057699\n",
            "Iteration 6352, loss = 0.88051621\n",
            "Iteration 6353, loss = 0.88045544\n",
            "Iteration 6354, loss = 0.88039469\n",
            "Iteration 6355, loss = 0.88033394\n",
            "Iteration 6356, loss = 0.88027321\n",
            "Iteration 6357, loss = 0.88021249\n",
            "Iteration 6358, loss = 0.88015178\n",
            "Iteration 6359, loss = 0.88009108\n",
            "Iteration 6360, loss = 0.88003039\n",
            "Iteration 6361, loss = 0.87996972\n",
            "Iteration 6362, loss = 0.87990905\n",
            "Iteration 6363, loss = 0.87984840\n",
            "Iteration 6364, loss = 0.87978775\n",
            "Iteration 6365, loss = 0.87972712\n",
            "Iteration 6366, loss = 0.87966650\n",
            "Iteration 6367, loss = 0.87960589\n",
            "Iteration 6368, loss = 0.87954529\n",
            "Iteration 6369, loss = 0.87948470\n",
            "Iteration 6370, loss = 0.87942413\n",
            "Iteration 6371, loss = 0.87936356\n",
            "Iteration 6372, loss = 0.87930301\n",
            "Iteration 6373, loss = 0.87924246\n",
            "Iteration 6374, loss = 0.87918193\n",
            "Iteration 6375, loss = 0.87912141\n",
            "Iteration 6376, loss = 0.87906090\n",
            "Iteration 6377, loss = 0.87900040\n",
            "Iteration 6378, loss = 0.87893991\n",
            "Iteration 6379, loss = 0.87887944\n",
            "Iteration 6380, loss = 0.87881897\n",
            "Iteration 6381, loss = 0.87875851\n",
            "Iteration 6382, loss = 0.87869807\n",
            "Iteration 6383, loss = 0.87863764\n",
            "Iteration 6384, loss = 0.87857722\n",
            "Iteration 6385, loss = 0.87851681\n",
            "Iteration 6386, loss = 0.87845641\n",
            "Iteration 6387, loss = 0.87839602\n",
            "Iteration 6388, loss = 0.87833564\n",
            "Iteration 6389, loss = 0.87827528\n",
            "Iteration 6390, loss = 0.87821492\n",
            "Iteration 6391, loss = 0.87815458\n",
            "Iteration 6392, loss = 0.87809424\n",
            "Iteration 6393, loss = 0.87803392\n",
            "Iteration 6394, loss = 0.87797361\n",
            "Iteration 6395, loss = 0.87791331\n",
            "Iteration 6396, loss = 0.87785302\n",
            "Iteration 6397, loss = 0.87779275\n",
            "Iteration 6398, loss = 0.87773248\n",
            "Iteration 6399, loss = 0.87767222\n",
            "Iteration 6400, loss = 0.87761198\n",
            "Iteration 6401, loss = 0.87755174\n",
            "Iteration 6402, loss = 0.87749152\n",
            "Iteration 6403, loss = 0.87743131\n",
            "Iteration 6404, loss = 0.87737111\n",
            "Iteration 6405, loss = 0.87731092\n",
            "Iteration 6406, loss = 0.87725074\n",
            "Iteration 6407, loss = 0.87719057\n",
            "Iteration 6408, loss = 0.87713042\n",
            "Iteration 6409, loss = 0.87707027\n",
            "Iteration 6410, loss = 0.87701013\n",
            "Iteration 6411, loss = 0.87695001\n",
            "Iteration 6412, loss = 0.87688990\n",
            "Iteration 6413, loss = 0.87682980\n",
            "Iteration 6414, loss = 0.87676970\n",
            "Iteration 6415, loss = 0.87670962\n",
            "Iteration 6416, loss = 0.87664956\n",
            "Iteration 6417, loss = 0.87658950\n",
            "Iteration 6418, loss = 0.87652945\n",
            "Iteration 6419, loss = 0.87646941\n",
            "Iteration 6420, loss = 0.87640939\n",
            "Iteration 6421, loss = 0.87634937\n",
            "Iteration 6422, loss = 0.87628937\n",
            "Iteration 6423, loss = 0.87622938\n",
            "Iteration 6424, loss = 0.87616940\n",
            "Iteration 6425, loss = 0.87610943\n",
            "Iteration 6426, loss = 0.87604947\n",
            "Iteration 6427, loss = 0.87598952\n",
            "Iteration 6428, loss = 0.87592958\n",
            "Iteration 6429, loss = 0.87586965\n",
            "Iteration 6430, loss = 0.87580974\n",
            "Iteration 6431, loss = 0.87574983\n",
            "Iteration 6432, loss = 0.87568994\n",
            "Iteration 6433, loss = 0.87563005\n",
            "Iteration 6434, loss = 0.87557018\n",
            "Iteration 6435, loss = 0.87551032\n",
            "Iteration 6436, loss = 0.87545047\n",
            "Iteration 6437, loss = 0.87539063\n",
            "Iteration 6438, loss = 0.87533080\n",
            "Iteration 6439, loss = 0.87527098\n",
            "Iteration 6440, loss = 0.87521117\n",
            "Iteration 6441, loss = 0.87515137\n",
            "Iteration 6442, loss = 0.87509159\n",
            "Iteration 6443, loss = 0.87503181\n",
            "Iteration 6444, loss = 0.87497205\n",
            "Iteration 6445, loss = 0.87491230\n",
            "Iteration 6446, loss = 0.87485255\n",
            "Iteration 6447, loss = 0.87479282\n",
            "Iteration 6448, loss = 0.87473310\n",
            "Iteration 6449, loss = 0.87467339\n",
            "Iteration 6450, loss = 0.87461369\n",
            "Iteration 6451, loss = 0.87455400\n",
            "Iteration 6452, loss = 0.87449433\n",
            "Iteration 6453, loss = 0.87443466\n",
            "Iteration 6454, loss = 0.87437500\n",
            "Iteration 6455, loss = 0.87431536\n",
            "Iteration 6456, loss = 0.87425572\n",
            "Iteration 6457, loss = 0.87419610\n",
            "Iteration 6458, loss = 0.87413649\n",
            "Iteration 6459, loss = 0.87407688\n",
            "Iteration 6460, loss = 0.87401729\n",
            "Iteration 6461, loss = 0.87395771\n",
            "Iteration 6462, loss = 0.87389814\n",
            "Iteration 6463, loss = 0.87383858\n",
            "Iteration 6464, loss = 0.87377903\n",
            "Iteration 6465, loss = 0.87371950\n",
            "Iteration 6466, loss = 0.87365997\n",
            "Iteration 6467, loss = 0.87360045\n",
            "Iteration 6468, loss = 0.87354095\n",
            "Iteration 6469, loss = 0.87348145\n",
            "Iteration 6470, loss = 0.87342197\n",
            "Iteration 6471, loss = 0.87336250\n",
            "Iteration 6472, loss = 0.87330303\n",
            "Iteration 6473, loss = 0.87324358\n",
            "Iteration 6474, loss = 0.87318414\n",
            "Iteration 6475, loss = 0.87312471\n",
            "Iteration 6476, loss = 0.87306529\n",
            "Iteration 6477, loss = 0.87300588\n",
            "Iteration 6478, loss = 0.87294648\n",
            "Iteration 6479, loss = 0.87288710\n",
            "Iteration 6480, loss = 0.87282772\n",
            "Iteration 6481, loss = 0.87276835\n",
            "Iteration 6482, loss = 0.87270900\n",
            "Iteration 6483, loss = 0.87264965\n",
            "Iteration 6484, loss = 0.87259032\n",
            "Iteration 6485, loss = 0.87253099\n",
            "Iteration 6486, loss = 0.87247168\n",
            "Iteration 6487, loss = 0.87241238\n",
            "Iteration 6488, loss = 0.87235309\n",
            "Iteration 6489, loss = 0.87229381\n",
            "Iteration 6490, loss = 0.87223454\n",
            "Iteration 6491, loss = 0.87217528\n",
            "Iteration 6492, loss = 0.87211603\n",
            "Iteration 6493, loss = 0.87205679\n",
            "Iteration 6494, loss = 0.87199756\n",
            "Iteration 6495, loss = 0.87193834\n",
            "Iteration 6496, loss = 0.87187914\n",
            "Iteration 6497, loss = 0.87181994\n",
            "Iteration 6498, loss = 0.87176076\n",
            "Iteration 6499, loss = 0.87170158\n",
            "Iteration 6500, loss = 0.87164242\n",
            "Iteration 6501, loss = 0.87158326\n",
            "Iteration 6502, loss = 0.87152412\n",
            "Iteration 6503, loss = 0.87146499\n",
            "Iteration 6504, loss = 0.87140587\n",
            "Iteration 6505, loss = 0.87134675\n",
            "Iteration 6506, loss = 0.87128765\n",
            "Iteration 6507, loss = 0.87122856\n",
            "Iteration 6508, loss = 0.87116948\n",
            "Iteration 6509, loss = 0.87111041\n",
            "Iteration 6510, loss = 0.87105136\n",
            "Iteration 6511, loss = 0.87099231\n",
            "Iteration 6512, loss = 0.87093327\n",
            "Iteration 6513, loss = 0.87087424\n",
            "Iteration 6514, loss = 0.87081523\n",
            "Iteration 6515, loss = 0.87075622\n",
            "Iteration 6516, loss = 0.87069723\n",
            "Iteration 6517, loss = 0.87063824\n",
            "Iteration 6518, loss = 0.87057927\n",
            "Iteration 6519, loss = 0.87052030\n",
            "Iteration 6520, loss = 0.87046135\n",
            "Iteration 6521, loss = 0.87040241\n",
            "Iteration 6522, loss = 0.87034347\n",
            "Iteration 6523, loss = 0.87028455\n",
            "Iteration 6524, loss = 0.87022564\n",
            "Iteration 6525, loss = 0.87016674\n",
            "Iteration 6526, loss = 0.87010785\n",
            "Iteration 6527, loss = 0.87004897\n",
            "Iteration 6528, loss = 0.86999010\n",
            "Iteration 6529, loss = 0.86993124\n",
            "Iteration 6530, loss = 0.86987239\n",
            "Iteration 6531, loss = 0.86981355\n",
            "Iteration 6532, loss = 0.86975472\n",
            "Iteration 6533, loss = 0.86969591\n",
            "Iteration 6534, loss = 0.86963710\n",
            "Iteration 6535, loss = 0.86957830\n",
            "Iteration 6536, loss = 0.86951952\n",
            "Iteration 6537, loss = 0.86946074\n",
            "Iteration 6538, loss = 0.86940198\n",
            "Iteration 6539, loss = 0.86934322\n",
            "Iteration 6540, loss = 0.86928448\n",
            "Iteration 6541, loss = 0.86922574\n",
            "Iteration 6542, loss = 0.86916702\n",
            "Iteration 6543, loss = 0.86910831\n",
            "Iteration 6544, loss = 0.86904960\n",
            "Iteration 6545, loss = 0.86899091\n",
            "Iteration 6546, loss = 0.86893223\n",
            "Iteration 6547, loss = 0.86887356\n",
            "Iteration 6548, loss = 0.86881489\n",
            "Iteration 6549, loss = 0.86875624\n",
            "Iteration 6550, loss = 0.86869760\n",
            "Iteration 6551, loss = 0.86863897\n",
            "Iteration 6552, loss = 0.86858035\n",
            "Iteration 6553, loss = 0.86852174\n",
            "Iteration 6554, loss = 0.86846314\n",
            "Iteration 6555, loss = 0.86840455\n",
            "Iteration 6556, loss = 0.86834598\n",
            "Iteration 6557, loss = 0.86828741\n",
            "Iteration 6558, loss = 0.86822885\n",
            "Iteration 6559, loss = 0.86817030\n",
            "Iteration 6560, loss = 0.86811176\n",
            "Iteration 6561, loss = 0.86805324\n",
            "Iteration 6562, loss = 0.86799472\n",
            "Iteration 6563, loss = 0.86793621\n",
            "Iteration 6564, loss = 0.86787772\n",
            "Iteration 6565, loss = 0.86781923\n",
            "Iteration 6566, loss = 0.86776075\n",
            "Iteration 6567, loss = 0.86770229\n",
            "Iteration 6568, loss = 0.86764383\n",
            "Iteration 6569, loss = 0.86758539\n",
            "Iteration 6570, loss = 0.86752695\n",
            "Iteration 6571, loss = 0.86746853\n",
            "Iteration 6572, loss = 0.86741011\n",
            "Iteration 6573, loss = 0.86735171\n",
            "Iteration 6574, loss = 0.86729332\n",
            "Iteration 6575, loss = 0.86723493\n",
            "Iteration 6576, loss = 0.86717656\n",
            "Iteration 6577, loss = 0.86711819\n",
            "Iteration 6578, loss = 0.86705984\n",
            "Iteration 6579, loss = 0.86700150\n",
            "Iteration 6580, loss = 0.86694317\n",
            "Iteration 6581, loss = 0.86688484\n",
            "Iteration 6582, loss = 0.86682653\n",
            "Iteration 6583, loss = 0.86676823\n",
            "Iteration 6584, loss = 0.86670994\n",
            "Iteration 6585, loss = 0.86665166\n",
            "Iteration 6586, loss = 0.86659338\n",
            "Iteration 6587, loss = 0.86653512\n",
            "Iteration 6588, loss = 0.86647687\n",
            "Iteration 6589, loss = 0.86641863\n",
            "Iteration 6590, loss = 0.86636040\n",
            "Iteration 6591, loss = 0.86630218\n",
            "Iteration 6592, loss = 0.86624397\n",
            "Iteration 6593, loss = 0.86618577\n",
            "Iteration 6594, loss = 0.86612758\n",
            "Iteration 6595, loss = 0.86606940\n",
            "Iteration 6596, loss = 0.86601123\n",
            "Iteration 6597, loss = 0.86595306\n",
            "Iteration 6598, loss = 0.86589491\n",
            "Iteration 6599, loss = 0.86583677\n",
            "Iteration 6600, loss = 0.86577864\n",
            "Iteration 6601, loss = 0.86572053\n",
            "Iteration 6602, loss = 0.86566242\n",
            "Iteration 6603, loss = 0.86560432\n",
            "Iteration 6604, loss = 0.86554623\n",
            "Iteration 6605, loss = 0.86548815\n",
            "Iteration 6606, loss = 0.86543008\n",
            "Iteration 6607, loss = 0.86537202\n",
            "Iteration 6608, loss = 0.86531397\n",
            "Iteration 6609, loss = 0.86525593\n",
            "Iteration 6610, loss = 0.86519790\n",
            "Iteration 6611, loss = 0.86513988\n",
            "Iteration 6612, loss = 0.86508187\n",
            "Iteration 6613, loss = 0.86502387\n",
            "Iteration 6614, loss = 0.86496588\n",
            "Iteration 6615, loss = 0.86490790\n",
            "Iteration 6616, loss = 0.86484993\n",
            "Iteration 6617, loss = 0.86479197\n",
            "Iteration 6618, loss = 0.86473403\n",
            "Iteration 6619, loss = 0.86467609\n",
            "Iteration 6620, loss = 0.86461816\n",
            "Iteration 6621, loss = 0.86456024\n",
            "Iteration 6622, loss = 0.86450233\n",
            "Iteration 6623, loss = 0.86444443\n",
            "Iteration 6624, loss = 0.86438654\n",
            "Iteration 6625, loss = 0.86432866\n",
            "Iteration 6626, loss = 0.86427079\n",
            "Iteration 6627, loss = 0.86421293\n",
            "Iteration 6628, loss = 0.86415508\n",
            "Iteration 6629, loss = 0.86409724\n",
            "Iteration 6630, loss = 0.86403941\n",
            "Iteration 6631, loss = 0.86398159\n",
            "Iteration 6632, loss = 0.86392378\n",
            "Iteration 6633, loss = 0.86386598\n",
            "Iteration 6634, loss = 0.86380819\n",
            "Iteration 6635, loss = 0.86375041\n",
            "Iteration 6636, loss = 0.86369264\n",
            "Iteration 6637, loss = 0.86363488\n",
            "Iteration 6638, loss = 0.86357713\n",
            "Iteration 6639, loss = 0.86351939\n",
            "Iteration 6640, loss = 0.86346166\n",
            "Iteration 6641, loss = 0.86340394\n",
            "Iteration 6642, loss = 0.86334623\n",
            "Iteration 6643, loss = 0.86328853\n",
            "Iteration 6644, loss = 0.86323083\n",
            "Iteration 6645, loss = 0.86317315\n",
            "Iteration 6646, loss = 0.86311548\n",
            "Iteration 6647, loss = 0.86305782\n",
            "Iteration 6648, loss = 0.86300017\n",
            "Iteration 6649, loss = 0.86294252\n",
            "Iteration 6650, loss = 0.86288489\n",
            "Iteration 6651, loss = 0.86282727\n",
            "Iteration 6652, loss = 0.86276966\n",
            "Iteration 6653, loss = 0.86271205\n",
            "Iteration 6654, loss = 0.86265446\n",
            "Iteration 6655, loss = 0.86259688\n",
            "Iteration 6656, loss = 0.86253930\n",
            "Iteration 6657, loss = 0.86248174\n",
            "Iteration 6658, loss = 0.86242419\n",
            "Iteration 6659, loss = 0.86236664\n",
            "Iteration 6660, loss = 0.86230911\n",
            "Iteration 6661, loss = 0.86225158\n",
            "Iteration 6662, loss = 0.86219407\n",
            "Iteration 6663, loss = 0.86213656\n",
            "Iteration 6664, loss = 0.86207906\n",
            "Iteration 6665, loss = 0.86202158\n",
            "Iteration 6666, loss = 0.86196410\n",
            "Iteration 6667, loss = 0.86190664\n",
            "Iteration 6668, loss = 0.86184918\n",
            "Iteration 6669, loss = 0.86179173\n",
            "Iteration 6670, loss = 0.86173429\n",
            "Iteration 6671, loss = 0.86167687\n",
            "Iteration 6672, loss = 0.86161945\n",
            "Iteration 6673, loss = 0.86156204\n",
            "Iteration 6674, loss = 0.86150464\n",
            "Iteration 6675, loss = 0.86144725\n",
            "Iteration 6676, loss = 0.86138987\n",
            "Iteration 6677, loss = 0.86133250\n",
            "Iteration 6678, loss = 0.86127514\n",
            "Iteration 6679, loss = 0.86121779\n",
            "Iteration 6680, loss = 0.86116045\n",
            "Iteration 6681, loss = 0.86110312\n",
            "Iteration 6682, loss = 0.86104580\n",
            "Iteration 6683, loss = 0.86098849\n",
            "Iteration 6684, loss = 0.86093119\n",
            "Iteration 6685, loss = 0.86087389\n",
            "Iteration 6686, loss = 0.86081661\n",
            "Iteration 6687, loss = 0.86075934\n",
            "Iteration 6688, loss = 0.86070207\n",
            "Iteration 6689, loss = 0.86064482\n",
            "Iteration 6690, loss = 0.86058757\n",
            "Iteration 6691, loss = 0.86053034\n",
            "Iteration 6692, loss = 0.86047311\n",
            "Iteration 6693, loss = 0.86041590\n",
            "Iteration 6694, loss = 0.86035869\n",
            "Iteration 6695, loss = 0.86030149\n",
            "Iteration 6696, loss = 0.86024431\n",
            "Iteration 6697, loss = 0.86018713\n",
            "Iteration 6698, loss = 0.86012996\n",
            "Iteration 6699, loss = 0.86007280\n",
            "Iteration 6700, loss = 0.86001565\n",
            "Iteration 6701, loss = 0.85995851\n",
            "Iteration 6702, loss = 0.85990138\n",
            "Iteration 6703, loss = 0.85984426\n",
            "Iteration 6704, loss = 0.85978715\n",
            "Iteration 6705, loss = 0.85973005\n",
            "Iteration 6706, loss = 0.85967296\n",
            "Iteration 6707, loss = 0.85961587\n",
            "Iteration 6708, loss = 0.85955880\n",
            "Iteration 6709, loss = 0.85950174\n",
            "Iteration 6710, loss = 0.85944468\n",
            "Iteration 6711, loss = 0.85938764\n",
            "Iteration 6712, loss = 0.85933060\n",
            "Iteration 6713, loss = 0.85927358\n",
            "Iteration 6714, loss = 0.85921656\n",
            "Iteration 6715, loss = 0.85915955\n",
            "Iteration 6716, loss = 0.85910256\n",
            "Iteration 6717, loss = 0.85904557\n",
            "Iteration 6718, loss = 0.85898859\n",
            "Iteration 6719, loss = 0.85893162\n",
            "Iteration 6720, loss = 0.85887466\n",
            "Iteration 6721, loss = 0.85881771\n",
            "Iteration 6722, loss = 0.85876077\n",
            "Iteration 6723, loss = 0.85870383\n",
            "Iteration 6724, loss = 0.85864691\n",
            "Iteration 6725, loss = 0.85859000\n",
            "Iteration 6726, loss = 0.85853310\n",
            "Iteration 6727, loss = 0.85847620\n",
            "Iteration 6728, loss = 0.85841932\n",
            "Iteration 6729, loss = 0.85836244\n",
            "Iteration 6730, loss = 0.85830557\n",
            "Iteration 6731, loss = 0.85824872\n",
            "Iteration 6732, loss = 0.85819187\n",
            "Iteration 6733, loss = 0.85813503\n",
            "Iteration 6734, loss = 0.85807820\n",
            "Iteration 6735, loss = 0.85802138\n",
            "Iteration 6736, loss = 0.85796457\n",
            "Iteration 6737, loss = 0.85790777\n",
            "Iteration 6738, loss = 0.85785098\n",
            "Iteration 6739, loss = 0.85779420\n",
            "Iteration 6740, loss = 0.85773742\n",
            "Iteration 6741, loss = 0.85768066\n",
            "Iteration 6742, loss = 0.85762390\n",
            "Iteration 6743, loss = 0.85756716\n",
            "Iteration 6744, loss = 0.85751042\n",
            "Iteration 6745, loss = 0.85745370\n",
            "Iteration 6746, loss = 0.85739698\n",
            "Iteration 6747, loss = 0.85734027\n",
            "Iteration 6748, loss = 0.85728357\n",
            "Iteration 6749, loss = 0.85722688\n",
            "Iteration 6750, loss = 0.85717020\n",
            "Iteration 6751, loss = 0.85711353\n",
            "Iteration 6752, loss = 0.85705687\n",
            "Iteration 6753, loss = 0.85700021\n",
            "Iteration 6754, loss = 0.85694357\n",
            "Iteration 6755, loss = 0.85688694\n",
            "Iteration 6756, loss = 0.85683031\n",
            "Iteration 6757, loss = 0.85677369\n",
            "Iteration 6758, loss = 0.85671709\n",
            "Iteration 6759, loss = 0.85666049\n",
            "Iteration 6760, loss = 0.85660390\n",
            "Iteration 6761, loss = 0.85654732\n",
            "Iteration 6762, loss = 0.85649075\n",
            "Iteration 6763, loss = 0.85643419\n",
            "Iteration 6764, loss = 0.85637764\n",
            "Iteration 6765, loss = 0.85632109\n",
            "Iteration 6766, loss = 0.85626456\n",
            "Iteration 6767, loss = 0.85620803\n",
            "Iteration 6768, loss = 0.85615152\n",
            "Iteration 6769, loss = 0.85609501\n",
            "Iteration 6770, loss = 0.85603851\n",
            "Iteration 6771, loss = 0.85598202\n",
            "Iteration 6772, loss = 0.85592555\n",
            "Iteration 6773, loss = 0.85586908\n",
            "Iteration 6774, loss = 0.85581261\n",
            "Iteration 6775, loss = 0.85575616\n",
            "Iteration 6776, loss = 0.85569972\n",
            "Iteration 6777, loss = 0.85564328\n",
            "Iteration 6778, loss = 0.85558686\n",
            "Iteration 6779, loss = 0.85553044\n",
            "Iteration 6780, loss = 0.85547404\n",
            "Iteration 6781, loss = 0.85541764\n",
            "Iteration 6782, loss = 0.85536125\n",
            "Iteration 6783, loss = 0.85530487\n",
            "Iteration 6784, loss = 0.85524850\n",
            "Iteration 6785, loss = 0.85519214\n",
            "Iteration 6786, loss = 0.85513578\n",
            "Iteration 6787, loss = 0.85507944\n",
            "Iteration 6788, loss = 0.85502311\n",
            "Iteration 6789, loss = 0.85496678\n",
            "Iteration 6790, loss = 0.85491046\n",
            "Iteration 6791, loss = 0.85485416\n",
            "Iteration 6792, loss = 0.85479786\n",
            "Iteration 6793, loss = 0.85474157\n",
            "Iteration 6794, loss = 0.85468529\n",
            "Iteration 6795, loss = 0.85462901\n",
            "Iteration 6796, loss = 0.85457275\n",
            "Iteration 6797, loss = 0.85451650\n",
            "Iteration 6798, loss = 0.85446025\n",
            "Iteration 6799, loss = 0.85440402\n",
            "Iteration 6800, loss = 0.85434779\n",
            "Iteration 6801, loss = 0.85429157\n",
            "Iteration 6802, loss = 0.85423536\n",
            "Iteration 6803, loss = 0.85417916\n",
            "Iteration 6804, loss = 0.85412297\n",
            "Iteration 6805, loss = 0.85406678\n",
            "Iteration 6806, loss = 0.85401061\n",
            "Iteration 6807, loss = 0.85395445\n",
            "Iteration 6808, loss = 0.85389829\n",
            "Iteration 6809, loss = 0.85384214\n",
            "Iteration 6810, loss = 0.85378600\n",
            "Iteration 6811, loss = 0.85372987\n",
            "Iteration 6812, loss = 0.85367375\n",
            "Iteration 6813, loss = 0.85361764\n",
            "Iteration 6814, loss = 0.85356154\n",
            "Iteration 6815, loss = 0.85350544\n",
            "Iteration 6816, loss = 0.85344936\n",
            "Iteration 6817, loss = 0.85339328\n",
            "Iteration 6818, loss = 0.85333721\n",
            "Iteration 6819, loss = 0.85328115\n",
            "Iteration 6820, loss = 0.85322510\n",
            "Iteration 6821, loss = 0.85316906\n",
            "Iteration 6822, loss = 0.85311303\n",
            "Iteration 6823, loss = 0.85305700\n",
            "Iteration 6824, loss = 0.85300099\n",
            "Iteration 6825, loss = 0.85294498\n",
            "Iteration 6826, loss = 0.85288899\n",
            "Iteration 6827, loss = 0.85283300\n",
            "Iteration 6828, loss = 0.85277702\n",
            "Iteration 6829, loss = 0.85272104\n",
            "Iteration 6830, loss = 0.85266508\n",
            "Iteration 6831, loss = 0.85260913\n",
            "Iteration 6832, loss = 0.85255318\n",
            "Iteration 6833, loss = 0.85249725\n",
            "Iteration 6834, loss = 0.85244132\n",
            "Iteration 6835, loss = 0.85238540\n",
            "Iteration 6836, loss = 0.85232949\n",
            "Iteration 6837, loss = 0.85227359\n",
            "Iteration 6838, loss = 0.85221769\n",
            "Iteration 6839, loss = 0.85216181\n",
            "Iteration 6840, loss = 0.85210593\n",
            "Iteration 6841, loss = 0.85205007\n",
            "Iteration 6842, loss = 0.85199421\n",
            "Iteration 6843, loss = 0.85193836\n",
            "Iteration 6844, loss = 0.85188252\n",
            "Iteration 6845, loss = 0.85182669\n",
            "Iteration 6846, loss = 0.85177086\n",
            "Iteration 6847, loss = 0.85171505\n",
            "Iteration 6848, loss = 0.85165924\n",
            "Iteration 6849, loss = 0.85160344\n",
            "Iteration 6850, loss = 0.85154765\n",
            "Iteration 6851, loss = 0.85149187\n",
            "Iteration 6852, loss = 0.85143610\n",
            "Iteration 6853, loss = 0.85138034\n",
            "Iteration 6854, loss = 0.85132458\n",
            "Iteration 6855, loss = 0.85126883\n",
            "Iteration 6856, loss = 0.85121310\n",
            "Iteration 6857, loss = 0.85115737\n",
            "Iteration 6858, loss = 0.85110165\n",
            "Iteration 6859, loss = 0.85104594\n",
            "Iteration 6860, loss = 0.85099023\n",
            "Iteration 6861, loss = 0.85093454\n",
            "Iteration 6862, loss = 0.85087885\n",
            "Iteration 6863, loss = 0.85082317\n",
            "Iteration 6864, loss = 0.85076750\n",
            "Iteration 6865, loss = 0.85071184\n",
            "Iteration 6866, loss = 0.85065619\n",
            "Iteration 6867, loss = 0.85060055\n",
            "Iteration 6868, loss = 0.85054491\n",
            "Iteration 6869, loss = 0.85048928\n",
            "Iteration 6870, loss = 0.85043367\n",
            "Iteration 6871, loss = 0.85037806\n",
            "Iteration 6872, loss = 0.85032245\n",
            "Iteration 6873, loss = 0.85026686\n",
            "Iteration 6874, loss = 0.85021128\n",
            "Iteration 6875, loss = 0.85015570\n",
            "Iteration 6876, loss = 0.85010013\n",
            "Iteration 6877, loss = 0.85004457\n",
            "Iteration 6878, loss = 0.84998902\n",
            "Iteration 6879, loss = 0.84993348\n",
            "Iteration 6880, loss = 0.84987795\n",
            "Iteration 6881, loss = 0.84982242\n",
            "Iteration 6882, loss = 0.84976691\n",
            "Iteration 6883, loss = 0.84971140\n",
            "Iteration 6884, loss = 0.84965590\n",
            "Iteration 6885, loss = 0.84960040\n",
            "Iteration 6886, loss = 0.84954492\n",
            "Iteration 6887, loss = 0.84948945\n",
            "Iteration 6888, loss = 0.84943398\n",
            "Iteration 6889, loss = 0.84937852\n",
            "Iteration 6890, loss = 0.84932307\n",
            "Iteration 6891, loss = 0.84926763\n",
            "Iteration 6892, loss = 0.84921220\n",
            "Iteration 6893, loss = 0.84915677\n",
            "Iteration 6894, loss = 0.84910136\n",
            "Iteration 6895, loss = 0.84904595\n",
            "Iteration 6896, loss = 0.84899055\n",
            "Iteration 6897, loss = 0.84893516\n",
            "Iteration 6898, loss = 0.84887977\n",
            "Iteration 6899, loss = 0.84882440\n",
            "Iteration 6900, loss = 0.84876903\n",
            "Iteration 6901, loss = 0.84871368\n",
            "Iteration 6902, loss = 0.84865833\n",
            "Iteration 6903, loss = 0.84860298\n",
            "Iteration 6904, loss = 0.84854765\n",
            "Iteration 6905, loss = 0.84849233\n",
            "Iteration 6906, loss = 0.84843701\n",
            "Iteration 6907, loss = 0.84838170\n",
            "Iteration 6908, loss = 0.84832640\n",
            "Iteration 6909, loss = 0.84827111\n",
            "Iteration 6910, loss = 0.84821582\n",
            "Iteration 6911, loss = 0.84816055\n",
            "Iteration 6912, loss = 0.84810528\n",
            "Iteration 6913, loss = 0.84805002\n",
            "Iteration 6914, loss = 0.84799477\n",
            "Iteration 6915, loss = 0.84793953\n",
            "Iteration 6916, loss = 0.84788430\n",
            "Iteration 6917, loss = 0.84782907\n",
            "Iteration 6918, loss = 0.84777385\n",
            "Iteration 6919, loss = 0.84771864\n",
            "Iteration 6920, loss = 0.84766344\n",
            "Iteration 6921, loss = 0.84760825\n",
            "Iteration 6922, loss = 0.84755306\n",
            "Iteration 6923, loss = 0.84749789\n",
            "Iteration 6924, loss = 0.84744272\n",
            "Iteration 6925, loss = 0.84738756\n",
            "Iteration 6926, loss = 0.84733240\n",
            "Iteration 6927, loss = 0.84727726\n",
            "Iteration 6928, loss = 0.84722212\n",
            "Iteration 6929, loss = 0.84716700\n",
            "Iteration 6930, loss = 0.84711188\n",
            "Iteration 6931, loss = 0.84705676\n",
            "Iteration 6932, loss = 0.84700166\n",
            "Iteration 6933, loss = 0.84694657\n",
            "Iteration 6934, loss = 0.84689148\n",
            "Iteration 6935, loss = 0.84683640\n",
            "Iteration 6936, loss = 0.84678133\n",
            "Iteration 6937, loss = 0.84672626\n",
            "Iteration 6938, loss = 0.84667121\n",
            "Iteration 6939, loss = 0.84661616\n",
            "Iteration 6940, loss = 0.84656112\n",
            "Iteration 6941, loss = 0.84650609\n",
            "Iteration 6942, loss = 0.84645107\n",
            "Iteration 6943, loss = 0.84639606\n",
            "Iteration 6944, loss = 0.84634105\n",
            "Iteration 6945, loss = 0.84628605\n",
            "Iteration 6946, loss = 0.84623106\n",
            "Iteration 6947, loss = 0.84617608\n",
            "Iteration 6948, loss = 0.84612110\n",
            "Iteration 6949, loss = 0.84606614\n",
            "Iteration 6950, loss = 0.84601118\n",
            "Iteration 6951, loss = 0.84595623\n",
            "Iteration 6952, loss = 0.84590129\n",
            "Iteration 6953, loss = 0.84584635\n",
            "Iteration 6954, loss = 0.84579143\n",
            "Iteration 6955, loss = 0.84573651\n",
            "Iteration 6956, loss = 0.84568160\n",
            "Iteration 6957, loss = 0.84562670\n",
            "Iteration 6958, loss = 0.84557180\n",
            "Iteration 6959, loss = 0.84551692\n",
            "Iteration 6960, loss = 0.84546204\n",
            "Iteration 6961, loss = 0.84540717\n",
            "Iteration 6962, loss = 0.84535230\n",
            "Iteration 6963, loss = 0.84529745\n",
            "Iteration 6964, loss = 0.84524260\n",
            "Iteration 6965, loss = 0.84518776\n",
            "Iteration 6966, loss = 0.84513293\n",
            "Iteration 6967, loss = 0.84507811\n",
            "Iteration 6968, loss = 0.84502330\n",
            "Iteration 6969, loss = 0.84496849\n",
            "Iteration 6970, loss = 0.84491369\n",
            "Iteration 6971, loss = 0.84485890\n",
            "Iteration 6972, loss = 0.84480412\n",
            "Iteration 6973, loss = 0.84474934\n",
            "Iteration 6974, loss = 0.84469457\n",
            "Iteration 6975, loss = 0.84463981\n",
            "Iteration 6976, loss = 0.84458506\n",
            "Iteration 6977, loss = 0.84453032\n",
            "Iteration 6978, loss = 0.84447558\n",
            "Iteration 6979, loss = 0.84442085\n",
            "Iteration 6980, loss = 0.84436613\n",
            "Iteration 6981, loss = 0.84431142\n",
            "Iteration 6982, loss = 0.84425672\n",
            "Iteration 6983, loss = 0.84420202\n",
            "Iteration 6984, loss = 0.84414733\n",
            "Iteration 6985, loss = 0.84409265\n",
            "Iteration 6986, loss = 0.84403798\n",
            "Iteration 6987, loss = 0.84398331\n",
            "Iteration 6988, loss = 0.84392865\n",
            "Iteration 6989, loss = 0.84387400\n",
            "Iteration 6990, loss = 0.84381936\n",
            "Iteration 6991, loss = 0.84376473\n",
            "Iteration 6992, loss = 0.84371010\n",
            "Iteration 6993, loss = 0.84365548\n",
            "Iteration 6994, loss = 0.84360087\n",
            "Iteration 6995, loss = 0.84354627\n",
            "Iteration 6996, loss = 0.84349167\n",
            "Iteration 6997, loss = 0.84343708\n",
            "Iteration 6998, loss = 0.84338250\n",
            "Iteration 6999, loss = 0.84332793\n",
            "Iteration 7000, loss = 0.84327337\n",
            "Iteration 7001, loss = 0.84321881\n",
            "Iteration 7002, loss = 0.84316426\n",
            "Iteration 7003, loss = 0.84310972\n",
            "Iteration 7004, loss = 0.84305519\n",
            "Iteration 7005, loss = 0.84300066\n",
            "Iteration 7006, loss = 0.84294614\n",
            "Iteration 7007, loss = 0.84289163\n",
            "Iteration 7008, loss = 0.84283713\n",
            "Iteration 7009, loss = 0.84278264\n",
            "Iteration 7010, loss = 0.84272815\n",
            "Iteration 7011, loss = 0.84267367\n",
            "Iteration 7012, loss = 0.84261920\n",
            "Iteration 7013, loss = 0.84256473\n",
            "Iteration 7014, loss = 0.84251028\n",
            "Iteration 7015, loss = 0.84245583\n",
            "Iteration 7016, loss = 0.84240139\n",
            "Iteration 7017, loss = 0.84234696\n",
            "Iteration 7018, loss = 0.84229253\n",
            "Iteration 7019, loss = 0.84223811\n",
            "Iteration 7020, loss = 0.84218370\n",
            "Iteration 7021, loss = 0.84212930\n",
            "Iteration 7022, loss = 0.84207490\n",
            "Iteration 7023, loss = 0.84202052\n",
            "Iteration 7024, loss = 0.84196614\n",
            "Iteration 7025, loss = 0.84191176\n",
            "Iteration 7026, loss = 0.84185740\n",
            "Iteration 7027, loss = 0.84180304\n",
            "Iteration 7028, loss = 0.84174869\n",
            "Iteration 7029, loss = 0.84169435\n",
            "Iteration 7030, loss = 0.84164002\n",
            "Iteration 7031, loss = 0.84158569\n",
            "Iteration 7032, loss = 0.84153137\n",
            "Iteration 7033, loss = 0.84147706\n",
            "Iteration 7034, loss = 0.84142275\n",
            "Iteration 7035, loss = 0.84136846\n",
            "Iteration 7036, loss = 0.84131417\n",
            "Iteration 7037, loss = 0.84125989\n",
            "Iteration 7038, loss = 0.84120561\n",
            "Iteration 7039, loss = 0.84115135\n",
            "Iteration 7040, loss = 0.84109709\n",
            "Iteration 7041, loss = 0.84104284\n",
            "Iteration 7042, loss = 0.84098859\n",
            "Iteration 7043, loss = 0.84093436\n",
            "Iteration 7044, loss = 0.84088013\n",
            "Iteration 7045, loss = 0.84082591\n",
            "Iteration 7046, loss = 0.84077169\n",
            "Iteration 7047, loss = 0.84071749\n",
            "Iteration 7048, loss = 0.84066329\n",
            "Iteration 7049, loss = 0.84060910\n",
            "Iteration 7050, loss = 0.84055491\n",
            "Iteration 7051, loss = 0.84050074\n",
            "Iteration 7052, loss = 0.84044657\n",
            "Iteration 7053, loss = 0.84039241\n",
            "Iteration 7054, loss = 0.84033826\n",
            "Iteration 7055, loss = 0.84028411\n",
            "Iteration 7056, loss = 0.84022997\n",
            "Iteration 7057, loss = 0.84017584\n",
            "Iteration 7058, loss = 0.84012172\n",
            "Iteration 7059, loss = 0.84006760\n",
            "Iteration 7060, loss = 0.84001349\n",
            "Iteration 7061, loss = 0.83995939\n",
            "Iteration 7062, loss = 0.83990529\n",
            "Iteration 7063, loss = 0.83985121\n",
            "Iteration 7064, loss = 0.83979713\n",
            "Iteration 7065, loss = 0.83974306\n",
            "Iteration 7066, loss = 0.83968899\n",
            "Iteration 7067, loss = 0.83963494\n",
            "Iteration 7068, loss = 0.83958089\n",
            "Iteration 7069, loss = 0.83952684\n",
            "Iteration 7070, loss = 0.83947281\n",
            "Iteration 7071, loss = 0.83941878\n",
            "Iteration 7072, loss = 0.83936476\n",
            "Iteration 7073, loss = 0.83931075\n",
            "Iteration 7074, loss = 0.83925674\n",
            "Iteration 7075, loss = 0.83920275\n",
            "Iteration 7076, loss = 0.83914876\n",
            "Iteration 7077, loss = 0.83909477\n",
            "Iteration 7078, loss = 0.83904080\n",
            "Iteration 7079, loss = 0.83898683\n",
            "Iteration 7080, loss = 0.83893287\n",
            "Iteration 7081, loss = 0.83887891\n",
            "Iteration 7082, loss = 0.83882497\n",
            "Iteration 7083, loss = 0.83877103\n",
            "Iteration 7084, loss = 0.83871710\n",
            "Iteration 7085, loss = 0.83866317\n",
            "Iteration 7086, loss = 0.83860926\n",
            "Iteration 7087, loss = 0.83855535\n",
            "Iteration 7088, loss = 0.83850144\n",
            "Iteration 7089, loss = 0.83844755\n",
            "Iteration 7090, loss = 0.83839366\n",
            "Iteration 7091, loss = 0.83833978\n",
            "Iteration 7092, loss = 0.83828591\n",
            "Iteration 7093, loss = 0.83823204\n",
            "Iteration 7094, loss = 0.83817818\n",
            "Iteration 7095, loss = 0.83812433\n",
            "Iteration 7096, loss = 0.83807049\n",
            "Iteration 7097, loss = 0.83801665\n",
            "Iteration 7098, loss = 0.83796282\n",
            "Iteration 7099, loss = 0.83790900\n",
            "Iteration 7100, loss = 0.83785519\n",
            "Iteration 7101, loss = 0.83780138\n",
            "Iteration 7102, loss = 0.83774758\n",
            "Iteration 7103, loss = 0.83769379\n",
            "Iteration 7104, loss = 0.83764000\n",
            "Iteration 7105, loss = 0.83758622\n",
            "Iteration 7106, loss = 0.83753245\n",
            "Iteration 7107, loss = 0.83747869\n",
            "Iteration 7108, loss = 0.83742493\n",
            "Iteration 7109, loss = 0.83737118\n",
            "Iteration 7110, loss = 0.83731744\n",
            "Iteration 7111, loss = 0.83726370\n",
            "Iteration 7112, loss = 0.83720997\n",
            "Iteration 7113, loss = 0.83715625\n",
            "Iteration 7114, loss = 0.83710254\n",
            "Iteration 7115, loss = 0.83704883\n",
            "Iteration 7116, loss = 0.83699513\n",
            "Iteration 7117, loss = 0.83694144\n",
            "Iteration 7118, loss = 0.83688776\n",
            "Iteration 7119, loss = 0.83683408\n",
            "Iteration 7120, loss = 0.83678041\n",
            "Iteration 7121, loss = 0.83672675\n",
            "Iteration 7122, loss = 0.83667309\n",
            "Iteration 7123, loss = 0.83661944\n",
            "Iteration 7124, loss = 0.83656580\n",
            "Iteration 7125, loss = 0.83651216\n",
            "Iteration 7126, loss = 0.83645854\n",
            "Iteration 7127, loss = 0.83640492\n",
            "Iteration 7128, loss = 0.83635130\n",
            "Iteration 7129, loss = 0.83629770\n",
            "Iteration 7130, loss = 0.83624410\n",
            "Iteration 7131, loss = 0.83619051\n",
            "Iteration 7132, loss = 0.83613692\n",
            "Iteration 7133, loss = 0.83608335\n",
            "Iteration 7134, loss = 0.83602978\n",
            "Iteration 7135, loss = 0.83597621\n",
            "Iteration 7136, loss = 0.83592266\n",
            "Iteration 7137, loss = 0.83586911\n",
            "Iteration 7138, loss = 0.83581557\n",
            "Iteration 7139, loss = 0.83576203\n",
            "Iteration 7140, loss = 0.83570850\n",
            "Iteration 7141, loss = 0.83565498\n",
            "Iteration 7142, loss = 0.83560147\n",
            "Iteration 7143, loss = 0.83554796\n",
            "Iteration 7144, loss = 0.83549446\n",
            "Iteration 7145, loss = 0.83544097\n",
            "Iteration 7146, loss = 0.83538749\n",
            "Iteration 7147, loss = 0.83533401\n",
            "Iteration 7148, loss = 0.83528054\n",
            "Iteration 7149, loss = 0.83522708\n",
            "Iteration 7150, loss = 0.83517362\n",
            "Iteration 7151, loss = 0.83512017\n",
            "Iteration 7152, loss = 0.83506673\n",
            "Iteration 7153, loss = 0.83501329\n",
            "Iteration 7154, loss = 0.83495986\n",
            "Iteration 7155, loss = 0.83490644\n",
            "Iteration 7156, loss = 0.83485303\n",
            "Iteration 7157, loss = 0.83479962\n",
            "Iteration 7158, loss = 0.83474622\n",
            "Iteration 7159, loss = 0.83469283\n",
            "Iteration 7160, loss = 0.83463944\n",
            "Iteration 7161, loss = 0.83458606\n",
            "Iteration 7162, loss = 0.83453269\n",
            "Iteration 7163, loss = 0.83447932\n",
            "Iteration 7164, loss = 0.83442597\n",
            "Iteration 7165, loss = 0.83437261\n",
            "Iteration 7166, loss = 0.83431927\n",
            "Iteration 7167, loss = 0.83426593\n",
            "Iteration 7168, loss = 0.83421260\n",
            "Iteration 7169, loss = 0.83415928\n",
            "Iteration 7170, loss = 0.83410596\n",
            "Iteration 7171, loss = 0.83405266\n",
            "Iteration 7172, loss = 0.83399935\n",
            "Iteration 7173, loss = 0.83394606\n",
            "Iteration 7174, loss = 0.83389277\n",
            "Iteration 7175, loss = 0.83383949\n",
            "Iteration 7176, loss = 0.83378622\n",
            "Iteration 7177, loss = 0.83373295\n",
            "Iteration 7178, loss = 0.83367969\n",
            "Iteration 7179, loss = 0.83362644\n",
            "Iteration 7180, loss = 0.83357319\n",
            "Iteration 7181, loss = 0.83351995\n",
            "Iteration 7182, loss = 0.83346672\n",
            "Iteration 7183, loss = 0.83341349\n",
            "Iteration 7184, loss = 0.83336027\n",
            "Iteration 7185, loss = 0.83330706\n",
            "Iteration 7186, loss = 0.83325386\n",
            "Iteration 7187, loss = 0.83320066\n",
            "Iteration 7188, loss = 0.83314747\n",
            "Iteration 7189, loss = 0.83309429\n",
            "Iteration 7190, loss = 0.83304111\n",
            "Iteration 7191, loss = 0.83298794\n",
            "Iteration 7192, loss = 0.83293478\n",
            "Iteration 7193, loss = 0.83288162\n",
            "Iteration 7194, loss = 0.83282847\n",
            "Iteration 7195, loss = 0.83277533\n",
            "Iteration 7196, loss = 0.83272220\n",
            "Iteration 7197, loss = 0.83266907\n",
            "Iteration 7198, loss = 0.83261595\n",
            "Iteration 7199, loss = 0.83256283\n",
            "Iteration 7200, loss = 0.83250973\n",
            "Iteration 7201, loss = 0.83245662\n",
            "Iteration 7202, loss = 0.83240353\n",
            "Iteration 7203, loss = 0.83235044\n",
            "Iteration 7204, loss = 0.83229736\n",
            "Iteration 7205, loss = 0.83224429\n",
            "Iteration 7206, loss = 0.83219123\n",
            "Iteration 7207, loss = 0.83213817\n",
            "Iteration 7208, loss = 0.83208511\n",
            "Iteration 7209, loss = 0.83203207\n",
            "Iteration 7210, loss = 0.83197903\n",
            "Iteration 7211, loss = 0.83192600\n",
            "Iteration 7212, loss = 0.83187298\n",
            "Iteration 7213, loss = 0.83181996\n",
            "Iteration 7214, loss = 0.83176695\n",
            "Iteration 7215, loss = 0.83171394\n",
            "Iteration 7216, loss = 0.83166095\n",
            "Iteration 7217, loss = 0.83160795\n",
            "Iteration 7218, loss = 0.83155497\n",
            "Iteration 7219, loss = 0.83150199\n",
            "Iteration 7220, loss = 0.83144903\n",
            "Iteration 7221, loss = 0.83139606\n",
            "Iteration 7222, loss = 0.83134311\n",
            "Iteration 7223, loss = 0.83129016\n",
            "Iteration 7224, loss = 0.83123721\n",
            "Iteration 7225, loss = 0.83118428\n",
            "Iteration 7226, loss = 0.83113135\n",
            "Iteration 7227, loss = 0.83107843\n",
            "Iteration 7228, loss = 0.83102551\n",
            "Iteration 7229, loss = 0.83097261\n",
            "Iteration 7230, loss = 0.83091970\n",
            "Iteration 7231, loss = 0.83086681\n",
            "Iteration 7232, loss = 0.83081392\n",
            "Iteration 7233, loss = 0.83076104\n",
            "Iteration 7234, loss = 0.83070817\n",
            "Iteration 7235, loss = 0.83065530\n",
            "Iteration 7236, loss = 0.83060244\n",
            "Iteration 7237, loss = 0.83054959\n",
            "Iteration 7238, loss = 0.83049674\n",
            "Iteration 7239, loss = 0.83044390\n",
            "Iteration 7240, loss = 0.83039107\n",
            "Iteration 7241, loss = 0.83033824\n",
            "Iteration 7242, loss = 0.83028542\n",
            "Iteration 7243, loss = 0.83023261\n",
            "Iteration 7244, loss = 0.83017980\n",
            "Iteration 7245, loss = 0.83012700\n",
            "Iteration 7246, loss = 0.83007421\n",
            "Iteration 7247, loss = 0.83002143\n",
            "Iteration 7248, loss = 0.82996865\n",
            "Iteration 7249, loss = 0.82991588\n",
            "Iteration 7250, loss = 0.82986311\n",
            "Iteration 7251, loss = 0.82981035\n",
            "Iteration 7252, loss = 0.82975760\n",
            "Iteration 7253, loss = 0.82970486\n",
            "Iteration 7254, loss = 0.82965212\n",
            "Iteration 7255, loss = 0.82959939\n",
            "Iteration 7256, loss = 0.82954666\n",
            "Iteration 7257, loss = 0.82949395\n",
            "Iteration 7258, loss = 0.82944124\n",
            "Iteration 7259, loss = 0.82938853\n",
            "Iteration 7260, loss = 0.82933583\n",
            "Iteration 7261, loss = 0.82928314\n",
            "Iteration 7262, loss = 0.82923046\n",
            "Iteration 7263, loss = 0.82917778\n",
            "Iteration 7264, loss = 0.82912511\n",
            "Iteration 7265, loss = 0.82907245\n",
            "Iteration 7266, loss = 0.82901979\n",
            "Iteration 7267, loss = 0.82896714\n",
            "Iteration 7268, loss = 0.82891450\n",
            "Iteration 7269, loss = 0.82886186\n",
            "Iteration 7270, loss = 0.82880923\n",
            "Iteration 7271, loss = 0.82875661\n",
            "Iteration 7272, loss = 0.82870400\n",
            "Iteration 7273, loss = 0.82865139\n",
            "Iteration 7274, loss = 0.82859878\n",
            "Iteration 7275, loss = 0.82854619\n",
            "Iteration 7276, loss = 0.82849360\n",
            "Iteration 7277, loss = 0.82844102\n",
            "Iteration 7278, loss = 0.82838844\n",
            "Iteration 7279, loss = 0.82833587\n",
            "Iteration 7280, loss = 0.82828331\n",
            "Iteration 7281, loss = 0.82823076\n",
            "Iteration 7282, loss = 0.82817821\n",
            "Iteration 7283, loss = 0.82812567\n",
            "Iteration 7284, loss = 0.82807313\n",
            "Iteration 7285, loss = 0.82802060\n",
            "Iteration 7286, loss = 0.82796808\n",
            "Iteration 7287, loss = 0.82791557\n",
            "Iteration 7288, loss = 0.82786306\n",
            "Iteration 7289, loss = 0.82781056\n",
            "Iteration 7290, loss = 0.82775806\n",
            "Iteration 7291, loss = 0.82770557\n",
            "Iteration 7292, loss = 0.82765309\n",
            "Iteration 7293, loss = 0.82760062\n",
            "Iteration 7294, loss = 0.82754815\n",
            "Iteration 7295, loss = 0.82749569\n",
            "Iteration 7296, loss = 0.82744323\n",
            "Iteration 7297, loss = 0.82739079\n",
            "Iteration 7298, loss = 0.82733835\n",
            "Iteration 7299, loss = 0.82728591\n",
            "Iteration 7300, loss = 0.82723348\n",
            "Iteration 7301, loss = 0.82718106\n",
            "Iteration 7302, loss = 0.82712865\n",
            "Iteration 7303, loss = 0.82707624\n",
            "Iteration 7304, loss = 0.82702384\n",
            "Iteration 7305, loss = 0.82697145\n",
            "Iteration 7306, loss = 0.82691906\n",
            "Iteration 7307, loss = 0.82686668\n",
            "Iteration 7308, loss = 0.82681431\n",
            "Iteration 7309, loss = 0.82676194\n",
            "Iteration 7310, loss = 0.82670958\n",
            "Iteration 7311, loss = 0.82665722\n",
            "Iteration 7312, loss = 0.82660488\n",
            "Iteration 7313, loss = 0.82655254\n",
            "Iteration 7314, loss = 0.82650020\n",
            "Iteration 7315, loss = 0.82644788\n",
            "Iteration 7316, loss = 0.82639556\n",
            "Iteration 7317, loss = 0.82634324\n",
            "Iteration 7318, loss = 0.82629094\n",
            "Iteration 7319, loss = 0.82623864\n",
            "Iteration 7320, loss = 0.82618634\n",
            "Iteration 7321, loss = 0.82613406\n",
            "Iteration 7322, loss = 0.82608178\n",
            "Iteration 7323, loss = 0.82602950\n",
            "Iteration 7324, loss = 0.82597724\n",
            "Iteration 7325, loss = 0.82592498\n",
            "Iteration 7326, loss = 0.82587273\n",
            "Iteration 7327, loss = 0.82582048\n",
            "Iteration 7328, loss = 0.82576824\n",
            "Iteration 7329, loss = 0.82571601\n",
            "Iteration 7330, loss = 0.82566378\n",
            "Iteration 7331, loss = 0.82561156\n",
            "Iteration 7332, loss = 0.82555935\n",
            "Iteration 7333, loss = 0.82550714\n",
            "Iteration 7334, loss = 0.82545494\n",
            "Iteration 7335, loss = 0.82540275\n",
            "Iteration 7336, loss = 0.82535056\n",
            "Iteration 7337, loss = 0.82529838\n",
            "Iteration 7338, loss = 0.82524621\n",
            "Iteration 7339, loss = 0.82519404\n",
            "Iteration 7340, loss = 0.82514189\n",
            "Iteration 7341, loss = 0.82508973\n",
            "Iteration 7342, loss = 0.82503759\n",
            "Iteration 7343, loss = 0.82498545\n",
            "Iteration 7344, loss = 0.82493332\n",
            "Iteration 7345, loss = 0.82488119\n",
            "Iteration 7346, loss = 0.82482907\n",
            "Iteration 7347, loss = 0.82477696\n",
            "Iteration 7348, loss = 0.82472485\n",
            "Iteration 7349, loss = 0.82467275\n",
            "Iteration 7350, loss = 0.82462066\n",
            "Iteration 7351, loss = 0.82456857\n",
            "Iteration 7352, loss = 0.82451650\n",
            "Iteration 7353, loss = 0.82446442\n",
            "Iteration 7354, loss = 0.82441236\n",
            "Iteration 7355, loss = 0.82436030\n",
            "Iteration 7356, loss = 0.82430825\n",
            "Iteration 7357, loss = 0.82425620\n",
            "Iteration 7358, loss = 0.82420416\n",
            "Iteration 7359, loss = 0.82415213\n",
            "Iteration 7360, loss = 0.82410010\n",
            "Iteration 7361, loss = 0.82404809\n",
            "Iteration 7362, loss = 0.82399607\n",
            "Iteration 7363, loss = 0.82394407\n",
            "Iteration 7364, loss = 0.82389207\n",
            "Iteration 7365, loss = 0.82384008\n",
            "Iteration 7366, loss = 0.82378809\n",
            "Iteration 7367, loss = 0.82373611\n",
            "Iteration 7368, loss = 0.82368414\n",
            "Iteration 7369, loss = 0.82363218\n",
            "Iteration 7370, loss = 0.82358022\n",
            "Iteration 7371, loss = 0.82352827\n",
            "Iteration 7372, loss = 0.82347632\n",
            "Iteration 7373, loss = 0.82342438\n",
            "Iteration 7374, loss = 0.82337245\n",
            "Iteration 7375, loss = 0.82332053\n",
            "Iteration 7376, loss = 0.82326861\n",
            "Iteration 7377, loss = 0.82321670\n",
            "Iteration 7378, loss = 0.82316479\n",
            "Iteration 7379, loss = 0.82311289\n",
            "Iteration 7380, loss = 0.82306100\n",
            "Iteration 7381, loss = 0.82300912\n",
            "Iteration 7382, loss = 0.82295724\n",
            "Iteration 7383, loss = 0.82290537\n",
            "Iteration 7384, loss = 0.82285350\n",
            "Iteration 7385, loss = 0.82280164\n",
            "Iteration 7386, loss = 0.82274979\n",
            "Iteration 7387, loss = 0.82269795\n",
            "Iteration 7388, loss = 0.82264611\n",
            "Iteration 7389, loss = 0.82259428\n",
            "Iteration 7390, loss = 0.82254245\n",
            "Iteration 7391, loss = 0.82249064\n",
            "Iteration 7392, loss = 0.82243883\n",
            "Iteration 7393, loss = 0.82238702\n",
            "Iteration 7394, loss = 0.82233522\n",
            "Iteration 7395, loss = 0.82228343\n",
            "Iteration 7396, loss = 0.82223165\n",
            "Iteration 7397, loss = 0.82217987\n",
            "Iteration 7398, loss = 0.82212810\n",
            "Iteration 7399, loss = 0.82207634\n",
            "Iteration 7400, loss = 0.82202458\n",
            "Iteration 7401, loss = 0.82197283\n",
            "Iteration 7402, loss = 0.82192108\n",
            "Iteration 7403, loss = 0.82186935\n",
            "Iteration 7404, loss = 0.82181762\n",
            "Iteration 7405, loss = 0.82176589\n",
            "Iteration 7406, loss = 0.82171417\n",
            "Iteration 7407, loss = 0.82166246\n",
            "Iteration 7408, loss = 0.82161076\n",
            "Iteration 7409, loss = 0.82155906\n",
            "Iteration 7410, loss = 0.82150737\n",
            "Iteration 7411, loss = 0.82145569\n",
            "Iteration 7412, loss = 0.82140401\n",
            "Iteration 7413, loss = 0.82135234\n",
            "Iteration 7414, loss = 0.82130068\n",
            "Iteration 7415, loss = 0.82124902\n",
            "Iteration 7416, loss = 0.82119737\n",
            "Iteration 7417, loss = 0.82114573\n",
            "Iteration 7418, loss = 0.82109409\n",
            "Iteration 7419, loss = 0.82104246\n",
            "Iteration 7420, loss = 0.82099084\n",
            "Iteration 7421, loss = 0.82093922\n",
            "Iteration 7422, loss = 0.82088762\n",
            "Iteration 7423, loss = 0.82083601\n",
            "Iteration 7424, loss = 0.82078442\n",
            "Iteration 7425, loss = 0.82073283\n",
            "Iteration 7426, loss = 0.82068125\n",
            "Iteration 7427, loss = 0.82062967\n",
            "Iteration 7428, loss = 0.82057810\n",
            "Iteration 7429, loss = 0.82052654\n",
            "Iteration 7430, loss = 0.82047498\n",
            "Iteration 7431, loss = 0.82042343\n",
            "Iteration 7432, loss = 0.82037189\n",
            "Iteration 7433, loss = 0.82032036\n",
            "Iteration 7434, loss = 0.82026883\n",
            "Iteration 7435, loss = 0.82021731\n",
            "Iteration 7436, loss = 0.82016579\n",
            "Iteration 7437, loss = 0.82011428\n",
            "Iteration 7438, loss = 0.82006278\n",
            "Iteration 7439, loss = 0.82001129\n",
            "Iteration 7440, loss = 0.81995980\n",
            "Iteration 7441, loss = 0.81990832\n",
            "Iteration 7442, loss = 0.81985685\n",
            "Iteration 7443, loss = 0.81980538\n",
            "Iteration 7444, loss = 0.81975392\n",
            "Iteration 7445, loss = 0.81970246\n",
            "Iteration 7446, loss = 0.81965102\n",
            "Iteration 7447, loss = 0.81959958\n",
            "Iteration 7448, loss = 0.81954814\n",
            "Iteration 7449, loss = 0.81949672\n",
            "Iteration 7450, loss = 0.81944530\n",
            "Iteration 7451, loss = 0.81939388\n",
            "Iteration 7452, loss = 0.81934248\n",
            "Iteration 7453, loss = 0.81929108\n",
            "Iteration 7454, loss = 0.81923968\n",
            "Iteration 7455, loss = 0.81918830\n",
            "Iteration 7456, loss = 0.81913692\n",
            "Iteration 7457, loss = 0.81908555\n",
            "Iteration 7458, loss = 0.81903418\n",
            "Iteration 7459, loss = 0.81898282\n",
            "Iteration 7460, loss = 0.81893147\n",
            "Iteration 7461, loss = 0.81888012\n",
            "Iteration 7462, loss = 0.81882879\n",
            "Iteration 7463, loss = 0.81877745\n",
            "Iteration 7464, loss = 0.81872613\n",
            "Iteration 7465, loss = 0.81867481\n",
            "Iteration 7466, loss = 0.81862350\n",
            "Iteration 7467, loss = 0.81857220\n",
            "Iteration 7468, loss = 0.81852090\n",
            "Iteration 7469, loss = 0.81846961\n",
            "Iteration 7470, loss = 0.81841832\n",
            "Iteration 7471, loss = 0.81836705\n",
            "Iteration 7472, loss = 0.81831578\n",
            "Iteration 7473, loss = 0.81826451\n",
            "Iteration 7474, loss = 0.81821326\n",
            "Iteration 7475, loss = 0.81816201\n",
            "Iteration 7476, loss = 0.81811076\n",
            "Iteration 7477, loss = 0.81805953\n",
            "Iteration 7478, loss = 0.81800830\n",
            "Iteration 7479, loss = 0.81795708\n",
            "Iteration 7480, loss = 0.81790586\n",
            "Iteration 7481, loss = 0.81785465\n",
            "Iteration 7482, loss = 0.81780345\n",
            "Iteration 7483, loss = 0.81775226\n",
            "Iteration 7484, loss = 0.81770107\n",
            "Iteration 7485, loss = 0.81764989\n",
            "Iteration 7486, loss = 0.81759871\n",
            "Iteration 7487, loss = 0.81754755\n",
            "Iteration 7488, loss = 0.81749639\n",
            "Iteration 7489, loss = 0.81744523\n",
            "Iteration 7490, loss = 0.81739409\n",
            "Iteration 7491, loss = 0.81734295\n",
            "Iteration 7492, loss = 0.81729181\n",
            "Iteration 7493, loss = 0.81724069\n",
            "Iteration 7494, loss = 0.81718957\n",
            "Iteration 7495, loss = 0.81713846\n",
            "Iteration 7496, loss = 0.81708735\n",
            "Iteration 7497, loss = 0.81703625\n",
            "Iteration 7498, loss = 0.81698516\n",
            "Iteration 7499, loss = 0.81693408\n",
            "Iteration 7500, loss = 0.81688300\n",
            "Iteration 7501, loss = 0.81683193\n",
            "Iteration 7502, loss = 0.81678086\n",
            "Iteration 7503, loss = 0.81672981\n",
            "Iteration 7504, loss = 0.81667876\n",
            "Iteration 7505, loss = 0.81662771\n",
            "Iteration 7506, loss = 0.81657668\n",
            "Iteration 7507, loss = 0.81652565\n",
            "Iteration 7508, loss = 0.81647463\n",
            "Iteration 7509, loss = 0.81642361\n",
            "Iteration 7510, loss = 0.81637260\n",
            "Iteration 7511, loss = 0.81632160\n",
            "Iteration 7512, loss = 0.81627061\n",
            "Iteration 7513, loss = 0.81621962\n",
            "Iteration 7514, loss = 0.81616864\n",
            "Iteration 7515, loss = 0.81611766\n",
            "Iteration 7516, loss = 0.81606670\n",
            "Iteration 7517, loss = 0.81601574\n",
            "Iteration 7518, loss = 0.81596478\n",
            "Iteration 7519, loss = 0.81591384\n",
            "Iteration 7520, loss = 0.81586290\n",
            "Iteration 7521, loss = 0.81581197\n",
            "Iteration 7522, loss = 0.81576104\n",
            "Iteration 7523, loss = 0.81571013\n",
            "Iteration 7524, loss = 0.81565921\n",
            "Iteration 7525, loss = 0.81560831\n",
            "Iteration 7526, loss = 0.81555741\n",
            "Iteration 7527, loss = 0.81550652\n",
            "Iteration 7528, loss = 0.81545564\n",
            "Iteration 7529, loss = 0.81540477\n",
            "Iteration 7530, loss = 0.81535390\n",
            "Iteration 7531, loss = 0.81530303\n",
            "Iteration 7532, loss = 0.81525218\n",
            "Iteration 7533, loss = 0.81520133\n",
            "Iteration 7534, loss = 0.81515049\n",
            "Iteration 7535, loss = 0.81509966\n",
            "Iteration 7536, loss = 0.81504883\n",
            "Iteration 7537, loss = 0.81499801\n",
            "Iteration 7538, loss = 0.81494720\n",
            "Iteration 7539, loss = 0.81489639\n",
            "Iteration 7540, loss = 0.81484559\n",
            "Iteration 7541, loss = 0.81479480\n",
            "Iteration 7542, loss = 0.81474402\n",
            "Iteration 7543, loss = 0.81469324\n",
            "Iteration 7544, loss = 0.81464247\n",
            "Iteration 7545, loss = 0.81459171\n",
            "Iteration 7546, loss = 0.81454095\n",
            "Iteration 7547, loss = 0.81449020\n",
            "Iteration 7548, loss = 0.81443946\n",
            "Iteration 7549, loss = 0.81438872\n",
            "Iteration 7550, loss = 0.81433800\n",
            "Iteration 7551, loss = 0.81428728\n",
            "Iteration 7552, loss = 0.81423656\n",
            "Iteration 7553, loss = 0.81418586\n",
            "Iteration 7554, loss = 0.81413516\n",
            "Iteration 7555, loss = 0.81408446\n",
            "Iteration 7556, loss = 0.81403378\n",
            "Iteration 7557, loss = 0.81398310\n",
            "Iteration 7558, loss = 0.81393243\n",
            "Iteration 7559, loss = 0.81388176\n",
            "Iteration 7560, loss = 0.81383111\n",
            "Iteration 7561, loss = 0.81378046\n",
            "Iteration 7562, loss = 0.81372982\n",
            "Iteration 7563, loss = 0.81367918\n",
            "Iteration 7564, loss = 0.81362855\n",
            "Iteration 7565, loss = 0.81357793\n",
            "Iteration 7566, loss = 0.81352732\n",
            "Iteration 7567, loss = 0.81347671\n",
            "Iteration 7568, loss = 0.81342611\n",
            "Iteration 7569, loss = 0.81337552\n",
            "Iteration 7570, loss = 0.81332493\n",
            "Iteration 7571, loss = 0.81327435\n",
            "Iteration 7572, loss = 0.81322378\n",
            "Iteration 7573, loss = 0.81317322\n",
            "Iteration 7574, loss = 0.81312266\n",
            "Iteration 7575, loss = 0.81307211\n",
            "Iteration 7576, loss = 0.81302157\n",
            "Iteration 7577, loss = 0.81297103\n",
            "Iteration 7578, loss = 0.81292051\n",
            "Iteration 7579, loss = 0.81286999\n",
            "Iteration 7580, loss = 0.81281947\n",
            "Iteration 7581, loss = 0.81276897\n",
            "Iteration 7582, loss = 0.81271847\n",
            "Iteration 7583, loss = 0.81266798\n",
            "Iteration 7584, loss = 0.81261749\n",
            "Iteration 7585, loss = 0.81256701\n",
            "Iteration 7586, loss = 0.81251654\n",
            "Iteration 7587, loss = 0.81246608\n",
            "Iteration 7588, loss = 0.81241562\n",
            "Iteration 7589, loss = 0.81236518\n",
            "Iteration 7590, loss = 0.81231473\n",
            "Iteration 7591, loss = 0.81226430\n",
            "Iteration 7592, loss = 0.81221387\n",
            "Iteration 7593, loss = 0.81216345\n",
            "Iteration 7594, loss = 0.81211304\n",
            "Iteration 7595, loss = 0.81206264\n",
            "Iteration 7596, loss = 0.81201224\n",
            "Iteration 7597, loss = 0.81196185\n",
            "Iteration 7598, loss = 0.81191147\n",
            "Iteration 7599, loss = 0.81186109\n",
            "Iteration 7600, loss = 0.81181072\n",
            "Iteration 7601, loss = 0.81176036\n",
            "Iteration 7602, loss = 0.81171001\n",
            "Iteration 7603, loss = 0.81165966\n",
            "Iteration 7604, loss = 0.81160932\n",
            "Iteration 7605, loss = 0.81155899\n",
            "Iteration 7606, loss = 0.81150867\n",
            "Iteration 7607, loss = 0.81145835\n",
            "Iteration 7608, loss = 0.81140804\n",
            "Iteration 7609, loss = 0.81135774\n",
            "Iteration 7610, loss = 0.81130744\n",
            "Iteration 7611, loss = 0.81125715\n",
            "Iteration 7612, loss = 0.81120687\n",
            "Iteration 7613, loss = 0.81115660\n",
            "Iteration 7614, loss = 0.81110633\n",
            "Iteration 7615, loss = 0.81105608\n",
            "Iteration 7616, loss = 0.81100583\n",
            "Iteration 7617, loss = 0.81095558\n",
            "Iteration 7618, loss = 0.81090535\n",
            "Iteration 7619, loss = 0.81085512\n",
            "Iteration 7620, loss = 0.81080490\n",
            "Iteration 7621, loss = 0.81075468\n",
            "Iteration 7622, loss = 0.81070448\n",
            "Iteration 7623, loss = 0.81065428\n",
            "Iteration 7624, loss = 0.81060408\n",
            "Iteration 7625, loss = 0.81055390\n",
            "Iteration 7626, loss = 0.81050372\n",
            "Iteration 7627, loss = 0.81045355\n",
            "Iteration 7628, loss = 0.81040339\n",
            "Iteration 7629, loss = 0.81035324\n",
            "Iteration 7630, loss = 0.81030309\n",
            "Iteration 7631, loss = 0.81025295\n",
            "Iteration 7632, loss = 0.81020282\n",
            "Iteration 7633, loss = 0.81015269\n",
            "Iteration 7634, loss = 0.81010258\n",
            "Iteration 7635, loss = 0.81005247\n",
            "Iteration 7636, loss = 0.81000237\n",
            "Iteration 7637, loss = 0.80995227\n",
            "Iteration 7638, loss = 0.80990218\n",
            "Iteration 7639, loss = 0.80985210\n",
            "Iteration 7640, loss = 0.80980203\n",
            "Iteration 7641, loss = 0.80975197\n",
            "Iteration 7642, loss = 0.80970191\n",
            "Iteration 7643, loss = 0.80965186\n",
            "Iteration 7644, loss = 0.80960182\n",
            "Iteration 7645, loss = 0.80955178\n",
            "Iteration 7646, loss = 0.80950176\n",
            "Iteration 7647, loss = 0.80945174\n",
            "Iteration 7648, loss = 0.80940173\n",
            "Iteration 7649, loss = 0.80935172\n",
            "Iteration 7650, loss = 0.80930172\n",
            "Iteration 7651, loss = 0.80925173\n",
            "Iteration 7652, loss = 0.80920175\n",
            "Iteration 7653, loss = 0.80915178\n",
            "Iteration 7654, loss = 0.80910181\n",
            "Iteration 7655, loss = 0.80905185\n",
            "Iteration 7656, loss = 0.80900190\n",
            "Iteration 7657, loss = 0.80895196\n",
            "Iteration 7658, loss = 0.80890202\n",
            "Iteration 7659, loss = 0.80885209\n",
            "Iteration 7660, loss = 0.80880217\n",
            "Iteration 7661, loss = 0.80875226\n",
            "Iteration 7662, loss = 0.80870235\n",
            "Iteration 7663, loss = 0.80865246\n",
            "Iteration 7664, loss = 0.80860257\n",
            "Iteration 7665, loss = 0.80855268\n",
            "Iteration 7666, loss = 0.80850281\n",
            "Iteration 7667, loss = 0.80845294\n",
            "Iteration 7668, loss = 0.80840308\n",
            "Iteration 7669, loss = 0.80835323\n",
            "Iteration 7670, loss = 0.80830338\n",
            "Iteration 7671, loss = 0.80825355\n",
            "Iteration 7672, loss = 0.80820372\n",
            "Iteration 7673, loss = 0.80815390\n",
            "Iteration 7674, loss = 0.80810408\n",
            "Iteration 7675, loss = 0.80805428\n",
            "Iteration 7676, loss = 0.80800448\n",
            "Iteration 7677, loss = 0.80795469\n",
            "Iteration 7678, loss = 0.80790491\n",
            "Iteration 7679, loss = 0.80785513\n",
            "Iteration 7680, loss = 0.80780536\n",
            "Iteration 7681, loss = 0.80775560\n",
            "Iteration 7682, loss = 0.80770585\n",
            "Iteration 7683, loss = 0.80765611\n",
            "Iteration 7684, loss = 0.80760637\n",
            "Iteration 7685, loss = 0.80755664\n",
            "Iteration 7686, loss = 0.80750692\n",
            "Iteration 7687, loss = 0.80745721\n",
            "Iteration 7688, loss = 0.80740750\n",
            "Iteration 7689, loss = 0.80735781\n",
            "Iteration 7690, loss = 0.80730812\n",
            "Iteration 7691, loss = 0.80725844\n",
            "Iteration 7692, loss = 0.80720876\n",
            "Iteration 7693, loss = 0.80715910\n",
            "Iteration 7694, loss = 0.80710944\n",
            "Iteration 7695, loss = 0.80705979\n",
            "Iteration 7696, loss = 0.80701014\n",
            "Iteration 7697, loss = 0.80696051\n",
            "Iteration 7698, loss = 0.80691088\n",
            "Iteration 7699, loss = 0.80686126\n",
            "Iteration 7700, loss = 0.80681165\n",
            "Iteration 7701, loss = 0.80676205\n",
            "Iteration 7702, loss = 0.80671245\n",
            "Iteration 7703, loss = 0.80666287\n",
            "Iteration 7704, loss = 0.80661329\n",
            "Iteration 7705, loss = 0.80656371\n",
            "Iteration 7706, loss = 0.80651415\n",
            "Iteration 7707, loss = 0.80646459\n",
            "Iteration 7708, loss = 0.80641505\n",
            "Iteration 7709, loss = 0.80636551\n",
            "Iteration 7710, loss = 0.80631597\n",
            "Iteration 7711, loss = 0.80626645\n",
            "Iteration 7712, loss = 0.80621693\n",
            "Iteration 7713, loss = 0.80616742\n",
            "Iteration 7714, loss = 0.80611792\n",
            "Iteration 7715, loss = 0.80606843\n",
            "Iteration 7716, loss = 0.80601895\n",
            "Iteration 7717, loss = 0.80596947\n",
            "Iteration 7718, loss = 0.80592000\n",
            "Iteration 7719, loss = 0.80587054\n",
            "Iteration 7720, loss = 0.80582109\n",
            "Iteration 7721, loss = 0.80577164\n",
            "Iteration 7722, loss = 0.80572221\n",
            "Iteration 7723, loss = 0.80567278\n",
            "Iteration 7724, loss = 0.80562336\n",
            "Iteration 7725, loss = 0.80557394\n",
            "Iteration 7726, loss = 0.80552454\n",
            "Iteration 7727, loss = 0.80547514\n",
            "Iteration 7728, loss = 0.80542575\n",
            "Iteration 7729, loss = 0.80537637\n",
            "Iteration 7730, loss = 0.80532700\n",
            "Iteration 7731, loss = 0.80527764\n",
            "Iteration 7732, loss = 0.80522828\n",
            "Iteration 7733, loss = 0.80517893\n",
            "Iteration 7734, loss = 0.80512959\n",
            "Iteration 7735, loss = 0.80508026\n",
            "Iteration 7736, loss = 0.80503094\n",
            "Iteration 7737, loss = 0.80498162\n",
            "Iteration 7738, loss = 0.80493231\n",
            "Iteration 7739, loss = 0.80488301\n",
            "Iteration 7740, loss = 0.80483372\n",
            "Iteration 7741, loss = 0.80478444\n",
            "Iteration 7742, loss = 0.80473516\n",
            "Iteration 7743, loss = 0.80468590\n",
            "Iteration 7744, loss = 0.80463664\n",
            "Iteration 7745, loss = 0.80458739\n",
            "Iteration 7746, loss = 0.80453814\n",
            "Iteration 7747, loss = 0.80448891\n",
            "Iteration 7748, loss = 0.80443968\n",
            "Iteration 7749, loss = 0.80439046\n",
            "Iteration 7750, loss = 0.80434125\n",
            "Iteration 7751, loss = 0.80429205\n",
            "Iteration 7752, loss = 0.80424286\n",
            "Iteration 7753, loss = 0.80419367\n",
            "Iteration 7754, loss = 0.80414450\n",
            "Iteration 7755, loss = 0.80409533\n",
            "Iteration 7756, loss = 0.80404617\n",
            "Iteration 7757, loss = 0.80399701\n",
            "Iteration 7758, loss = 0.80394787\n",
            "Iteration 7759, loss = 0.80389873\n",
            "Iteration 7760, loss = 0.80384961\n",
            "Iteration 7761, loss = 0.80380049\n",
            "Iteration 7762, loss = 0.80375138\n",
            "Iteration 7763, loss = 0.80370227\n",
            "Iteration 7764, loss = 0.80365318\n",
            "Iteration 7765, loss = 0.80360409\n",
            "Iteration 7766, loss = 0.80355501\n",
            "Iteration 7767, loss = 0.80350594\n",
            "Iteration 7768, loss = 0.80345688\n",
            "Iteration 7769, loss = 0.80340783\n",
            "Iteration 7770, loss = 0.80335879\n",
            "Iteration 7771, loss = 0.80330975\n",
            "Iteration 7772, loss = 0.80326072\n",
            "Iteration 7773, loss = 0.80321170\n",
            "Iteration 7774, loss = 0.80316269\n",
            "Iteration 7775, loss = 0.80311369\n",
            "Iteration 7776, loss = 0.80306469\n",
            "Iteration 7777, loss = 0.80301571\n",
            "Iteration 7778, loss = 0.80296673\n",
            "Iteration 7779, loss = 0.80291776\n",
            "Iteration 7780, loss = 0.80286880\n",
            "Iteration 7781, loss = 0.80281985\n",
            "Iteration 7782, loss = 0.80277090\n",
            "Iteration 7783, loss = 0.80272197\n",
            "Iteration 7784, loss = 0.80267304\n",
            "Iteration 7785, loss = 0.80262412\n",
            "Iteration 7786, loss = 0.80257521\n",
            "Iteration 7787, loss = 0.80252631\n",
            "Iteration 7788, loss = 0.80247741\n",
            "Iteration 7789, loss = 0.80242853\n",
            "Iteration 7790, loss = 0.80237965\n",
            "Iteration 7791, loss = 0.80233078\n",
            "Iteration 7792, loss = 0.80228192\n",
            "Iteration 7793, loss = 0.80223307\n",
            "Iteration 7794, loss = 0.80218423\n",
            "Iteration 7795, loss = 0.80213540\n",
            "Iteration 7796, loss = 0.80208657\n",
            "Iteration 7797, loss = 0.80203775\n",
            "Iteration 7798, loss = 0.80198895\n",
            "Iteration 7799, loss = 0.80194015\n",
            "Iteration 7800, loss = 0.80189135\n",
            "Iteration 7801, loss = 0.80184257\n",
            "Iteration 7802, loss = 0.80179380\n",
            "Iteration 7803, loss = 0.80174503\n",
            "Iteration 7804, loss = 0.80169627\n",
            "Iteration 7805, loss = 0.80164752\n",
            "Iteration 7806, loss = 0.80159878\n",
            "Iteration 7807, loss = 0.80155005\n",
            "Iteration 7808, loss = 0.80150133\n",
            "Iteration 7809, loss = 0.80145262\n",
            "Iteration 7810, loss = 0.80140391\n",
            "Iteration 7811, loss = 0.80135521\n",
            "Iteration 7812, loss = 0.80130653\n",
            "Iteration 7813, loss = 0.80125785\n",
            "Iteration 7814, loss = 0.80120917\n",
            "Iteration 7815, loss = 0.80116051\n",
            "Iteration 7816, loss = 0.80111186\n",
            "Iteration 7817, loss = 0.80106321\n",
            "Iteration 7818, loss = 0.80101458\n",
            "Iteration 7819, loss = 0.80096595\n",
            "Iteration 7820, loss = 0.80091733\n",
            "Iteration 7821, loss = 0.80086872\n",
            "Iteration 7822, loss = 0.80082012\n",
            "Iteration 7823, loss = 0.80077153\n",
            "Iteration 7824, loss = 0.80072294\n",
            "Iteration 7825, loss = 0.80067437\n",
            "Iteration 7826, loss = 0.80062580\n",
            "Iteration 7827, loss = 0.80057724\n",
            "Iteration 7828, loss = 0.80052869\n",
            "Iteration 7829, loss = 0.80048015\n",
            "Iteration 7830, loss = 0.80043162\n",
            "Iteration 7831, loss = 0.80038310\n",
            "Iteration 7832, loss = 0.80033458\n",
            "Iteration 7833, loss = 0.80028608\n",
            "Iteration 7834, loss = 0.80023758\n",
            "Iteration 7835, loss = 0.80018909\n",
            "Iteration 7836, loss = 0.80014062\n",
            "Iteration 7837, loss = 0.80009214\n",
            "Iteration 7838, loss = 0.80004368\n",
            "Iteration 7839, loss = 0.79999523\n",
            "Iteration 7840, loss = 0.79994679\n",
            "Iteration 7841, loss = 0.79989835\n",
            "Iteration 7842, loss = 0.79984993\n",
            "Iteration 7843, loss = 0.79980151\n",
            "Iteration 7844, loss = 0.79975310\n",
            "Iteration 7845, loss = 0.79970470\n",
            "Iteration 7846, loss = 0.79965631\n",
            "Iteration 7847, loss = 0.79960793\n",
            "Iteration 7848, loss = 0.79955956\n",
            "Iteration 7849, loss = 0.79951120\n",
            "Iteration 7850, loss = 0.79946284\n",
            "Iteration 7851, loss = 0.79941449\n",
            "Iteration 7852, loss = 0.79936616\n",
            "Iteration 7853, loss = 0.79931783\n",
            "Iteration 7854, loss = 0.79926951\n",
            "Iteration 7855, loss = 0.79922120\n",
            "Iteration 7856, loss = 0.79917290\n",
            "Iteration 7857, loss = 0.79912461\n",
            "Iteration 7858, loss = 0.79907633\n",
            "Iteration 7859, loss = 0.79902805\n",
            "Iteration 7860, loss = 0.79897979\n",
            "Iteration 7861, loss = 0.79893153\n",
            "Iteration 7862, loss = 0.79888328\n",
            "Iteration 7863, loss = 0.79883504\n",
            "Iteration 7864, loss = 0.79878682\n",
            "Iteration 7865, loss = 0.79873860\n",
            "Iteration 7866, loss = 0.79869038\n",
            "Iteration 7867, loss = 0.79864218\n",
            "Iteration 7868, loss = 0.79859399\n",
            "Iteration 7869, loss = 0.79854581\n",
            "Iteration 7870, loss = 0.79849763\n",
            "Iteration 7871, loss = 0.79844947\n",
            "Iteration 7872, loss = 0.79840131\n",
            "Iteration 7873, loss = 0.79835316\n",
            "Iteration 7874, loss = 0.79830502\n",
            "Iteration 7875, loss = 0.79825689\n",
            "Iteration 7876, loss = 0.79820877\n",
            "Iteration 7877, loss = 0.79816066\n",
            "Iteration 7878, loss = 0.79811256\n",
            "Iteration 7879, loss = 0.79806447\n",
            "Iteration 7880, loss = 0.79801639\n",
            "Iteration 7881, loss = 0.79796831\n",
            "Iteration 7882, loss = 0.79792025\n",
            "Iteration 7883, loss = 0.79787219\n",
            "Iteration 7884, loss = 0.79782414\n",
            "Iteration 7885, loss = 0.79777611\n",
            "Iteration 7886, loss = 0.79772808\n",
            "Iteration 7887, loss = 0.79768006\n",
            "Iteration 7888, loss = 0.79763205\n",
            "Iteration 7889, loss = 0.79758405\n",
            "Iteration 7890, loss = 0.79753606\n",
            "Iteration 7891, loss = 0.79748807\n",
            "Iteration 7892, loss = 0.79744010\n",
            "Iteration 7893, loss = 0.79739214\n",
            "Iteration 7894, loss = 0.79734418\n",
            "Iteration 7895, loss = 0.79729624\n",
            "Iteration 7896, loss = 0.79724830\n",
            "Iteration 7897, loss = 0.79720037\n",
            "Iteration 7898, loss = 0.79715246\n",
            "Iteration 7899, loss = 0.79710455\n",
            "Iteration 7900, loss = 0.79705665\n",
            "Iteration 7901, loss = 0.79700876\n",
            "Iteration 7902, loss = 0.79696088\n",
            "Iteration 7903, loss = 0.79691301\n",
            "Iteration 7904, loss = 0.79686515\n",
            "Iteration 7905, loss = 0.79681729\n",
            "Iteration 7906, loss = 0.79676945\n",
            "Iteration 7907, loss = 0.79672162\n",
            "Iteration 7908, loss = 0.79667379\n",
            "Iteration 7909, loss = 0.79662598\n",
            "Iteration 7910, loss = 0.79657817\n",
            "Iteration 7911, loss = 0.79653038\n",
            "Iteration 7912, loss = 0.79648259\n",
            "Iteration 7913, loss = 0.79643481\n",
            "Iteration 7914, loss = 0.79638705\n",
            "Iteration 7915, loss = 0.79633929\n",
            "Iteration 7916, loss = 0.79629154\n",
            "Iteration 7917, loss = 0.79624380\n",
            "Iteration 7918, loss = 0.79619607\n",
            "Iteration 7919, loss = 0.79614835\n",
            "Iteration 7920, loss = 0.79610064\n",
            "Iteration 7921, loss = 0.79605294\n",
            "Iteration 7922, loss = 0.79600524\n",
            "Iteration 7923, loss = 0.79595756\n",
            "Iteration 7924, loss = 0.79590989\n",
            "Iteration 7925, loss = 0.79586222\n",
            "Iteration 7926, loss = 0.79581457\n",
            "Iteration 7927, loss = 0.79576693\n",
            "Iteration 7928, loss = 0.79571929\n",
            "Iteration 7929, loss = 0.79567167\n",
            "Iteration 7930, loss = 0.79562405\n",
            "Iteration 7931, loss = 0.79557644\n",
            "Iteration 7932, loss = 0.79552885\n",
            "Iteration 7933, loss = 0.79548126\n",
            "Iteration 7934, loss = 0.79543368\n",
            "Iteration 7935, loss = 0.79538611\n",
            "Iteration 7936, loss = 0.79533855\n",
            "Iteration 7937, loss = 0.79529100\n",
            "Iteration 7938, loss = 0.79524347\n",
            "Iteration 7939, loss = 0.79519594\n",
            "Iteration 7940, loss = 0.79514842\n",
            "Iteration 7941, loss = 0.79510090\n",
            "Iteration 7942, loss = 0.79505340\n",
            "Iteration 7943, loss = 0.79500591\n",
            "Iteration 7944, loss = 0.79495843\n",
            "Iteration 7945, loss = 0.79491096\n",
            "Iteration 7946, loss = 0.79486350\n",
            "Iteration 7947, loss = 0.79481604\n",
            "Iteration 7948, loss = 0.79476860\n",
            "Iteration 7949, loss = 0.79472117\n",
            "Iteration 7950, loss = 0.79467374\n",
            "Iteration 7951, loss = 0.79462633\n",
            "Iteration 7952, loss = 0.79457893\n",
            "Iteration 7953, loss = 0.79453153\n",
            "Iteration 7954, loss = 0.79448415\n",
            "Iteration 7955, loss = 0.79443677\n",
            "Iteration 7956, loss = 0.79438941\n",
            "Iteration 7957, loss = 0.79434205\n",
            "Iteration 7958, loss = 0.79429471\n",
            "Iteration 7959, loss = 0.79424737\n",
            "Iteration 7960, loss = 0.79420004\n",
            "Iteration 7961, loss = 0.79415273\n",
            "Iteration 7962, loss = 0.79410542\n",
            "Iteration 7963, loss = 0.79405812\n",
            "Iteration 7964, loss = 0.79401084\n",
            "Iteration 7965, loss = 0.79396356\n",
            "Iteration 7966, loss = 0.79391629\n",
            "Iteration 7967, loss = 0.79386903\n",
            "Iteration 7968, loss = 0.79382179\n",
            "Iteration 7969, loss = 0.79377455\n",
            "Iteration 7970, loss = 0.79372732\n",
            "Iteration 7971, loss = 0.79368010\n",
            "Iteration 7972, loss = 0.79363289\n",
            "Iteration 7973, loss = 0.79358570\n",
            "Iteration 7974, loss = 0.79353851\n",
            "Iteration 7975, loss = 0.79349133\n",
            "Iteration 7976, loss = 0.79344416\n",
            "Iteration 7977, loss = 0.79339700\n",
            "Iteration 7978, loss = 0.79334985\n",
            "Iteration 7979, loss = 0.79330271\n",
            "Iteration 7980, loss = 0.79325558\n",
            "Iteration 7981, loss = 0.79320847\n",
            "Iteration 7982, loss = 0.79316136\n",
            "Iteration 7983, loss = 0.79311426\n",
            "Iteration 7984, loss = 0.79306717\n",
            "Iteration 7985, loss = 0.79302009\n",
            "Iteration 7986, loss = 0.79297302\n",
            "Iteration 7987, loss = 0.79292596\n",
            "Iteration 7988, loss = 0.79287891\n",
            "Iteration 7989, loss = 0.79283187\n",
            "Iteration 7990, loss = 0.79278484\n",
            "Iteration 7991, loss = 0.79273782\n",
            "Iteration 7992, loss = 0.79269081\n",
            "Iteration 7993, loss = 0.79264381\n",
            "Iteration 7994, loss = 0.79259683\n",
            "Iteration 7995, loss = 0.79254985\n",
            "Iteration 7996, loss = 0.79250288\n",
            "Iteration 7997, loss = 0.79245592\n",
            "Iteration 7998, loss = 0.79240897\n",
            "Iteration 7999, loss = 0.79236203\n",
            "Iteration 8000, loss = 0.79231510\n",
            "Iteration 8001, loss = 0.79226818\n",
            "Iteration 8002, loss = 0.79222127\n",
            "Iteration 8003, loss = 0.79217438\n",
            "Iteration 8004, loss = 0.79212749\n",
            "Iteration 8005, loss = 0.79208061\n",
            "Iteration 8006, loss = 0.79203374\n",
            "Iteration 8007, loss = 0.79198688\n",
            "Iteration 8008, loss = 0.79194004\n",
            "Iteration 8009, loss = 0.79189320\n",
            "Iteration 8010, loss = 0.79184637\n",
            "Iteration 8011, loss = 0.79179955\n",
            "Iteration 8012, loss = 0.79175275\n",
            "Iteration 8013, loss = 0.79170595\n",
            "Iteration 8014, loss = 0.79165916\n",
            "Iteration 8015, loss = 0.79161239\n",
            "Iteration 8016, loss = 0.79156562\n",
            "Iteration 8017, loss = 0.79151886\n",
            "Iteration 8018, loss = 0.79147212\n",
            "Iteration 8019, loss = 0.79142538\n",
            "Iteration 8020, loss = 0.79137866\n",
            "Iteration 8021, loss = 0.79133194\n",
            "Iteration 8022, loss = 0.79128524\n",
            "Iteration 8023, loss = 0.79123854\n",
            "Iteration 8024, loss = 0.79119186\n",
            "Iteration 8025, loss = 0.79114518\n",
            "Iteration 8026, loss = 0.79109852\n",
            "Iteration 8027, loss = 0.79105187\n",
            "Iteration 8028, loss = 0.79100522\n",
            "Iteration 8029, loss = 0.79095859\n",
            "Iteration 8030, loss = 0.79091197\n",
            "Iteration 8031, loss = 0.79086535\n",
            "Iteration 8032, loss = 0.79081875\n",
            "Iteration 8033, loss = 0.79077216\n",
            "Iteration 8034, loss = 0.79072558\n",
            "Iteration 8035, loss = 0.79067901\n",
            "Iteration 8036, loss = 0.79063245\n",
            "Iteration 8037, loss = 0.79058590\n",
            "Iteration 8038, loss = 0.79053936\n",
            "Iteration 8039, loss = 0.79049283\n",
            "Iteration 8040, loss = 0.79044631\n",
            "Iteration 8041, loss = 0.79039980\n",
            "Iteration 8042, loss = 0.79035330\n",
            "Iteration 8043, loss = 0.79030682\n",
            "Iteration 8044, loss = 0.79026034\n",
            "Iteration 8045, loss = 0.79021387\n",
            "Iteration 8046, loss = 0.79016741\n",
            "Iteration 8047, loss = 0.79012097\n",
            "Iteration 8048, loss = 0.79007453\n",
            "Iteration 8049, loss = 0.79002811\n",
            "Iteration 8050, loss = 0.78998169\n",
            "Iteration 8051, loss = 0.78993529\n",
            "Iteration 8052, loss = 0.78988890\n",
            "Iteration 8053, loss = 0.78984251\n",
            "Iteration 8054, loss = 0.78979614\n",
            "Iteration 8055, loss = 0.78974978\n",
            "Iteration 8056, loss = 0.78970343\n",
            "Iteration 8057, loss = 0.78965708\n",
            "Iteration 8058, loss = 0.78961075\n",
            "Iteration 8059, loss = 0.78956443\n",
            "Iteration 8060, loss = 0.78951812\n",
            "Iteration 8061, loss = 0.78947183\n",
            "Iteration 8062, loss = 0.78942554\n",
            "Iteration 8063, loss = 0.78937926\n",
            "Iteration 8064, loss = 0.78933299\n",
            "Iteration 8065, loss = 0.78928674\n",
            "Iteration 8066, loss = 0.78924049\n",
            "Iteration 8067, loss = 0.78919425\n",
            "Iteration 8068, loss = 0.78914803\n",
            "Iteration 8069, loss = 0.78910181\n",
            "Iteration 8070, loss = 0.78905561\n",
            "Iteration 8071, loss = 0.78900942\n",
            "Iteration 8072, loss = 0.78896324\n",
            "Iteration 8073, loss = 0.78891706\n",
            "Iteration 8074, loss = 0.78887090\n",
            "Iteration 8075, loss = 0.78882475\n",
            "Iteration 8076, loss = 0.78877861\n",
            "Iteration 8077, loss = 0.78873248\n",
            "Iteration 8078, loss = 0.78868636\n",
            "Iteration 8079, loss = 0.78864026\n",
            "Iteration 8080, loss = 0.78859416\n",
            "Iteration 8081, loss = 0.78854807\n",
            "Iteration 8082, loss = 0.78850200\n",
            "Iteration 8083, loss = 0.78845593\n",
            "Iteration 8084, loss = 0.78840988\n",
            "Iteration 8085, loss = 0.78836383\n",
            "Iteration 8086, loss = 0.78831780\n",
            "Iteration 8087, loss = 0.78827178\n",
            "Iteration 8088, loss = 0.78822577\n",
            "Iteration 8089, loss = 0.78817977\n",
            "Iteration 8090, loss = 0.78813378\n",
            "Iteration 8091, loss = 0.78808780\n",
            "Iteration 8092, loss = 0.78804183\n",
            "Iteration 8093, loss = 0.78799587\n",
            "Iteration 8094, loss = 0.78794993\n",
            "Iteration 8095, loss = 0.78790399\n",
            "Iteration 8096, loss = 0.78785807\n",
            "Iteration 8097, loss = 0.78781215\n",
            "Iteration 8098, loss = 0.78776625\n",
            "Iteration 8099, loss = 0.78772036\n",
            "Iteration 8100, loss = 0.78767447\n",
            "Iteration 8101, loss = 0.78762860\n",
            "Iteration 8102, loss = 0.78758274\n",
            "Iteration 8103, loss = 0.78753689\n",
            "Iteration 8104, loss = 0.78749106\n",
            "Iteration 8105, loss = 0.78744523\n",
            "Iteration 8106, loss = 0.78739941\n",
            "Iteration 8107, loss = 0.78735361\n",
            "Iteration 8108, loss = 0.78730781\n",
            "Iteration 8109, loss = 0.78726203\n",
            "Iteration 8110, loss = 0.78721625\n",
            "Iteration 8111, loss = 0.78717049\n",
            "Iteration 8112, loss = 0.78712474\n",
            "Iteration 8113, loss = 0.78707900\n",
            "Iteration 8114, loss = 0.78703327\n",
            "Iteration 8115, loss = 0.78698755\n",
            "Iteration 8116, loss = 0.78694185\n",
            "Iteration 8117, loss = 0.78689615\n",
            "Iteration 8118, loss = 0.78685046\n",
            "Iteration 8119, loss = 0.78680479\n",
            "Iteration 8120, loss = 0.78675913\n",
            "Iteration 8121, loss = 0.78671347\n",
            "Iteration 8122, loss = 0.78666783\n",
            "Iteration 8123, loss = 0.78662220\n",
            "Iteration 8124, loss = 0.78657658\n",
            "Iteration 8125, loss = 0.78653097\n",
            "Iteration 8126, loss = 0.78648538\n",
            "Iteration 8127, loss = 0.78643979\n",
            "Iteration 8128, loss = 0.78639421\n",
            "Iteration 8129, loss = 0.78634865\n",
            "Iteration 8130, loss = 0.78630310\n",
            "Iteration 8131, loss = 0.78625755\n",
            "Iteration 8132, loss = 0.78621202\n",
            "Iteration 8133, loss = 0.78616650\n",
            "Iteration 8134, loss = 0.78612099\n",
            "Iteration 8135, loss = 0.78607550\n",
            "Iteration 8136, loss = 0.78603001\n",
            "Iteration 8137, loss = 0.78598453\n",
            "Iteration 8138, loss = 0.78593907\n",
            "Iteration 8139, loss = 0.78589362\n",
            "Iteration 8140, loss = 0.78584817\n",
            "Iteration 8141, loss = 0.78580274\n",
            "Iteration 8142, loss = 0.78575732\n",
            "Iteration 8143, loss = 0.78571191\n",
            "Iteration 8144, loss = 0.78566651\n",
            "Iteration 8145, loss = 0.78562113\n",
            "Iteration 8146, loss = 0.78557575\n",
            "Iteration 8147, loss = 0.78553039\n",
            "Iteration 8148, loss = 0.78548503\n",
            "Iteration 8149, loss = 0.78543969\n",
            "Iteration 8150, loss = 0.78539436\n",
            "Iteration 8151, loss = 0.78534904\n",
            "Iteration 8152, loss = 0.78530373\n",
            "Iteration 8153, loss = 0.78525844\n",
            "Iteration 8154, loss = 0.78521315\n",
            "Iteration 8155, loss = 0.78516788\n",
            "Iteration 8156, loss = 0.78512261\n",
            "Iteration 8157, loss = 0.78507736\n",
            "Iteration 8158, loss = 0.78503212\n",
            "Iteration 8159, loss = 0.78498689\n",
            "Iteration 8160, loss = 0.78494167\n",
            "Iteration 8161, loss = 0.78489646\n",
            "Iteration 8162, loss = 0.78485127\n",
            "Iteration 8163, loss = 0.78480608\n",
            "Iteration 8164, loss = 0.78476091\n",
            "Iteration 8165, loss = 0.78471575\n",
            "Iteration 8166, loss = 0.78467060\n",
            "Iteration 8167, loss = 0.78462546\n",
            "Iteration 8168, loss = 0.78458033\n",
            "Iteration 8169, loss = 0.78453521\n",
            "Iteration 8170, loss = 0.78449011\n",
            "Iteration 8171, loss = 0.78444501\n",
            "Iteration 8172, loss = 0.78439993\n",
            "Iteration 8173, loss = 0.78435486\n",
            "Iteration 8174, loss = 0.78430980\n",
            "Iteration 8175, loss = 0.78426475\n",
            "Iteration 8176, loss = 0.78421971\n",
            "Iteration 8177, loss = 0.78417468\n",
            "Iteration 8178, loss = 0.78412967\n",
            "Iteration 8179, loss = 0.78408466\n",
            "Iteration 8180, loss = 0.78403967\n",
            "Iteration 8181, loss = 0.78399469\n",
            "Iteration 8182, loss = 0.78394972\n",
            "Iteration 8183, loss = 0.78390476\n",
            "Iteration 8184, loss = 0.78385982\n",
            "Iteration 8185, loss = 0.78381488\n",
            "Iteration 8186, loss = 0.78376996\n",
            "Iteration 8187, loss = 0.78372505\n",
            "Iteration 8188, loss = 0.78368015\n",
            "Iteration 8189, loss = 0.78363526\n",
            "Iteration 8190, loss = 0.78359038\n",
            "Iteration 8191, loss = 0.78354551\n",
            "Iteration 8192, loss = 0.78350066\n",
            "Iteration 8193, loss = 0.78345582\n",
            "Iteration 8194, loss = 0.78341098\n",
            "Iteration 8195, loss = 0.78336616\n",
            "Iteration 8196, loss = 0.78332135\n",
            "Iteration 8197, loss = 0.78327656\n",
            "Iteration 8198, loss = 0.78323177\n",
            "Iteration 8199, loss = 0.78318700\n",
            "Iteration 8200, loss = 0.78314223\n",
            "Iteration 8201, loss = 0.78309748\n",
            "Iteration 8202, loss = 0.78305274\n",
            "Iteration 8203, loss = 0.78300801\n",
            "Iteration 8204, loss = 0.78296330\n",
            "Iteration 8205, loss = 0.78291859\n",
            "Iteration 8206, loss = 0.78287390\n",
            "Iteration 8207, loss = 0.78282922\n",
            "Iteration 8208, loss = 0.78278454\n",
            "Iteration 8209, loss = 0.78273989\n",
            "Iteration 8210, loss = 0.78269524\n",
            "Iteration 8211, loss = 0.78265060\n",
            "Iteration 8212, loss = 0.78260598\n",
            "Iteration 8213, loss = 0.78256137\n",
            "Iteration 8214, loss = 0.78251676\n",
            "Iteration 8215, loss = 0.78247218\n",
            "Iteration 8216, loss = 0.78242760\n",
            "Iteration 8217, loss = 0.78238303\n",
            "Iteration 8218, loss = 0.78233848\n",
            "Iteration 8219, loss = 0.78229393\n",
            "Iteration 8220, loss = 0.78224940\n",
            "Iteration 8221, loss = 0.78220488\n",
            "Iteration 8222, loss = 0.78216037\n",
            "Iteration 8223, loss = 0.78211588\n",
            "Iteration 8224, loss = 0.78207139\n",
            "Iteration 8225, loss = 0.78202692\n",
            "Iteration 8226, loss = 0.78198246\n",
            "Iteration 8227, loss = 0.78193801\n",
            "Iteration 8228, loss = 0.78189357\n",
            "Iteration 8229, loss = 0.78184915\n",
            "Iteration 8230, loss = 0.78180473\n",
            "Iteration 8231, loss = 0.78176033\n",
            "Iteration 8232, loss = 0.78171594\n",
            "Iteration 8233, loss = 0.78167156\n",
            "Iteration 8234, loss = 0.78162719\n",
            "Iteration 8235, loss = 0.78158283\n",
            "Iteration 8236, loss = 0.78153849\n",
            "Iteration 8237, loss = 0.78149416\n",
            "Iteration 8238, loss = 0.78144984\n",
            "Iteration 8239, loss = 0.78140553\n",
            "Iteration 8240, loss = 0.78136123\n",
            "Iteration 8241, loss = 0.78131695\n",
            "Iteration 8242, loss = 0.78127267\n",
            "Iteration 8243, loss = 0.78122841\n",
            "Iteration 8244, loss = 0.78118416\n",
            "Iteration 8245, loss = 0.78113992\n",
            "Iteration 8246, loss = 0.78109570\n",
            "Iteration 8247, loss = 0.78105148\n",
            "Iteration 8248, loss = 0.78100728\n",
            "Iteration 8249, loss = 0.78096309\n",
            "Iteration 8250, loss = 0.78091891\n",
            "Iteration 8251, loss = 0.78087474\n",
            "Iteration 8252, loss = 0.78083058\n",
            "Iteration 8253, loss = 0.78078644\n",
            "Iteration 8254, loss = 0.78074231\n",
            "Iteration 8255, loss = 0.78069819\n",
            "Iteration 8256, loss = 0.78065408\n",
            "Iteration 8257, loss = 0.78060998\n",
            "Iteration 8258, loss = 0.78056590\n",
            "Iteration 8259, loss = 0.78052183\n",
            "Iteration 8260, loss = 0.78047777\n",
            "Iteration 8261, loss = 0.78043372\n",
            "Iteration 8262, loss = 0.78038968\n",
            "Iteration 8263, loss = 0.78034566\n",
            "Iteration 8264, loss = 0.78030164\n",
            "Iteration 8265, loss = 0.78025764\n",
            "Iteration 8266, loss = 0.78021365\n",
            "Iteration 8267, loss = 0.78016967\n",
            "Iteration 8268, loss = 0.78012571\n",
            "Iteration 8269, loss = 0.78008176\n",
            "Iteration 8270, loss = 0.78003781\n",
            "Iteration 8271, loss = 0.77999388\n",
            "Iteration 8272, loss = 0.77994997\n",
            "Iteration 8273, loss = 0.77990606\n",
            "Iteration 8274, loss = 0.77986217\n",
            "Iteration 8275, loss = 0.77981829\n",
            "Iteration 8276, loss = 0.77977442\n",
            "Iteration 8277, loss = 0.77973056\n",
            "Iteration 8278, loss = 0.77968671\n",
            "Iteration 8279, loss = 0.77964288\n",
            "Iteration 8280, loss = 0.77959906\n",
            "Iteration 8281, loss = 0.77955525\n",
            "Iteration 8282, loss = 0.77951145\n",
            "Iteration 8283, loss = 0.77946766\n",
            "Iteration 8284, loss = 0.77942389\n",
            "Iteration 8285, loss = 0.77938013\n",
            "Iteration 8286, loss = 0.77933638\n",
            "Iteration 8287, loss = 0.77929264\n",
            "Iteration 8288, loss = 0.77924891\n",
            "Iteration 8289, loss = 0.77920520\n",
            "Iteration 8290, loss = 0.77916150\n",
            "Iteration 8291, loss = 0.77911781\n",
            "Iteration 8292, loss = 0.77907413\n",
            "Iteration 8293, loss = 0.77903046\n",
            "Iteration 8294, loss = 0.77898681\n",
            "Iteration 8295, loss = 0.77894317\n",
            "Iteration 8296, loss = 0.77889954\n",
            "Iteration 8297, loss = 0.77885592\n",
            "Iteration 8298, loss = 0.77881232\n",
            "Iteration 8299, loss = 0.77876872\n",
            "Iteration 8300, loss = 0.77872514\n",
            "Iteration 8301, loss = 0.77868157\n",
            "Iteration 8302, loss = 0.77863802\n",
            "Iteration 8303, loss = 0.77859447\n",
            "Iteration 8304, loss = 0.77855094\n",
            "Iteration 8305, loss = 0.77850742\n",
            "Iteration 8306, loss = 0.77846391\n",
            "Iteration 8307, loss = 0.77842041\n",
            "Iteration 8308, loss = 0.77837693\n",
            "Iteration 8309, loss = 0.77833346\n",
            "Iteration 8310, loss = 0.77829000\n",
            "Iteration 8311, loss = 0.77824655\n",
            "Iteration 8312, loss = 0.77820311\n",
            "Iteration 8313, loss = 0.77815969\n",
            "Iteration 8314, loss = 0.77811628\n",
            "Iteration 8315, loss = 0.77807288\n",
            "Iteration 8316, loss = 0.77802949\n",
            "Iteration 8317, loss = 0.77798612\n",
            "Iteration 8318, loss = 0.77794276\n",
            "Iteration 8319, loss = 0.77789941\n",
            "Iteration 8320, loss = 0.77785607\n",
            "Iteration 8321, loss = 0.77781274\n",
            "Iteration 8322, loss = 0.77776943\n",
            "Iteration 8323, loss = 0.77772613\n",
            "Iteration 8324, loss = 0.77768284\n",
            "Iteration 8325, loss = 0.77763956\n",
            "Iteration 8326, loss = 0.77759630\n",
            "Iteration 8327, loss = 0.77755304\n",
            "Iteration 8328, loss = 0.77750980\n",
            "Iteration 8329, loss = 0.77746658\n",
            "Iteration 8330, loss = 0.77742336\n",
            "Iteration 8331, loss = 0.77738016\n",
            "Iteration 8332, loss = 0.77733696\n",
            "Iteration 8333, loss = 0.77729379\n",
            "Iteration 8334, loss = 0.77725062\n",
            "Iteration 8335, loss = 0.77720746\n",
            "Iteration 8336, loss = 0.77716432\n",
            "Iteration 8337, loss = 0.77712119\n",
            "Iteration 8338, loss = 0.77707807\n",
            "Iteration 8339, loss = 0.77703497\n",
            "Iteration 8340, loss = 0.77699188\n",
            "Iteration 8341, loss = 0.77694879\n",
            "Iteration 8342, loss = 0.77690573\n",
            "Iteration 8343, loss = 0.77686267\n",
            "Iteration 8344, loss = 0.77681963\n",
            "Iteration 8345, loss = 0.77677659\n",
            "Iteration 8346, loss = 0.77673358\n",
            "Iteration 8347, loss = 0.77669057\n",
            "Iteration 8348, loss = 0.77664757\n",
            "Iteration 8349, loss = 0.77660459\n",
            "Iteration 8350, loss = 0.77656162\n",
            "Iteration 8351, loss = 0.77651866\n",
            "Iteration 8352, loss = 0.77647572\n",
            "Iteration 8353, loss = 0.77643279\n",
            "Iteration 8354, loss = 0.77638987\n",
            "Iteration 8355, loss = 0.77634696\n",
            "Iteration 8356, loss = 0.77630406\n",
            "Iteration 8357, loss = 0.77626118\n",
            "Iteration 8358, loss = 0.77621831\n",
            "Iteration 8359, loss = 0.77617545\n",
            "Iteration 8360, loss = 0.77613260\n",
            "Iteration 8361, loss = 0.77608977\n",
            "Iteration 8362, loss = 0.77604695\n",
            "Iteration 8363, loss = 0.77600414\n",
            "Iteration 8364, loss = 0.77596134\n",
            "Iteration 8365, loss = 0.77591856\n",
            "Iteration 8366, loss = 0.77587578\n",
            "Iteration 8367, loss = 0.77583303\n",
            "Iteration 8368, loss = 0.77579028\n",
            "Iteration 8369, loss = 0.77574754\n",
            "Iteration 8370, loss = 0.77570482\n",
            "Iteration 8371, loss = 0.77566211\n",
            "Iteration 8372, loss = 0.77561941\n",
            "Iteration 8373, loss = 0.77557673\n",
            "Iteration 8374, loss = 0.77553406\n",
            "Iteration 8375, loss = 0.77549140\n",
            "Iteration 8376, loss = 0.77544875\n",
            "Iteration 8377, loss = 0.77540611\n",
            "Iteration 8378, loss = 0.77536349\n",
            "Iteration 8379, loss = 0.77532088\n",
            "Iteration 8380, loss = 0.77527828\n",
            "Iteration 8381, loss = 0.77523570\n",
            "Iteration 8382, loss = 0.77519313\n",
            "Iteration 8383, loss = 0.77515057\n",
            "Iteration 8384, loss = 0.77510802\n",
            "Iteration 8385, loss = 0.77506548\n",
            "Iteration 8386, loss = 0.77502296\n",
            "Iteration 8387, loss = 0.77498045\n",
            "Iteration 8388, loss = 0.77493795\n",
            "Iteration 8389, loss = 0.77489547\n",
            "Iteration 8390, loss = 0.77485300\n",
            "Iteration 8391, loss = 0.77481054\n",
            "Iteration 8392, loss = 0.77476809\n",
            "Iteration 8393, loss = 0.77472565\n",
            "Iteration 8394, loss = 0.77468323\n",
            "Iteration 8395, loss = 0.77464082\n",
            "Iteration 8396, loss = 0.77459842\n",
            "Iteration 8397, loss = 0.77455604\n",
            "Iteration 8398, loss = 0.77451366\n",
            "Iteration 8399, loss = 0.77447130\n",
            "Iteration 8400, loss = 0.77442896\n",
            "Iteration 8401, loss = 0.77438662\n",
            "Iteration 8402, loss = 0.77434430\n",
            "Iteration 8403, loss = 0.77430199\n",
            "Iteration 8404, loss = 0.77425969\n",
            "Iteration 8405, loss = 0.77421741\n",
            "Iteration 8406, loss = 0.77417514\n",
            "Iteration 8407, loss = 0.77413288\n",
            "Iteration 8408, loss = 0.77409063\n",
            "Iteration 8409, loss = 0.77404840\n",
            "Iteration 8410, loss = 0.77400618\n",
            "Iteration 8411, loss = 0.77396397\n",
            "Iteration 8412, loss = 0.77392177\n",
            "Iteration 8413, loss = 0.77387959\n",
            "Iteration 8414, loss = 0.77383742\n",
            "Iteration 8415, loss = 0.77379526\n",
            "Iteration 8416, loss = 0.77375311\n",
            "Iteration 8417, loss = 0.77371098\n",
            "Iteration 8418, loss = 0.77366886\n",
            "Iteration 8419, loss = 0.77362675\n",
            "Iteration 8420, loss = 0.77358465\n",
            "Iteration 8421, loss = 0.77354257\n",
            "Iteration 8422, loss = 0.77350050\n",
            "Iteration 8423, loss = 0.77345844\n",
            "Iteration 8424, loss = 0.77341640\n",
            "Iteration 8425, loss = 0.77337437\n",
            "Iteration 8426, loss = 0.77333235\n",
            "Iteration 8427, loss = 0.77329034\n",
            "Iteration 8428, loss = 0.77324835\n",
            "Iteration 8429, loss = 0.77320636\n",
            "Iteration 8430, loss = 0.77316439\n",
            "Iteration 8431, loss = 0.77312244\n",
            "Iteration 8432, loss = 0.77308049\n",
            "Iteration 8433, loss = 0.77303856\n",
            "Iteration 8434, loss = 0.77299665\n",
            "Iteration 8435, loss = 0.77295474\n",
            "Iteration 8436, loss = 0.77291285\n",
            "Iteration 8437, loss = 0.77287097\n",
            "Iteration 8438, loss = 0.77282910\n",
            "Iteration 8439, loss = 0.77278724\n",
            "Iteration 8440, loss = 0.77274540\n",
            "Iteration 8441, loss = 0.77270357\n",
            "Iteration 8442, loss = 0.77266176\n",
            "Iteration 8443, loss = 0.77261995\n",
            "Iteration 8444, loss = 0.77257816\n",
            "Iteration 8445, loss = 0.77253638\n",
            "Iteration 8446, loss = 0.77249462\n",
            "Iteration 8447, loss = 0.77245286\n",
            "Iteration 8448, loss = 0.77241112\n",
            "Iteration 8449, loss = 0.77236940\n",
            "Iteration 8450, loss = 0.77232768\n",
            "Iteration 8451, loss = 0.77228598\n",
            "Iteration 8452, loss = 0.77224429\n",
            "Iteration 8453, loss = 0.77220261\n",
            "Iteration 8454, loss = 0.77216095\n",
            "Iteration 8455, loss = 0.77211930\n",
            "Iteration 8456, loss = 0.77207766\n",
            "Iteration 8457, loss = 0.77203603\n",
            "Iteration 8458, loss = 0.77199442\n",
            "Iteration 8459, loss = 0.77195282\n",
            "Iteration 8460, loss = 0.77191123\n",
            "Iteration 8461, loss = 0.77186966\n",
            "Iteration 8462, loss = 0.77182810\n",
            "Iteration 8463, loss = 0.77178655\n",
            "Iteration 8464, loss = 0.77174501\n",
            "Iteration 8465, loss = 0.77170349\n",
            "Iteration 8466, loss = 0.77166198\n",
            "Iteration 8467, loss = 0.77162048\n",
            "Iteration 8468, loss = 0.77157899\n",
            "Iteration 8469, loss = 0.77153752\n",
            "Iteration 8470, loss = 0.77149606\n",
            "Iteration 8471, loss = 0.77145462\n",
            "Iteration 8472, loss = 0.77141318\n",
            "Iteration 8473, loss = 0.77137176\n",
            "Iteration 8474, loss = 0.77133035\n",
            "Iteration 8475, loss = 0.77128896\n",
            "Iteration 8476, loss = 0.77124757\n",
            "Iteration 8477, loss = 0.77120620\n",
            "Iteration 8478, loss = 0.77116485\n",
            "Iteration 8479, loss = 0.77112350\n",
            "Iteration 8480, loss = 0.77108217\n",
            "Iteration 8481, loss = 0.77104085\n",
            "Iteration 8482, loss = 0.77099955\n",
            "Iteration 8483, loss = 0.77095825\n",
            "Iteration 8484, loss = 0.77091697\n",
            "Iteration 8485, loss = 0.77087571\n",
            "Iteration 8486, loss = 0.77083445\n",
            "Iteration 8487, loss = 0.77079321\n",
            "Iteration 8488, loss = 0.77075198\n",
            "Iteration 8489, loss = 0.77071077\n",
            "Iteration 8490, loss = 0.77066956\n",
            "Iteration 8491, loss = 0.77062837\n",
            "Iteration 8492, loss = 0.77058720\n",
            "Iteration 8493, loss = 0.77054603\n",
            "Iteration 8494, loss = 0.77050488\n",
            "Iteration 8495, loss = 0.77046374\n",
            "Iteration 8496, loss = 0.77042261\n",
            "Iteration 8497, loss = 0.77038150\n",
            "Iteration 8498, loss = 0.77034040\n",
            "Iteration 8499, loss = 0.77029931\n",
            "Iteration 8500, loss = 0.77025824\n",
            "Iteration 8501, loss = 0.77021718\n",
            "Iteration 8502, loss = 0.77017613\n",
            "Iteration 8503, loss = 0.77013509\n",
            "Iteration 8504, loss = 0.77009407\n",
            "Iteration 8505, loss = 0.77005306\n",
            "Iteration 8506, loss = 0.77001206\n",
            "Iteration 8507, loss = 0.76997108\n",
            "Iteration 8508, loss = 0.76993011\n",
            "Iteration 8509, loss = 0.76988915\n",
            "Iteration 8510, loss = 0.76984821\n",
            "Iteration 8511, loss = 0.76980727\n",
            "Iteration 8512, loss = 0.76976635\n",
            "Iteration 8513, loss = 0.76972545\n",
            "Iteration 8514, loss = 0.76968455\n",
            "Iteration 8515, loss = 0.76964367\n",
            "Iteration 8516, loss = 0.76960280\n",
            "Iteration 8517, loss = 0.76956195\n",
            "Iteration 8518, loss = 0.76952111\n",
            "Iteration 8519, loss = 0.76948028\n",
            "Iteration 8520, loss = 0.76943946\n",
            "Iteration 8521, loss = 0.76939866\n",
            "Iteration 8522, loss = 0.76935787\n",
            "Iteration 8523, loss = 0.76931709\n",
            "Iteration 8524, loss = 0.76927633\n",
            "Iteration 8525, loss = 0.76923558\n",
            "Iteration 8526, loss = 0.76919484\n",
            "Iteration 8527, loss = 0.76915411\n",
            "Iteration 8528, loss = 0.76911340\n",
            "Iteration 8529, loss = 0.76907270\n",
            "Iteration 8530, loss = 0.76903201\n",
            "Iteration 8531, loss = 0.76899134\n",
            "Iteration 8532, loss = 0.76895068\n",
            "Iteration 8533, loss = 0.76891003\n",
            "Iteration 8534, loss = 0.76886939\n",
            "Iteration 8535, loss = 0.76882877\n",
            "Iteration 8536, loss = 0.76878816\n",
            "Iteration 8537, loss = 0.76874757\n",
            "Iteration 8538, loss = 0.76870698\n",
            "Iteration 8539, loss = 0.76866641\n",
            "Iteration 8540, loss = 0.76862586\n",
            "Iteration 8541, loss = 0.76858531\n",
            "Iteration 8542, loss = 0.76854478\n",
            "Iteration 8543, loss = 0.76850426\n",
            "Iteration 8544, loss = 0.76846376\n",
            "Iteration 8545, loss = 0.76842327\n",
            "Iteration 8546, loss = 0.76838279\n",
            "Iteration 8547, loss = 0.76834232\n",
            "Iteration 8548, loss = 0.76830187\n",
            "Iteration 8549, loss = 0.76826143\n",
            "Iteration 8550, loss = 0.76822100\n",
            "Iteration 8551, loss = 0.76818059\n",
            "Iteration 8552, loss = 0.76814018\n",
            "Iteration 8553, loss = 0.76809980\n",
            "Iteration 8554, loss = 0.76805942\n",
            "Iteration 8555, loss = 0.76801906\n",
            "Iteration 8556, loss = 0.76797871\n",
            "Iteration 8557, loss = 0.76793837\n",
            "Iteration 8558, loss = 0.76789805\n",
            "Iteration 8559, loss = 0.76785774\n",
            "Iteration 8560, loss = 0.76781744\n",
            "Iteration 8561, loss = 0.76777716\n",
            "Iteration 8562, loss = 0.76773689\n",
            "Iteration 8563, loss = 0.76769663\n",
            "Iteration 8564, loss = 0.76765638\n",
            "Iteration 8565, loss = 0.76761615\n",
            "Iteration 8566, loss = 0.76757593\n",
            "Iteration 8567, loss = 0.76753573\n",
            "Iteration 8568, loss = 0.76749553\n",
            "Iteration 8569, loss = 0.76745535\n",
            "Iteration 8570, loss = 0.76741519\n",
            "Iteration 8571, loss = 0.76737503\n",
            "Iteration 8572, loss = 0.76733489\n",
            "Iteration 8573, loss = 0.76729476\n",
            "Iteration 8574, loss = 0.76725465\n",
            "Iteration 8575, loss = 0.76721455\n",
            "Iteration 8576, loss = 0.76717446\n",
            "Iteration 8577, loss = 0.76713438\n",
            "Iteration 8578, loss = 0.76709432\n",
            "Iteration 8579, loss = 0.76705427\n",
            "Iteration 8580, loss = 0.76701423\n",
            "Iteration 8581, loss = 0.76697421\n",
            "Iteration 8582, loss = 0.76693420\n",
            "Iteration 8583, loss = 0.76689420\n",
            "Iteration 8584, loss = 0.76685422\n",
            "Iteration 8585, loss = 0.76681425\n",
            "Iteration 8586, loss = 0.76677429\n",
            "Iteration 8587, loss = 0.76673434\n",
            "Iteration 8588, loss = 0.76669441\n",
            "Iteration 8589, loss = 0.76665449\n",
            "Iteration 8590, loss = 0.76661458\n",
            "Iteration 8591, loss = 0.76657469\n",
            "Iteration 8592, loss = 0.76653481\n",
            "Iteration 8593, loss = 0.76649494\n",
            "Iteration 8594, loss = 0.76645509\n",
            "Iteration 8595, loss = 0.76641525\n",
            "Iteration 8596, loss = 0.76637542\n",
            "Iteration 8597, loss = 0.76633561\n",
            "Iteration 8598, loss = 0.76629581\n",
            "Iteration 8599, loss = 0.76625602\n",
            "Iteration 8600, loss = 0.76621624\n",
            "Iteration 8601, loss = 0.76617648\n",
            "Iteration 8602, loss = 0.76613673\n",
            "Iteration 8603, loss = 0.76609699\n",
            "Iteration 8604, loss = 0.76605727\n",
            "Iteration 8605, loss = 0.76601756\n",
            "Iteration 8606, loss = 0.76597786\n",
            "Iteration 8607, loss = 0.76593818\n",
            "Iteration 8608, loss = 0.76589851\n",
            "Iteration 8609, loss = 0.76585885\n",
            "Iteration 8610, loss = 0.76581921\n",
            "Iteration 8611, loss = 0.76577958\n",
            "Iteration 8612, loss = 0.76573996\n",
            "Iteration 8613, loss = 0.76570035\n",
            "Iteration 8614, loss = 0.76566076\n",
            "Iteration 8615, loss = 0.76562118\n",
            "Iteration 8616, loss = 0.76558161\n",
            "Iteration 8617, loss = 0.76554206\n",
            "Iteration 8618, loss = 0.76550252\n",
            "Iteration 8619, loss = 0.76546299\n",
            "Iteration 8620, loss = 0.76542348\n",
            "Iteration 8621, loss = 0.76538398\n",
            "Iteration 8622, loss = 0.76534449\n",
            "Iteration 8623, loss = 0.76530502\n",
            "Iteration 8624, loss = 0.76526556\n",
            "Iteration 8625, loss = 0.76522611\n",
            "Iteration 8626, loss = 0.76518667\n",
            "Iteration 8627, loss = 0.76514725\n",
            "Iteration 8628, loss = 0.76510784\n",
            "Iteration 8629, loss = 0.76506845\n",
            "Iteration 8630, loss = 0.76502906\n",
            "Iteration 8631, loss = 0.76498969\n",
            "Iteration 8632, loss = 0.76495034\n",
            "Iteration 8633, loss = 0.76491099\n",
            "Iteration 8634, loss = 0.76487166\n",
            "Iteration 8635, loss = 0.76483235\n",
            "Iteration 8636, loss = 0.76479304\n",
            "Iteration 8637, loss = 0.76475375\n",
            "Iteration 8638, loss = 0.76471447\n",
            "Iteration 8639, loss = 0.76467521\n",
            "Iteration 8640, loss = 0.76463596\n",
            "Iteration 8641, loss = 0.76459672\n",
            "Iteration 8642, loss = 0.76455749\n",
            "Iteration 8643, loss = 0.76451828\n",
            "Iteration 8644, loss = 0.76447908\n",
            "Iteration 8645, loss = 0.76443990\n",
            "Iteration 8646, loss = 0.76440072\n",
            "Iteration 8647, loss = 0.76436156\n",
            "Iteration 8648, loss = 0.76432242\n",
            "Iteration 8649, loss = 0.76428328\n",
            "Iteration 8650, loss = 0.76424416\n",
            "Iteration 8651, loss = 0.76420506\n",
            "Iteration 8652, loss = 0.76416596\n",
            "Iteration 8653, loss = 0.76412688\n",
            "Iteration 8654, loss = 0.76408781\n",
            "Iteration 8655, loss = 0.76404876\n",
            "Iteration 8656, loss = 0.76400972\n",
            "Iteration 8657, loss = 0.76397069\n",
            "Iteration 8658, loss = 0.76393167\n",
            "Iteration 8659, loss = 0.76389267\n",
            "Iteration 8660, loss = 0.76385368\n",
            "Iteration 8661, loss = 0.76381470\n",
            "Iteration 8662, loss = 0.76377574\n",
            "Iteration 8663, loss = 0.76373679\n",
            "Iteration 8664, loss = 0.76369786\n",
            "Iteration 8665, loss = 0.76365893\n",
            "Iteration 8666, loss = 0.76362002\n",
            "Iteration 8667, loss = 0.76358112\n",
            "Iteration 8668, loss = 0.76354224\n",
            "Iteration 8669, loss = 0.76350337\n",
            "Iteration 8670, loss = 0.76346451\n",
            "Iteration 8671, loss = 0.76342567\n",
            "Iteration 8672, loss = 0.76338683\n",
            "Iteration 8673, loss = 0.76334802\n",
            "Iteration 8674, loss = 0.76330921\n",
            "Iteration 8675, loss = 0.76327042\n",
            "Iteration 8676, loss = 0.76323164\n",
            "Iteration 8677, loss = 0.76319287\n",
            "Iteration 8678, loss = 0.76315412\n",
            "Iteration 8679, loss = 0.76311538\n",
            "Iteration 8680, loss = 0.76307665\n",
            "Iteration 8681, loss = 0.76303794\n",
            "Iteration 8682, loss = 0.76299924\n",
            "Iteration 8683, loss = 0.76296055\n",
            "Iteration 8684, loss = 0.76292188\n",
            "Iteration 8685, loss = 0.76288322\n",
            "Iteration 8686, loss = 0.76284457\n",
            "Iteration 8687, loss = 0.76280594\n",
            "Iteration 8688, loss = 0.76276731\n",
            "Iteration 8689, loss = 0.76272871\n",
            "Iteration 8690, loss = 0.76269011\n",
            "Iteration 8691, loss = 0.76265153\n",
            "Iteration 8692, loss = 0.76261296\n",
            "Iteration 8693, loss = 0.76257440\n",
            "Iteration 8694, loss = 0.76253586\n",
            "Iteration 8695, loss = 0.76249733\n",
            "Iteration 8696, loss = 0.76245881\n",
            "Iteration 8697, loss = 0.76242031\n",
            "Iteration 8698, loss = 0.76238182\n",
            "Iteration 8699, loss = 0.76234334\n",
            "Iteration 8700, loss = 0.76230488\n",
            "Iteration 8701, loss = 0.76226643\n",
            "Iteration 8702, loss = 0.76222799\n",
            "Iteration 8703, loss = 0.76218957\n",
            "Iteration 8704, loss = 0.76215115\n",
            "Iteration 8705, loss = 0.76211276\n",
            "Iteration 8706, loss = 0.76207437\n",
            "Iteration 8707, loss = 0.76203600\n",
            "Iteration 8708, loss = 0.76199764\n",
            "Iteration 8709, loss = 0.76195929\n",
            "Iteration 8710, loss = 0.76192096\n",
            "Iteration 8711, loss = 0.76188264\n",
            "Iteration 8712, loss = 0.76184434\n",
            "Iteration 8713, loss = 0.76180604\n",
            "Iteration 8714, loss = 0.76176776\n",
            "Iteration 8715, loss = 0.76172950\n",
            "Iteration 8716, loss = 0.76169124\n",
            "Iteration 8717, loss = 0.76165300\n",
            "Iteration 8718, loss = 0.76161477\n",
            "Iteration 8719, loss = 0.76157656\n",
            "Iteration 8720, loss = 0.76153836\n",
            "Iteration 8721, loss = 0.76150017\n",
            "Iteration 8722, loss = 0.76146200\n",
            "Iteration 8723, loss = 0.76142383\n",
            "Iteration 8724, loss = 0.76138568\n",
            "Iteration 8725, loss = 0.76134755\n",
            "Iteration 8726, loss = 0.76130943\n",
            "Iteration 8727, loss = 0.76127132\n",
            "Iteration 8728, loss = 0.76123322\n",
            "Iteration 8729, loss = 0.76119514\n",
            "Iteration 8730, loss = 0.76115707\n",
            "Iteration 8731, loss = 0.76111901\n",
            "Iteration 8732, loss = 0.76108097\n",
            "Iteration 8733, loss = 0.76104294\n",
            "Iteration 8734, loss = 0.76100492\n",
            "Iteration 8735, loss = 0.76096691\n",
            "Iteration 8736, loss = 0.76092892\n",
            "Iteration 8737, loss = 0.76089095\n",
            "Iteration 8738, loss = 0.76085298\n",
            "Iteration 8739, loss = 0.76081503\n",
            "Iteration 8740, loss = 0.76077709\n",
            "Iteration 8741, loss = 0.76073916\n",
            "Iteration 8742, loss = 0.76070125\n",
            "Iteration 8743, loss = 0.76066335\n",
            "Iteration 8744, loss = 0.76062547\n",
            "Iteration 8745, loss = 0.76058759\n",
            "Iteration 8746, loss = 0.76054973\n",
            "Iteration 8747, loss = 0.76051189\n",
            "Iteration 8748, loss = 0.76047405\n",
            "Iteration 8749, loss = 0.76043623\n",
            "Iteration 8750, loss = 0.76039842\n",
            "Iteration 8751, loss = 0.76036063\n",
            "Iteration 8752, loss = 0.76032285\n",
            "Iteration 8753, loss = 0.76028508\n",
            "Iteration 8754, loss = 0.76024732\n",
            "Iteration 8755, loss = 0.76020958\n",
            "Iteration 8756, loss = 0.76017185\n",
            "Iteration 8757, loss = 0.76013414\n",
            "Iteration 8758, loss = 0.76009644\n",
            "Iteration 8759, loss = 0.76005875\n",
            "Iteration 8760, loss = 0.76002107\n",
            "Iteration 8761, loss = 0.75998341\n",
            "Iteration 8762, loss = 0.75994576\n",
            "Iteration 8763, loss = 0.75990812\n",
            "Iteration 8764, loss = 0.75987049\n",
            "Iteration 8765, loss = 0.75983288\n",
            "Iteration 8766, loss = 0.75979529\n",
            "Iteration 8767, loss = 0.75975770\n",
            "Iteration 8768, loss = 0.75972013\n",
            "Iteration 8769, loss = 0.75968257\n",
            "Iteration 8770, loss = 0.75964503\n",
            "Iteration 8771, loss = 0.75960749\n",
            "Iteration 8772, loss = 0.75956997\n",
            "Iteration 8773, loss = 0.75953247\n",
            "Iteration 8774, loss = 0.75949498\n",
            "Iteration 8775, loss = 0.75945750\n",
            "Iteration 8776, loss = 0.75942003\n",
            "Iteration 8777, loss = 0.75938257\n",
            "Iteration 8778, loss = 0.75934513\n",
            "Iteration 8779, loss = 0.75930771\n",
            "Iteration 8780, loss = 0.75927029\n",
            "Iteration 8781, loss = 0.75923289\n",
            "Iteration 8782, loss = 0.75919550\n",
            "Iteration 8783, loss = 0.75915813\n",
            "Iteration 8784, loss = 0.75912077\n",
            "Iteration 8785, loss = 0.75908342\n",
            "Iteration 8786, loss = 0.75904608\n",
            "Iteration 8787, loss = 0.75900876\n",
            "Iteration 8788, loss = 0.75897145\n",
            "Iteration 8789, loss = 0.75893415\n",
            "Iteration 8790, loss = 0.75889687\n",
            "Iteration 8791, loss = 0.75885960\n",
            "Iteration 8792, loss = 0.75882234\n",
            "Iteration 8793, loss = 0.75878510\n",
            "Iteration 8794, loss = 0.75874786\n",
            "Iteration 8795, loss = 0.75871065\n",
            "Iteration 8796, loss = 0.75867344\n",
            "Iteration 8797, loss = 0.75863625\n",
            "Iteration 8798, loss = 0.75859907\n",
            "Iteration 8799, loss = 0.75856190\n",
            "Iteration 8800, loss = 0.75852475\n",
            "Iteration 8801, loss = 0.75848761\n",
            "Iteration 8802, loss = 0.75845049\n",
            "Iteration 8803, loss = 0.75841337\n",
            "Iteration 8804, loss = 0.75837627\n",
            "Iteration 8805, loss = 0.75833918\n",
            "Iteration 8806, loss = 0.75830211\n",
            "Iteration 8807, loss = 0.75826505\n",
            "Iteration 8808, loss = 0.75822800\n",
            "Iteration 8809, loss = 0.75819096\n",
            "Iteration 8810, loss = 0.75815394\n",
            "Iteration 8811, loss = 0.75811693\n",
            "Iteration 8812, loss = 0.75807994\n",
            "Iteration 8813, loss = 0.75804295\n",
            "Iteration 8814, loss = 0.75800598\n",
            "Iteration 8815, loss = 0.75796903\n",
            "Iteration 8816, loss = 0.75793208\n",
            "Iteration 8817, loss = 0.75789515\n",
            "Iteration 8818, loss = 0.75785823\n",
            "Iteration 8819, loss = 0.75782133\n",
            "Iteration 8820, loss = 0.75778444\n",
            "Iteration 8821, loss = 0.75774756\n",
            "Iteration 8822, loss = 0.75771069\n",
            "Iteration 8823, loss = 0.75767384\n",
            "Iteration 8824, loss = 0.75763700\n",
            "Iteration 8825, loss = 0.75760018\n",
            "Iteration 8826, loss = 0.75756336\n",
            "Iteration 8827, loss = 0.75752656\n",
            "Iteration 8828, loss = 0.75748977\n",
            "Iteration 8829, loss = 0.75745300\n",
            "Iteration 8830, loss = 0.75741624\n",
            "Iteration 8831, loss = 0.75737949\n",
            "Iteration 8832, loss = 0.75734276\n",
            "Iteration 8833, loss = 0.75730603\n",
            "Iteration 8834, loss = 0.75726932\n",
            "Iteration 8835, loss = 0.75723263\n",
            "Iteration 8836, loss = 0.75719594\n",
            "Iteration 8837, loss = 0.75715927\n",
            "Iteration 8838, loss = 0.75712262\n",
            "Iteration 8839, loss = 0.75708597\n",
            "Iteration 8840, loss = 0.75704934\n",
            "Iteration 8841, loss = 0.75701272\n",
            "Iteration 8842, loss = 0.75697612\n",
            "Iteration 8843, loss = 0.75693953\n",
            "Iteration 8844, loss = 0.75690295\n",
            "Iteration 8845, loss = 0.75686638\n",
            "Iteration 8846, loss = 0.75682983\n",
            "Iteration 8847, loss = 0.75679329\n",
            "Iteration 8848, loss = 0.75675676\n",
            "Iteration 8849, loss = 0.75672025\n",
            "Iteration 8850, loss = 0.75668375\n",
            "Iteration 8851, loss = 0.75664726\n",
            "Iteration 8852, loss = 0.75661078\n",
            "Iteration 8853, loss = 0.75657432\n",
            "Iteration 8854, loss = 0.75653787\n",
            "Iteration 8855, loss = 0.75650143\n",
            "Iteration 8856, loss = 0.75646501\n",
            "Iteration 8857, loss = 0.75642860\n",
            "Iteration 8858, loss = 0.75639220\n",
            "Iteration 8859, loss = 0.75635582\n",
            "Iteration 8860, loss = 0.75631945\n",
            "Iteration 8861, loss = 0.75628309\n",
            "Iteration 8862, loss = 0.75624674\n",
            "Iteration 8863, loss = 0.75621041\n",
            "Iteration 8864, loss = 0.75617409\n",
            "Iteration 8865, loss = 0.75613779\n",
            "Iteration 8866, loss = 0.75610149\n",
            "Iteration 8867, loss = 0.75606521\n",
            "Iteration 8868, loss = 0.75602894\n",
            "Iteration 8869, loss = 0.75599269\n",
            "Iteration 8870, loss = 0.75595645\n",
            "Iteration 8871, loss = 0.75592022\n",
            "Iteration 8872, loss = 0.75588400\n",
            "Iteration 8873, loss = 0.75584780\n",
            "Iteration 8874, loss = 0.75581161\n",
            "Iteration 8875, loss = 0.75577543\n",
            "Iteration 8876, loss = 0.75573927\n",
            "Iteration 8877, loss = 0.75570312\n",
            "Iteration 8878, loss = 0.75566698\n",
            "Iteration 8879, loss = 0.75563086\n",
            "Iteration 8880, loss = 0.75559474\n",
            "Iteration 8881, loss = 0.75555864\n",
            "Iteration 8882, loss = 0.75552256\n",
            "Iteration 8883, loss = 0.75548649\n",
            "Iteration 8884, loss = 0.75545042\n",
            "Iteration 8885, loss = 0.75541438\n",
            "Iteration 8886, loss = 0.75537834\n",
            "Iteration 8887, loss = 0.75534232\n",
            "Iteration 8888, loss = 0.75530631\n",
            "Iteration 8889, loss = 0.75527032\n",
            "Iteration 8890, loss = 0.75523433\n",
            "Iteration 8891, loss = 0.75519836\n",
            "Iteration 8892, loss = 0.75516241\n",
            "Iteration 8893, loss = 0.75512646\n",
            "Iteration 8894, loss = 0.75509053\n",
            "Iteration 8895, loss = 0.75505461\n",
            "Iteration 8896, loss = 0.75501871\n",
            "Iteration 8897, loss = 0.75498281\n",
            "Iteration 8898, loss = 0.75494693\n",
            "Iteration 8899, loss = 0.75491107\n",
            "Iteration 8900, loss = 0.75487521\n",
            "Iteration 8901, loss = 0.75483937\n",
            "Iteration 8902, loss = 0.75480354\n",
            "Iteration 8903, loss = 0.75476773\n",
            "Iteration 8904, loss = 0.75473193\n",
            "Iteration 8905, loss = 0.75469614\n",
            "Iteration 8906, loss = 0.75466036\n",
            "Iteration 8907, loss = 0.75462460\n",
            "Iteration 8908, loss = 0.75458885\n",
            "Iteration 8909, loss = 0.75455311\n",
            "Iteration 8910, loss = 0.75451738\n",
            "Iteration 8911, loss = 0.75448167\n",
            "Iteration 8912, loss = 0.75444597\n",
            "Iteration 8913, loss = 0.75441028\n",
            "Iteration 8914, loss = 0.75437461\n",
            "Iteration 8915, loss = 0.75433895\n",
            "Iteration 8916, loss = 0.75430330\n",
            "Iteration 8917, loss = 0.75426767\n",
            "Iteration 8918, loss = 0.75423204\n",
            "Iteration 8919, loss = 0.75419643\n",
            "Iteration 8920, loss = 0.75416084\n",
            "Iteration 8921, loss = 0.75412525\n",
            "Iteration 8922, loss = 0.75408968\n",
            "Iteration 8923, loss = 0.75405412\n",
            "Iteration 8924, loss = 0.75401858\n",
            "Iteration 8925, loss = 0.75398305\n",
            "Iteration 8926, loss = 0.75394753\n",
            "Iteration 8927, loss = 0.75391202\n",
            "Iteration 8928, loss = 0.75387653\n",
            "Iteration 8929, loss = 0.75384105\n",
            "Iteration 8930, loss = 0.75380558\n",
            "Iteration 8931, loss = 0.75377012\n",
            "Iteration 8932, loss = 0.75373468\n",
            "Iteration 8933, loss = 0.75369925\n",
            "Iteration 8934, loss = 0.75366383\n",
            "Iteration 8935, loss = 0.75362843\n",
            "Iteration 8936, loss = 0.75359304\n",
            "Iteration 8937, loss = 0.75355766\n",
            "Iteration 8938, loss = 0.75352229\n",
            "Iteration 8939, loss = 0.75348694\n",
            "Iteration 8940, loss = 0.75345160\n",
            "Iteration 8941, loss = 0.75341627\n",
            "Iteration 8942, loss = 0.75338096\n",
            "Iteration 8943, loss = 0.75334565\n",
            "Iteration 8944, loss = 0.75331036\n",
            "Iteration 8945, loss = 0.75327509\n",
            "Iteration 8946, loss = 0.75323982\n",
            "Iteration 8947, loss = 0.75320457\n",
            "Iteration 8948, loss = 0.75316934\n",
            "Iteration 8949, loss = 0.75313411\n",
            "Iteration 8950, loss = 0.75309890\n",
            "Iteration 8951, loss = 0.75306370\n",
            "Iteration 8952, loss = 0.75302851\n",
            "Iteration 8953, loss = 0.75299334\n",
            "Iteration 8954, loss = 0.75295818\n",
            "Iteration 8955, loss = 0.75292303\n",
            "Iteration 8956, loss = 0.75288789\n",
            "Iteration 8957, loss = 0.75285277\n",
            "Iteration 8958, loss = 0.75281766\n",
            "Iteration 8959, loss = 0.75278256\n",
            "Iteration 8960, loss = 0.75274747\n",
            "Iteration 8961, loss = 0.75271240\n",
            "Iteration 8962, loss = 0.75267734\n",
            "Iteration 8963, loss = 0.75264230\n",
            "Iteration 8964, loss = 0.75260726\n",
            "Iteration 8965, loss = 0.75257224\n",
            "Iteration 8966, loss = 0.75253723\n",
            "Iteration 8967, loss = 0.75250223\n",
            "Iteration 8968, loss = 0.75246725\n",
            "Iteration 8969, loss = 0.75243228\n",
            "Iteration 8970, loss = 0.75239732\n",
            "Iteration 8971, loss = 0.75236238\n",
            "Iteration 8972, loss = 0.75232745\n",
            "Iteration 8973, loss = 0.75229253\n",
            "Iteration 8974, loss = 0.75225762\n",
            "Iteration 8975, loss = 0.75222272\n",
            "Iteration 8976, loss = 0.75218784\n",
            "Iteration 8977, loss = 0.75215297\n",
            "Iteration 8978, loss = 0.75211812\n",
            "Iteration 8979, loss = 0.75208327\n",
            "Iteration 8980, loss = 0.75204844\n",
            "Iteration 8981, loss = 0.75201362\n",
            "Iteration 8982, loss = 0.75197882\n",
            "Iteration 8983, loss = 0.75194403\n",
            "Iteration 8984, loss = 0.75190924\n",
            "Iteration 8985, loss = 0.75187448\n",
            "Iteration 8986, loss = 0.75183972\n",
            "Iteration 8987, loss = 0.75180498\n",
            "Iteration 8988, loss = 0.75177025\n",
            "Iteration 8989, loss = 0.75173553\n",
            "Iteration 8990, loss = 0.75170083\n",
            "Iteration 8991, loss = 0.75166614\n",
            "Iteration 8992, loss = 0.75163146\n",
            "Iteration 8993, loss = 0.75159679\n",
            "Iteration 8994, loss = 0.75156214\n",
            "Iteration 8995, loss = 0.75152750\n",
            "Iteration 8996, loss = 0.75149287\n",
            "Iteration 8997, loss = 0.75145825\n",
            "Iteration 8998, loss = 0.75142365\n",
            "Iteration 8999, loss = 0.75138906\n",
            "Iteration 9000, loss = 0.75135448\n",
            "Iteration 9001, loss = 0.75131991\n",
            "Iteration 9002, loss = 0.75128536\n",
            "Iteration 9003, loss = 0.75125082\n",
            "Iteration 9004, loss = 0.75121629\n",
            "Iteration 9005, loss = 0.75118178\n",
            "Iteration 9006, loss = 0.75114727\n",
            "Iteration 9007, loss = 0.75111278\n",
            "Iteration 9008, loss = 0.75107831\n",
            "Iteration 9009, loss = 0.75104384\n",
            "Iteration 9010, loss = 0.75100939\n",
            "Iteration 9011, loss = 0.75097495\n",
            "Iteration 9012, loss = 0.75094052\n",
            "Iteration 9013, loss = 0.75090611\n",
            "Iteration 9014, loss = 0.75087171\n",
            "Iteration 9015, loss = 0.75083732\n",
            "Iteration 9016, loss = 0.75080294\n",
            "Iteration 9017, loss = 0.75076858\n",
            "Iteration 9018, loss = 0.75073422\n",
            "Iteration 9019, loss = 0.75069989\n",
            "Iteration 9020, loss = 0.75066556\n",
            "Iteration 9021, loss = 0.75063124\n",
            "Iteration 9022, loss = 0.75059694\n",
            "Iteration 9023, loss = 0.75056265\n",
            "Iteration 9024, loss = 0.75052838\n",
            "Iteration 9025, loss = 0.75049411\n",
            "Iteration 9026, loss = 0.75045986\n",
            "Iteration 9027, loss = 0.75042562\n",
            "Iteration 9028, loss = 0.75039140\n",
            "Iteration 9029, loss = 0.75035718\n",
            "Iteration 9030, loss = 0.75032298\n",
            "Iteration 9031, loss = 0.75028879\n",
            "Iteration 9032, loss = 0.75025462\n",
            "Iteration 9033, loss = 0.75022045\n",
            "Iteration 9034, loss = 0.75018630\n",
            "Iteration 9035, loss = 0.75015216\n",
            "Iteration 9036, loss = 0.75011804\n",
            "Iteration 9037, loss = 0.75008392\n",
            "Iteration 9038, loss = 0.75004982\n",
            "Iteration 9039, loss = 0.75001573\n",
            "Iteration 9040, loss = 0.74998166\n",
            "Iteration 9041, loss = 0.74994759\n",
            "Iteration 9042, loss = 0.74991354\n",
            "Iteration 9043, loss = 0.74987950\n",
            "Iteration 9044, loss = 0.74984547\n",
            "Iteration 9045, loss = 0.74981146\n",
            "Iteration 9046, loss = 0.74977746\n",
            "Iteration 9047, loss = 0.74974347\n",
            "Iteration 9048, loss = 0.74970949\n",
            "Iteration 9049, loss = 0.74967553\n",
            "Iteration 9050, loss = 0.74964158\n",
            "Iteration 9051, loss = 0.74960764\n",
            "Iteration 9052, loss = 0.74957371\n",
            "Iteration 9053, loss = 0.74953980\n",
            "Iteration 9054, loss = 0.74950589\n",
            "Iteration 9055, loss = 0.74947200\n",
            "Iteration 9056, loss = 0.74943813\n",
            "Iteration 9057, loss = 0.74940426\n",
            "Iteration 9058, loss = 0.74937041\n",
            "Iteration 9059, loss = 0.74933657\n",
            "Iteration 9060, loss = 0.74930274\n",
            "Iteration 9061, loss = 0.74926893\n",
            "Iteration 9062, loss = 0.74923512\n",
            "Iteration 9063, loss = 0.74920133\n",
            "Iteration 9064, loss = 0.74916755\n",
            "Iteration 9065, loss = 0.74913379\n",
            "Iteration 9066, loss = 0.74910004\n",
            "Iteration 9067, loss = 0.74906629\n",
            "Iteration 9068, loss = 0.74903257\n",
            "Iteration 9069, loss = 0.74899885\n",
            "Iteration 9070, loss = 0.74896515\n",
            "Iteration 9071, loss = 0.74893145\n",
            "Iteration 9072, loss = 0.74889778\n",
            "Iteration 9073, loss = 0.74886411\n",
            "Iteration 9074, loss = 0.74883045\n",
            "Iteration 9075, loss = 0.74879681\n",
            "Iteration 9076, loss = 0.74876318\n",
            "Iteration 9077, loss = 0.74872956\n",
            "Iteration 9078, loss = 0.74869596\n",
            "Iteration 9079, loss = 0.74866237\n",
            "Iteration 9080, loss = 0.74862879\n",
            "Iteration 9081, loss = 0.74859522\n",
            "Iteration 9082, loss = 0.74856166\n",
            "Iteration 9083, loss = 0.74852812\n",
            "Iteration 9084, loss = 0.74849459\n",
            "Iteration 9085, loss = 0.74846107\n",
            "Iteration 9086, loss = 0.74842756\n",
            "Iteration 9087, loss = 0.74839407\n",
            "Iteration 9088, loss = 0.74836059\n",
            "Iteration 9089, loss = 0.74832712\n",
            "Iteration 9090, loss = 0.74829366\n",
            "Iteration 9091, loss = 0.74826021\n",
            "Iteration 9092, loss = 0.74822678\n",
            "Iteration 9093, loss = 0.74819336\n",
            "Iteration 9094, loss = 0.74815995\n",
            "Iteration 9095, loss = 0.74812656\n",
            "Iteration 9096, loss = 0.74809317\n",
            "Iteration 9097, loss = 0.74805980\n",
            "Iteration 9098, loss = 0.74802644\n",
            "Iteration 9099, loss = 0.74799309\n",
            "Iteration 9100, loss = 0.74795976\n",
            "Iteration 9101, loss = 0.74792644\n",
            "Iteration 9102, loss = 0.74789313\n",
            "Iteration 9103, loss = 0.74785983\n",
            "Iteration 9104, loss = 0.74782654\n",
            "Iteration 9105, loss = 0.74779327\n",
            "Iteration 9106, loss = 0.74776001\n",
            "Iteration 9107, loss = 0.74772676\n",
            "Iteration 9108, loss = 0.74769352\n",
            "Iteration 9109, loss = 0.74766030\n",
            "Iteration 9110, loss = 0.74762709\n",
            "Iteration 9111, loss = 0.74759389\n",
            "Iteration 9112, loss = 0.74756070\n",
            "Iteration 9113, loss = 0.74752752\n",
            "Iteration 9114, loss = 0.74749436\n",
            "Iteration 9115, loss = 0.74746121\n",
            "Iteration 9116, loss = 0.74742807\n",
            "Iteration 9117, loss = 0.74739494\n",
            "Iteration 9118, loss = 0.74736183\n",
            "Iteration 9119, loss = 0.74732872\n",
            "Iteration 9120, loss = 0.74729563\n",
            "Iteration 9121, loss = 0.74726255\n",
            "Iteration 9122, loss = 0.74722949\n",
            "Iteration 9123, loss = 0.74719643\n",
            "Iteration 9124, loss = 0.74716339\n",
            "Iteration 9125, loss = 0.74713036\n",
            "Iteration 9126, loss = 0.74709734\n",
            "Iteration 9127, loss = 0.74706434\n",
            "Iteration 9128, loss = 0.74703134\n",
            "Iteration 9129, loss = 0.74699836\n",
            "Iteration 9130, loss = 0.74696539\n",
            "Iteration 9131, loss = 0.74693244\n",
            "Iteration 9132, loss = 0.74689949\n",
            "Iteration 9133, loss = 0.74686656\n",
            "Iteration 9134, loss = 0.74683364\n",
            "Iteration 9135, loss = 0.74680073\n",
            "Iteration 9136, loss = 0.74676783\n",
            "Iteration 9137, loss = 0.74673495\n",
            "Iteration 9138, loss = 0.74670208\n",
            "Iteration 9139, loss = 0.74666922\n",
            "Iteration 9140, loss = 0.74663637\n",
            "Iteration 9141, loss = 0.74660353\n",
            "Iteration 9142, loss = 0.74657071\n",
            "Iteration 9143, loss = 0.74653790\n",
            "Iteration 9144, loss = 0.74650510\n",
            "Iteration 9145, loss = 0.74647231\n",
            "Iteration 9146, loss = 0.74643953\n",
            "Iteration 9147, loss = 0.74640677\n",
            "Iteration 9148, loss = 0.74637402\n",
            "Iteration 9149, loss = 0.74634128\n",
            "Iteration 9150, loss = 0.74630855\n",
            "Iteration 9151, loss = 0.74627583\n",
            "Iteration 9152, loss = 0.74624313\n",
            "Iteration 9153, loss = 0.74621044\n",
            "Iteration 9154, loss = 0.74617776\n",
            "Iteration 9155, loss = 0.74614509\n",
            "Iteration 9156, loss = 0.74611244\n",
            "Iteration 9157, loss = 0.74607979\n",
            "Iteration 9158, loss = 0.74604716\n",
            "Iteration 9159, loss = 0.74601454\n",
            "Iteration 9160, loss = 0.74598194\n",
            "Iteration 9161, loss = 0.74594934\n",
            "Iteration 9162, loss = 0.74591676\n",
            "Iteration 9163, loss = 0.74588419\n",
            "Iteration 9164, loss = 0.74585163\n",
            "Iteration 9165, loss = 0.74581908\n",
            "Iteration 9166, loss = 0.74578654\n",
            "Iteration 9167, loss = 0.74575402\n",
            "Iteration 9168, loss = 0.74572151\n",
            "Iteration 9169, loss = 0.74568901\n",
            "Iteration 9170, loss = 0.74565652\n",
            "Iteration 9171, loss = 0.74562404\n",
            "Iteration 9172, loss = 0.74559158\n",
            "Iteration 9173, loss = 0.74555913\n",
            "Iteration 9174, loss = 0.74552669\n",
            "Iteration 9175, loss = 0.74549426\n",
            "Iteration 9176, loss = 0.74546185\n",
            "Iteration 9177, loss = 0.74542944\n",
            "Iteration 9178, loss = 0.74539705\n",
            "Iteration 9179, loss = 0.74536467\n",
            "Iteration 9180, loss = 0.74533230\n",
            "Iteration 9181, loss = 0.74529994\n",
            "Iteration 9182, loss = 0.74526760\n",
            "Iteration 9183, loss = 0.74523527\n",
            "Iteration 9184, loss = 0.74520295\n",
            "Iteration 9185, loss = 0.74517064\n",
            "Iteration 9186, loss = 0.74513834\n",
            "Iteration 9187, loss = 0.74510605\n",
            "Iteration 9188, loss = 0.74507378\n",
            "Iteration 9189, loss = 0.74504152\n",
            "Iteration 9190, loss = 0.74500927\n",
            "Iteration 9191, loss = 0.74497703\n",
            "Iteration 9192, loss = 0.74494481\n",
            "Iteration 9193, loss = 0.74491259\n",
            "Iteration 9194, loss = 0.74488039\n",
            "Iteration 9195, loss = 0.74484820\n",
            "Iteration 9196, loss = 0.74481602\n",
            "Iteration 9197, loss = 0.74478385\n",
            "Iteration 9198, loss = 0.74475170\n",
            "Iteration 9199, loss = 0.74471956\n",
            "Iteration 9200, loss = 0.74468743\n",
            "Iteration 9201, loss = 0.74465531\n",
            "Iteration 9202, loss = 0.74462320\n",
            "Iteration 9203, loss = 0.74459110\n",
            "Iteration 9204, loss = 0.74455902\n",
            "Iteration 9205, loss = 0.74452695\n",
            "Iteration 9206, loss = 0.74449489\n",
            "Iteration 9207, loss = 0.74446284\n",
            "Iteration 9208, loss = 0.74443080\n",
            "Iteration 9209, loss = 0.74439878\n",
            "Iteration 9210, loss = 0.74436676\n",
            "Iteration 9211, loss = 0.74433476\n",
            "Iteration 9212, loss = 0.74430277\n",
            "Iteration 9213, loss = 0.74427079\n",
            "Iteration 9214, loss = 0.74423883\n",
            "Iteration 9215, loss = 0.74420687\n",
            "Iteration 9216, loss = 0.74417493\n",
            "Iteration 9217, loss = 0.74414300\n",
            "Iteration 9218, loss = 0.74411108\n",
            "Iteration 9219, loss = 0.74407917\n",
            "Iteration 9220, loss = 0.74404728\n",
            "Iteration 9221, loss = 0.74401539\n",
            "Iteration 9222, loss = 0.74398352\n",
            "Iteration 9223, loss = 0.74395166\n",
            "Iteration 9224, loss = 0.74391981\n",
            "Iteration 9225, loss = 0.74388797\n",
            "Iteration 9226, loss = 0.74385615\n",
            "Iteration 9227, loss = 0.74382433\n",
            "Iteration 9228, loss = 0.74379253\n",
            "Iteration 9229, loss = 0.74376074\n",
            "Iteration 9230, loss = 0.74372896\n",
            "Iteration 9231, loss = 0.74369719\n",
            "Iteration 9232, loss = 0.74366544\n",
            "Iteration 9233, loss = 0.74363370\n",
            "Iteration 9234, loss = 0.74360196\n",
            "Iteration 9235, loss = 0.74357024\n",
            "Iteration 9236, loss = 0.74353853\n",
            "Iteration 9237, loss = 0.74350684\n",
            "Iteration 9238, loss = 0.74347515\n",
            "Iteration 9239, loss = 0.74344348\n",
            "Iteration 9240, loss = 0.74341181\n",
            "Iteration 9241, loss = 0.74338016\n",
            "Iteration 9242, loss = 0.74334852\n",
            "Iteration 9243, loss = 0.74331690\n",
            "Iteration 9244, loss = 0.74328528\n",
            "Iteration 9245, loss = 0.74325368\n",
            "Iteration 9246, loss = 0.74322208\n",
            "Iteration 9247, loss = 0.74319050\n",
            "Iteration 9248, loss = 0.74315893\n",
            "Iteration 9249, loss = 0.74312738\n",
            "Iteration 9250, loss = 0.74309583\n",
            "Iteration 9251, loss = 0.74306429\n",
            "Iteration 9252, loss = 0.74303277\n",
            "Iteration 9253, loss = 0.74300126\n",
            "Iteration 9254, loss = 0.74296976\n",
            "Iteration 9255, loss = 0.74293827\n",
            "Iteration 9256, loss = 0.74290679\n",
            "Iteration 9257, loss = 0.74287533\n",
            "Iteration 9258, loss = 0.74284388\n",
            "Iteration 9259, loss = 0.74281243\n",
            "Iteration 9260, loss = 0.74278100\n",
            "Iteration 9261, loss = 0.74274958\n",
            "Iteration 9262, loss = 0.74271818\n",
            "Iteration 9263, loss = 0.74268678\n",
            "Iteration 9264, loss = 0.74265539\n",
            "Iteration 9265, loss = 0.74262402\n",
            "Iteration 9266, loss = 0.74259266\n",
            "Iteration 9267, loss = 0.74256131\n",
            "Iteration 9268, loss = 0.74252997\n",
            "Iteration 9269, loss = 0.74249864\n",
            "Iteration 9270, loss = 0.74246733\n",
            "Iteration 9271, loss = 0.74243602\n",
            "Iteration 9272, loss = 0.74240473\n",
            "Iteration 9273, loss = 0.74237345\n",
            "Iteration 9274, loss = 0.74234218\n",
            "Iteration 9275, loss = 0.74231092\n",
            "Iteration 9276, loss = 0.74227968\n",
            "Iteration 9277, loss = 0.74224844\n",
            "Iteration 9278, loss = 0.74221722\n",
            "Iteration 9279, loss = 0.74218600\n",
            "Iteration 9280, loss = 0.74215480\n",
            "Iteration 9281, loss = 0.74212361\n",
            "Iteration 9282, loss = 0.74209243\n",
            "Iteration 9283, loss = 0.74206127\n",
            "Iteration 9284, loss = 0.74203011\n",
            "Iteration 9285, loss = 0.74199897\n",
            "Iteration 9286, loss = 0.74196784\n",
            "Iteration 9287, loss = 0.74193671\n",
            "Iteration 9288, loss = 0.74190560\n",
            "Iteration 9289, loss = 0.74187451\n",
            "Iteration 9290, loss = 0.74184342\n",
            "Iteration 9291, loss = 0.74181234\n",
            "Iteration 9292, loss = 0.74178128\n",
            "Iteration 9293, loss = 0.74175023\n",
            "Iteration 9294, loss = 0.74171918\n",
            "Iteration 9295, loss = 0.74168815\n",
            "Iteration 9296, loss = 0.74165714\n",
            "Iteration 9297, loss = 0.74162613\n",
            "Iteration 9298, loss = 0.74159513\n",
            "Iteration 9299, loss = 0.74156415\n",
            "Iteration 9300, loss = 0.74153317\n",
            "Iteration 9301, loss = 0.74150221\n",
            "Iteration 9302, loss = 0.74147126\n",
            "Iteration 9303, loss = 0.74144032\n",
            "Iteration 9304, loss = 0.74140939\n",
            "Iteration 9305, loss = 0.74137848\n",
            "Iteration 9306, loss = 0.74134757\n",
            "Iteration 9307, loss = 0.74131668\n",
            "Iteration 9308, loss = 0.74128579\n",
            "Iteration 9309, loss = 0.74125492\n",
            "Iteration 9310, loss = 0.74122406\n",
            "Iteration 9311, loss = 0.74119321\n",
            "Iteration 9312, loss = 0.74116238\n",
            "Iteration 9313, loss = 0.74113155\n",
            "Iteration 9314, loss = 0.74110073\n",
            "Iteration 9315, loss = 0.74106993\n",
            "Iteration 9316, loss = 0.74103914\n",
            "Iteration 9317, loss = 0.74100836\n",
            "Iteration 9318, loss = 0.74097759\n",
            "Iteration 9319, loss = 0.74094683\n",
            "Iteration 9320, loss = 0.74091608\n",
            "Iteration 9321, loss = 0.74088534\n",
            "Iteration 9322, loss = 0.74085462\n",
            "Iteration 9323, loss = 0.74082390\n",
            "Iteration 9324, loss = 0.74079320\n",
            "Iteration 9325, loss = 0.74076251\n",
            "Iteration 9326, loss = 0.74073183\n",
            "Iteration 9327, loss = 0.74070116\n",
            "Iteration 9328, loss = 0.74067050\n",
            "Iteration 9329, loss = 0.74063985\n",
            "Iteration 9330, loss = 0.74060922\n",
            "Iteration 9331, loss = 0.74057859\n",
            "Iteration 9332, loss = 0.74054798\n",
            "Iteration 9333, loss = 0.74051738\n",
            "Iteration 9334, loss = 0.74048679\n",
            "Iteration 9335, loss = 0.74045621\n",
            "Iteration 9336, loss = 0.74042564\n",
            "Iteration 9337, loss = 0.74039508\n",
            "Iteration 9338, loss = 0.74036454\n",
            "Iteration 9339, loss = 0.74033400\n",
            "Iteration 9340, loss = 0.74030348\n",
            "Iteration 9341, loss = 0.74027296\n",
            "Iteration 9342, loss = 0.74024246\n",
            "Iteration 9343, loss = 0.74021197\n",
            "Iteration 9344, loss = 0.74018149\n",
            "Iteration 9345, loss = 0.74015102\n",
            "Iteration 9346, loss = 0.74012056\n",
            "Iteration 9347, loss = 0.74009012\n",
            "Iteration 9348, loss = 0.74005968\n",
            "Iteration 9349, loss = 0.74002926\n",
            "Iteration 9350, loss = 0.73999885\n",
            "Iteration 9351, loss = 0.73996844\n",
            "Iteration 9352, loss = 0.73993805\n",
            "Iteration 9353, loss = 0.73990767\n",
            "Iteration 9354, loss = 0.73987731\n",
            "Iteration 9355, loss = 0.73984695\n",
            "Iteration 9356, loss = 0.73981660\n",
            "Iteration 9357, loss = 0.73978627\n",
            "Iteration 9358, loss = 0.73975594\n",
            "Iteration 9359, loss = 0.73972563\n",
            "Iteration 9360, loss = 0.73969533\n",
            "Iteration 9361, loss = 0.73966503\n",
            "Iteration 9362, loss = 0.73963475\n",
            "Iteration 9363, loss = 0.73960448\n",
            "Iteration 9364, loss = 0.73957423\n",
            "Iteration 9365, loss = 0.73954398\n",
            "Iteration 9366, loss = 0.73951374\n",
            "Iteration 9367, loss = 0.73948352\n",
            "Iteration 9368, loss = 0.73945330\n",
            "Iteration 9369, loss = 0.73942310\n",
            "Iteration 9370, loss = 0.73939291\n",
            "Iteration 9371, loss = 0.73936272\n",
            "Iteration 9372, loss = 0.73933255\n",
            "Iteration 9373, loss = 0.73930240\n",
            "Iteration 9374, loss = 0.73927225\n",
            "Iteration 9375, loss = 0.73924211\n",
            "Iteration 9376, loss = 0.73921198\n",
            "Iteration 9377, loss = 0.73918187\n",
            "Iteration 9378, loss = 0.73915176\n",
            "Iteration 9379, loss = 0.73912167\n",
            "Iteration 9380, loss = 0.73909159\n",
            "Iteration 9381, loss = 0.73906151\n",
            "Iteration 9382, loss = 0.73903145\n",
            "Iteration 9383, loss = 0.73900140\n",
            "Iteration 9384, loss = 0.73897136\n",
            "Iteration 9385, loss = 0.73894133\n",
            "Iteration 9386, loss = 0.73891132\n",
            "Iteration 9387, loss = 0.73888131\n",
            "Iteration 9388, loss = 0.73885132\n",
            "Iteration 9389, loss = 0.73882133\n",
            "Iteration 9390, loss = 0.73879136\n",
            "Iteration 9391, loss = 0.73876139\n",
            "Iteration 9392, loss = 0.73873144\n",
            "Iteration 9393, loss = 0.73870150\n",
            "Iteration 9394, loss = 0.73867157\n",
            "Iteration 9395, loss = 0.73864165\n",
            "Iteration 9396, loss = 0.73861174\n",
            "Iteration 9397, loss = 0.73858184\n",
            "Iteration 9398, loss = 0.73855196\n",
            "Iteration 9399, loss = 0.73852208\n",
            "Iteration 9400, loss = 0.73849222\n",
            "Iteration 9401, loss = 0.73846236\n",
            "Iteration 9402, loss = 0.73843252\n",
            "Iteration 9403, loss = 0.73840268\n",
            "Iteration 9404, loss = 0.73837286\n",
            "Iteration 9405, loss = 0.73834305\n",
            "Iteration 9406, loss = 0.73831325\n",
            "Iteration 9407, loss = 0.73828346\n",
            "Iteration 9408, loss = 0.73825368\n",
            "Iteration 9409, loss = 0.73822391\n",
            "Iteration 9410, loss = 0.73819416\n",
            "Iteration 9411, loss = 0.73816441\n",
            "Iteration 9412, loss = 0.73813467\n",
            "Iteration 9413, loss = 0.73810495\n",
            "Iteration 9414, loss = 0.73807523\n",
            "Iteration 9415, loss = 0.73804553\n",
            "Iteration 9416, loss = 0.73801584\n",
            "Iteration 9417, loss = 0.73798616\n",
            "Iteration 9418, loss = 0.73795648\n",
            "Iteration 9419, loss = 0.73792682\n",
            "Iteration 9420, loss = 0.73789717\n",
            "Iteration 9421, loss = 0.73786753\n",
            "Iteration 9422, loss = 0.73783791\n",
            "Iteration 9423, loss = 0.73780829\n",
            "Iteration 9424, loss = 0.73777868\n",
            "Iteration 9425, loss = 0.73774908\n",
            "Iteration 9426, loss = 0.73771950\n",
            "Iteration 9427, loss = 0.73768992\n",
            "Iteration 9428, loss = 0.73766036\n",
            "Iteration 9429, loss = 0.73763081\n",
            "Iteration 9430, loss = 0.73760126\n",
            "Iteration 9431, loss = 0.73757173\n",
            "Iteration 9432, loss = 0.73754221\n",
            "Iteration 9433, loss = 0.73751270\n",
            "Iteration 9434, loss = 0.73748320\n",
            "Iteration 9435, loss = 0.73745371\n",
            "Iteration 9436, loss = 0.73742423\n",
            "Iteration 9437, loss = 0.73739476\n",
            "Iteration 9438, loss = 0.73736530\n",
            "Iteration 9439, loss = 0.73733586\n",
            "Iteration 9440, loss = 0.73730642\n",
            "Iteration 9441, loss = 0.73727699\n",
            "Iteration 9442, loss = 0.73724758\n",
            "Iteration 9443, loss = 0.73721817\n",
            "Iteration 9444, loss = 0.73718878\n",
            "Iteration 9445, loss = 0.73715939\n",
            "Iteration 9446, loss = 0.73713002\n",
            "Iteration 9447, loss = 0.73710066\n",
            "Iteration 9448, loss = 0.73707131\n",
            "Iteration 9449, loss = 0.73704197\n",
            "Iteration 9450, loss = 0.73701264\n",
            "Iteration 9451, loss = 0.73698332\n",
            "Iteration 9452, loss = 0.73695401\n",
            "Iteration 9453, loss = 0.73692471\n",
            "Iteration 9454, loss = 0.73689542\n",
            "Iteration 9455, loss = 0.73686614\n",
            "Iteration 9456, loss = 0.73683687\n",
            "Iteration 9457, loss = 0.73680762\n",
            "Iteration 9458, loss = 0.73677837\n",
            "Iteration 9459, loss = 0.73674913\n",
            "Iteration 9460, loss = 0.73671991\n",
            "Iteration 9461, loss = 0.73669069\n",
            "Iteration 9462, loss = 0.73666149\n",
            "Iteration 9463, loss = 0.73663230\n",
            "Iteration 9464, loss = 0.73660311\n",
            "Iteration 9465, loss = 0.73657394\n",
            "Iteration 9466, loss = 0.73654478\n",
            "Iteration 9467, loss = 0.73651562\n",
            "Iteration 9468, loss = 0.73648648\n",
            "Iteration 9469, loss = 0.73645735\n",
            "Iteration 9470, loss = 0.73642823\n",
            "Iteration 9471, loss = 0.73639912\n",
            "Iteration 9472, loss = 0.73637002\n",
            "Iteration 9473, loss = 0.73634093\n",
            "Iteration 9474, loss = 0.73631185\n",
            "Iteration 9475, loss = 0.73628279\n",
            "Iteration 9476, loss = 0.73625373\n",
            "Iteration 9477, loss = 0.73622468\n",
            "Iteration 9478, loss = 0.73619564\n",
            "Iteration 9479, loss = 0.73616662\n",
            "Iteration 9480, loss = 0.73613760\n",
            "Iteration 9481, loss = 0.73610859\n",
            "Iteration 9482, loss = 0.73607960\n",
            "Iteration 9483, loss = 0.73605061\n",
            "Iteration 9484, loss = 0.73602164\n",
            "Iteration 9485, loss = 0.73599268\n",
            "Iteration 9486, loss = 0.73596372\n",
            "Iteration 9487, loss = 0.73593478\n",
            "Iteration 9488, loss = 0.73590584\n",
            "Iteration 9489, loss = 0.73587692\n",
            "Iteration 9490, loss = 0.73584801\n",
            "Iteration 9491, loss = 0.73581911\n",
            "Iteration 9492, loss = 0.73579022\n",
            "Iteration 9493, loss = 0.73576133\n",
            "Iteration 9494, loss = 0.73573246\n",
            "Iteration 9495, loss = 0.73570360\n",
            "Iteration 9496, loss = 0.73567475\n",
            "Iteration 9497, loss = 0.73564591\n",
            "Iteration 9498, loss = 0.73561708\n",
            "Iteration 9499, loss = 0.73558826\n",
            "Iteration 9500, loss = 0.73555945\n",
            "Iteration 9501, loss = 0.73553065\n",
            "Iteration 9502, loss = 0.73550187\n",
            "Iteration 9503, loss = 0.73547309\n",
            "Iteration 9504, loss = 0.73544432\n",
            "Iteration 9505, loss = 0.73541556\n",
            "Iteration 9506, loss = 0.73538681\n",
            "Iteration 9507, loss = 0.73535808\n",
            "Iteration 9508, loss = 0.73532935\n",
            "Iteration 9509, loss = 0.73530063\n",
            "Iteration 9510, loss = 0.73527193\n",
            "Iteration 9511, loss = 0.73524323\n",
            "Iteration 9512, loss = 0.73521454\n",
            "Iteration 9513, loss = 0.73518587\n",
            "Iteration 9514, loss = 0.73515720\n",
            "Iteration 9515, loss = 0.73512855\n",
            "Iteration 9516, loss = 0.73509990\n",
            "Iteration 9517, loss = 0.73507127\n",
            "Iteration 9518, loss = 0.73504264\n",
            "Iteration 9519, loss = 0.73501403\n",
            "Iteration 9520, loss = 0.73498542\n",
            "Iteration 9521, loss = 0.73495683\n",
            "Iteration 9522, loss = 0.73492825\n",
            "Iteration 9523, loss = 0.73489967\n",
            "Iteration 9524, loss = 0.73487111\n",
            "Iteration 9525, loss = 0.73484256\n",
            "Iteration 9526, loss = 0.73481401\n",
            "Iteration 9527, loss = 0.73478548\n",
            "Iteration 9528, loss = 0.73475696\n",
            "Iteration 9529, loss = 0.73472844\n",
            "Iteration 9530, loss = 0.73469994\n",
            "Iteration 9531, loss = 0.73467145\n",
            "Iteration 9532, loss = 0.73464297\n",
            "Iteration 9533, loss = 0.73461449\n",
            "Iteration 9534, loss = 0.73458603\n",
            "Iteration 9535, loss = 0.73455758\n",
            "Iteration 9536, loss = 0.73452914\n",
            "Iteration 9537, loss = 0.73450071\n",
            "Iteration 9538, loss = 0.73447229\n",
            "Iteration 9539, loss = 0.73444387\n",
            "Iteration 9540, loss = 0.73441547\n",
            "Iteration 9541, loss = 0.73438708\n",
            "Iteration 9542, loss = 0.73435870\n",
            "Iteration 9543, loss = 0.73433033\n",
            "Iteration 9544, loss = 0.73430197\n",
            "Iteration 9545, loss = 0.73427362\n",
            "Iteration 9546, loss = 0.73424528\n",
            "Iteration 9547, loss = 0.73421694\n",
            "Iteration 9548, loss = 0.73418862\n",
            "Iteration 9549, loss = 0.73416031\n",
            "Iteration 9550, loss = 0.73413201\n",
            "Iteration 9551, loss = 0.73410372\n",
            "Iteration 9552, loss = 0.73407544\n",
            "Iteration 9553, loss = 0.73404717\n",
            "Iteration 9554, loss = 0.73401891\n",
            "Iteration 9555, loss = 0.73399066\n",
            "Iteration 9556, loss = 0.73396242\n",
            "Iteration 9557, loss = 0.73393419\n",
            "Iteration 9558, loss = 0.73390597\n",
            "Iteration 9559, loss = 0.73387776\n",
            "Iteration 9560, loss = 0.73384955\n",
            "Iteration 9561, loss = 0.73382136\n",
            "Iteration 9562, loss = 0.73379318\n",
            "Iteration 9563, loss = 0.73376501\n",
            "Iteration 9564, loss = 0.73373685\n",
            "Iteration 9565, loss = 0.73370870\n",
            "Iteration 9566, loss = 0.73368056\n",
            "Iteration 9567, loss = 0.73365243\n",
            "Iteration 9568, loss = 0.73362431\n",
            "Iteration 9569, loss = 0.73359620\n",
            "Iteration 9570, loss = 0.73356810\n",
            "Iteration 9571, loss = 0.73354000\n",
            "Iteration 9572, loss = 0.73351192\n",
            "Iteration 9573, loss = 0.73348385\n",
            "Iteration 9574, loss = 0.73345579\n",
            "Iteration 9575, loss = 0.73342774\n",
            "Iteration 9576, loss = 0.73339970\n",
            "Iteration 9577, loss = 0.73337167\n",
            "Iteration 9578, loss = 0.73334364\n",
            "Iteration 9579, loss = 0.73331563\n",
            "Iteration 9580, loss = 0.73328763\n",
            "Iteration 9581, loss = 0.73325964\n",
            "Iteration 9582, loss = 0.73323165\n",
            "Iteration 9583, loss = 0.73320368\n",
            "Iteration 9584, loss = 0.73317572\n",
            "Iteration 9585, loss = 0.73314777\n",
            "Iteration 9586, loss = 0.73311982\n",
            "Iteration 9587, loss = 0.73309189\n",
            "Iteration 9588, loss = 0.73306397\n",
            "Iteration 9589, loss = 0.73303605\n",
            "Iteration 9590, loss = 0.73300815\n",
            "Iteration 9591, loss = 0.73298026\n",
            "Iteration 9592, loss = 0.73295237\n",
            "Iteration 9593, loss = 0.73292450\n",
            "Iteration 9594, loss = 0.73289663\n",
            "Iteration 9595, loss = 0.73286878\n",
            "Iteration 9596, loss = 0.73284094\n",
            "Iteration 9597, loss = 0.73281310\n",
            "Iteration 9598, loss = 0.73278528\n",
            "Iteration 9599, loss = 0.73275746\n",
            "Iteration 9600, loss = 0.73272966\n",
            "Iteration 9601, loss = 0.73270186\n",
            "Iteration 9602, loss = 0.73267407\n",
            "Iteration 9603, loss = 0.73264630\n",
            "Iteration 9604, loss = 0.73261853\n",
            "Iteration 9605, loss = 0.73259077\n",
            "Iteration 9606, loss = 0.73256303\n",
            "Iteration 9607, loss = 0.73253529\n",
            "Iteration 9608, loss = 0.73250756\n",
            "Iteration 9609, loss = 0.73247984\n",
            "Iteration 9610, loss = 0.73245214\n",
            "Iteration 9611, loss = 0.73242444\n",
            "Iteration 9612, loss = 0.73239675\n",
            "Iteration 9613, loss = 0.73236907\n",
            "Iteration 9614, loss = 0.73234140\n",
            "Iteration 9615, loss = 0.73231374\n",
            "Iteration 9616, loss = 0.73228609\n",
            "Iteration 9617, loss = 0.73225845\n",
            "Iteration 9618, loss = 0.73223082\n",
            "Iteration 9619, loss = 0.73220320\n",
            "Iteration 9620, loss = 0.73217559\n",
            "Iteration 9621, loss = 0.73214799\n",
            "Iteration 9622, loss = 0.73212040\n",
            "Iteration 9623, loss = 0.73209282\n",
            "Iteration 9624, loss = 0.73206524\n",
            "Iteration 9625, loss = 0.73203768\n",
            "Iteration 9626, loss = 0.73201013\n",
            "Iteration 9627, loss = 0.73198258\n",
            "Iteration 9628, loss = 0.73195505\n",
            "Iteration 9629, loss = 0.73192753\n",
            "Iteration 9630, loss = 0.73190001\n",
            "Iteration 9631, loss = 0.73187251\n",
            "Iteration 9632, loss = 0.73184501\n",
            "Iteration 9633, loss = 0.73181753\n",
            "Iteration 9634, loss = 0.73179005\n",
            "Iteration 9635, loss = 0.73176258\n",
            "Iteration 9636, loss = 0.73173513\n",
            "Iteration 9637, loss = 0.73170768\n",
            "Iteration 9638, loss = 0.73168024\n",
            "Iteration 9639, loss = 0.73165281\n",
            "Iteration 9640, loss = 0.73162539\n",
            "Iteration 9641, loss = 0.73159799\n",
            "Iteration 9642, loss = 0.73157059\n",
            "Iteration 9643, loss = 0.73154320\n",
            "Iteration 9644, loss = 0.73151582\n",
            "Iteration 9645, loss = 0.73148845\n",
            "Iteration 9646, loss = 0.73146108\n",
            "Iteration 9647, loss = 0.73143373\n",
            "Iteration 9648, loss = 0.73140639\n",
            "Iteration 9649, loss = 0.73137906\n",
            "Iteration 9650, loss = 0.73135173\n",
            "Iteration 9651, loss = 0.73132442\n",
            "Iteration 9652, loss = 0.73129712\n",
            "Iteration 9653, loss = 0.73126982\n",
            "Iteration 9654, loss = 0.73124254\n",
            "Iteration 9655, loss = 0.73121526\n",
            "Iteration 9656, loss = 0.73118800\n",
            "Iteration 9657, loss = 0.73116074\n",
            "Iteration 9658, loss = 0.73113349\n",
            "Iteration 9659, loss = 0.73110625\n",
            "Iteration 9660, loss = 0.73107903\n",
            "Iteration 9661, loss = 0.73105181\n",
            "Iteration 9662, loss = 0.73102460\n",
            "Iteration 9663, loss = 0.73099740\n",
            "Iteration 9664, loss = 0.73097021\n",
            "Iteration 9665, loss = 0.73094303\n",
            "Iteration 9666, loss = 0.73091586\n",
            "Iteration 9667, loss = 0.73088869\n",
            "Iteration 9668, loss = 0.73086154\n",
            "Iteration 9669, loss = 0.73083440\n",
            "Iteration 9670, loss = 0.73080726\n",
            "Iteration 9671, loss = 0.73078014\n",
            "Iteration 9672, loss = 0.73075303\n",
            "Iteration 9673, loss = 0.73072592\n",
            "Iteration 9674, loss = 0.73069882\n",
            "Iteration 9675, loss = 0.73067174\n",
            "Iteration 9676, loss = 0.73064466\n",
            "Iteration 9677, loss = 0.73061759\n",
            "Iteration 9678, loss = 0.73059053\n",
            "Iteration 9679, loss = 0.73056348\n",
            "Iteration 9680, loss = 0.73053645\n",
            "Iteration 9681, loss = 0.73050941\n",
            "Iteration 9682, loss = 0.73048239\n",
            "Iteration 9683, loss = 0.73045538\n",
            "Iteration 9684, loss = 0.73042838\n",
            "Iteration 9685, loss = 0.73040139\n",
            "Iteration 9686, loss = 0.73037440\n",
            "Iteration 9687, loss = 0.73034743\n",
            "Iteration 9688, loss = 0.73032046\n",
            "Iteration 9689, loss = 0.73029351\n",
            "Iteration 9690, loss = 0.73026656\n",
            "Iteration 9691, loss = 0.73023963\n",
            "Iteration 9692, loss = 0.73021270\n",
            "Iteration 9693, loss = 0.73018578\n",
            "Iteration 9694, loss = 0.73015887\n",
            "Iteration 9695, loss = 0.73013197\n",
            "Iteration 9696, loss = 0.73010508\n",
            "Iteration 9697, loss = 0.73007820\n",
            "Iteration 9698, loss = 0.73005133\n",
            "Iteration 9699, loss = 0.73002446\n",
            "Iteration 9700, loss = 0.72999761\n",
            "Iteration 9701, loss = 0.72997077\n",
            "Iteration 9702, loss = 0.72994393\n",
            "Iteration 9703, loss = 0.72991711\n",
            "Iteration 9704, loss = 0.72989029\n",
            "Iteration 9705, loss = 0.72986348\n",
            "Iteration 9706, loss = 0.72983669\n",
            "Iteration 9707, loss = 0.72980990\n",
            "Iteration 9708, loss = 0.72978312\n",
            "Iteration 9709, loss = 0.72975635\n",
            "Iteration 9710, loss = 0.72972959\n",
            "Iteration 9711, loss = 0.72970284\n",
            "Iteration 9712, loss = 0.72967609\n",
            "Iteration 9713, loss = 0.72964936\n",
            "Iteration 9714, loss = 0.72962264\n",
            "Iteration 9715, loss = 0.72959592\n",
            "Iteration 9716, loss = 0.72956922\n",
            "Iteration 9717, loss = 0.72954252\n",
            "Iteration 9718, loss = 0.72951583\n",
            "Iteration 9719, loss = 0.72948915\n",
            "Iteration 9720, loss = 0.72946248\n",
            "Iteration 9721, loss = 0.72943582\n",
            "Iteration 9722, loss = 0.72940917\n",
            "Iteration 9723, loss = 0.72938253\n",
            "Iteration 9724, loss = 0.72935590\n",
            "Iteration 9725, loss = 0.72932928\n",
            "Iteration 9726, loss = 0.72930266\n",
            "Iteration 9727, loss = 0.72927606\n",
            "Iteration 9728, loss = 0.72924946\n",
            "Iteration 9729, loss = 0.72922288\n",
            "Iteration 9730, loss = 0.72919630\n",
            "Iteration 9731, loss = 0.72916973\n",
            "Iteration 9732, loss = 0.72914317\n",
            "Iteration 9733, loss = 0.72911662\n",
            "Iteration 9734, loss = 0.72909008\n",
            "Iteration 9735, loss = 0.72906355\n",
            "Iteration 9736, loss = 0.72903703\n",
            "Iteration 9737, loss = 0.72901051\n",
            "Iteration 9738, loss = 0.72898401\n",
            "Iteration 9739, loss = 0.72895751\n",
            "Iteration 9740, loss = 0.72893103\n",
            "Iteration 9741, loss = 0.72890455\n",
            "Iteration 9742, loss = 0.72887808\n",
            "Iteration 9743, loss = 0.72885162\n",
            "Iteration 9744, loss = 0.72882517\n",
            "Iteration 9745, loss = 0.72879873\n",
            "Iteration 9746, loss = 0.72877230\n",
            "Iteration 9747, loss = 0.72874588\n",
            "Iteration 9748, loss = 0.72871946\n",
            "Iteration 9749, loss = 0.72869306\n",
            "Iteration 9750, loss = 0.72866666\n",
            "Iteration 9751, loss = 0.72864027\n",
            "Iteration 9752, loss = 0.72861390\n",
            "Iteration 9753, loss = 0.72858753\n",
            "Iteration 9754, loss = 0.72856117\n",
            "Iteration 9755, loss = 0.72853482\n",
            "Iteration 9756, loss = 0.72850847\n",
            "Iteration 9757, loss = 0.72848214\n",
            "Iteration 9758, loss = 0.72845582\n",
            "Iteration 9759, loss = 0.72842950\n",
            "Iteration 9760, loss = 0.72840320\n",
            "Iteration 9761, loss = 0.72837690\n",
            "Iteration 9762, loss = 0.72835061\n",
            "Iteration 9763, loss = 0.72832433\n",
            "Iteration 9764, loss = 0.72829806\n",
            "Iteration 9765, loss = 0.72827180\n",
            "Iteration 9766, loss = 0.72824555\n",
            "Iteration 9767, loss = 0.72821931\n",
            "Iteration 9768, loss = 0.72819307\n",
            "Iteration 9769, loss = 0.72816685\n",
            "Iteration 9770, loss = 0.72814063\n",
            "Iteration 9771, loss = 0.72811442\n",
            "Iteration 9772, loss = 0.72808823\n",
            "Iteration 9773, loss = 0.72806204\n",
            "Iteration 9774, loss = 0.72803586\n",
            "Iteration 9775, loss = 0.72800968\n",
            "Iteration 9776, loss = 0.72798352\n",
            "Iteration 9777, loss = 0.72795737\n",
            "Iteration 9778, loss = 0.72793122\n",
            "Iteration 9779, loss = 0.72790509\n",
            "Iteration 9780, loss = 0.72787896\n",
            "Iteration 9781, loss = 0.72785284\n",
            "Iteration 9782, loss = 0.72782673\n",
            "Iteration 9783, loss = 0.72780063\n",
            "Iteration 9784, loss = 0.72777454\n",
            "Iteration 9785, loss = 0.72774846\n",
            "Iteration 9786, loss = 0.72772238\n",
            "Iteration 9787, loss = 0.72769632\n",
            "Iteration 9788, loss = 0.72767026\n",
            "Iteration 9789, loss = 0.72764421\n",
            "Iteration 9790, loss = 0.72761818\n",
            "Iteration 9791, loss = 0.72759215\n",
            "Iteration 9792, loss = 0.72756613\n",
            "Iteration 9793, loss = 0.72754011\n",
            "Iteration 9794, loss = 0.72751411\n",
            "Iteration 9795, loss = 0.72748812\n",
            "Iteration 9796, loss = 0.72746213\n",
            "Iteration 9797, loss = 0.72743615\n",
            "Iteration 9798, loss = 0.72741019\n",
            "Iteration 9799, loss = 0.72738423\n",
            "Iteration 9800, loss = 0.72735828\n",
            "Iteration 9801, loss = 0.72733233\n",
            "Iteration 9802, loss = 0.72730640\n",
            "Iteration 9803, loss = 0.72728048\n",
            "Iteration 9804, loss = 0.72725456\n",
            "Iteration 9805, loss = 0.72722866\n",
            "Iteration 9806, loss = 0.72720276\n",
            "Iteration 9807, loss = 0.72717687\n",
            "Iteration 9808, loss = 0.72715099\n",
            "Iteration 9809, loss = 0.72712512\n",
            "Iteration 9810, loss = 0.72709926\n",
            "Iteration 9811, loss = 0.72707340\n",
            "Iteration 9812, loss = 0.72704756\n",
            "Iteration 9813, loss = 0.72702172\n",
            "Iteration 9814, loss = 0.72699589\n",
            "Iteration 9815, loss = 0.72697007\n",
            "Iteration 9816, loss = 0.72694426\n",
            "Iteration 9817, loss = 0.72691846\n",
            "Iteration 9818, loss = 0.72689267\n",
            "Iteration 9819, loss = 0.72686688\n",
            "Iteration 9820, loss = 0.72684111\n",
            "Iteration 9821, loss = 0.72681534\n",
            "Iteration 9822, loss = 0.72678958\n",
            "Iteration 9823, loss = 0.72676384\n",
            "Iteration 9824, loss = 0.72673809\n",
            "Iteration 9825, loss = 0.72671236\n",
            "Iteration 9826, loss = 0.72668664\n",
            "Iteration 9827, loss = 0.72666093\n",
            "Iteration 9828, loss = 0.72663522\n",
            "Iteration 9829, loss = 0.72660952\n",
            "Iteration 9830, loss = 0.72658383\n",
            "Iteration 9831, loss = 0.72655815\n",
            "Iteration 9832, loss = 0.72653248\n",
            "Iteration 9833, loss = 0.72650682\n",
            "Iteration 9834, loss = 0.72648117\n",
            "Iteration 9835, loss = 0.72645552\n",
            "Iteration 9836, loss = 0.72642988\n",
            "Iteration 9837, loss = 0.72640426\n",
            "Iteration 9838, loss = 0.72637864\n",
            "Iteration 9839, loss = 0.72635303\n",
            "Iteration 9840, loss = 0.72632742\n",
            "Iteration 9841, loss = 0.72630183\n",
            "Iteration 9842, loss = 0.72627624\n",
            "Iteration 9843, loss = 0.72625067\n",
            "Iteration 9844, loss = 0.72622510\n",
            "Iteration 9845, loss = 0.72619954\n",
            "Iteration 9846, loss = 0.72617399\n",
            "Iteration 9847, loss = 0.72614845\n",
            "Iteration 9848, loss = 0.72612292\n",
            "Iteration 9849, loss = 0.72609739\n",
            "Iteration 9850, loss = 0.72607187\n",
            "Iteration 9851, loss = 0.72604637\n",
            "Iteration 9852, loss = 0.72602087\n",
            "Iteration 9853, loss = 0.72599538\n",
            "Iteration 9854, loss = 0.72596989\n",
            "Iteration 9855, loss = 0.72594442\n",
            "Iteration 9856, loss = 0.72591896\n",
            "Iteration 9857, loss = 0.72589350\n",
            "Iteration 9858, loss = 0.72586805\n",
            "Iteration 9859, loss = 0.72584261\n",
            "Iteration 9860, loss = 0.72581718\n",
            "Iteration 9861, loss = 0.72579176\n",
            "Iteration 9862, loss = 0.72576635\n",
            "Iteration 9863, loss = 0.72574094\n",
            "Iteration 9864, loss = 0.72571554\n",
            "Iteration 9865, loss = 0.72569016\n",
            "Iteration 9866, loss = 0.72566478\n",
            "Iteration 9867, loss = 0.72563940\n",
            "Iteration 9868, loss = 0.72561404\n",
            "Iteration 9869, loss = 0.72558869\n",
            "Iteration 9870, loss = 0.72556334\n",
            "Iteration 9871, loss = 0.72553800\n",
            "Iteration 9872, loss = 0.72551268\n",
            "Iteration 9873, loss = 0.72548736\n",
            "Iteration 9874, loss = 0.72546204\n",
            "Iteration 9875, loss = 0.72543674\n",
            "Iteration 9876, loss = 0.72541145\n",
            "Iteration 9877, loss = 0.72538616\n",
            "Iteration 9878, loss = 0.72536088\n",
            "Iteration 9879, loss = 0.72533561\n",
            "Iteration 9880, loss = 0.72531035\n",
            "Iteration 9881, loss = 0.72528510\n",
            "Iteration 9882, loss = 0.72525985\n",
            "Iteration 9883, loss = 0.72523462\n",
            "Iteration 9884, loss = 0.72520939\n",
            "Iteration 9885, loss = 0.72518417\n",
            "Iteration 9886, loss = 0.72515896\n",
            "Iteration 9887, loss = 0.72513376\n",
            "Iteration 9888, loss = 0.72510856\n",
            "Iteration 9889, loss = 0.72508338\n",
            "Iteration 9890, loss = 0.72505820\n",
            "Iteration 9891, loss = 0.72503303\n",
            "Iteration 9892, loss = 0.72500787\n",
            "Iteration 9893, loss = 0.72498272\n",
            "Iteration 9894, loss = 0.72495758\n",
            "Iteration 9895, loss = 0.72493244\n",
            "Iteration 9896, loss = 0.72490732\n",
            "Iteration 9897, loss = 0.72488220\n",
            "Iteration 9898, loss = 0.72485709\n",
            "Iteration 9899, loss = 0.72483199\n",
            "Iteration 9900, loss = 0.72480689\n",
            "Iteration 9901, loss = 0.72478181\n",
            "Iteration 9902, loss = 0.72475673\n",
            "Iteration 9903, loss = 0.72473166\n",
            "Iteration 9904, loss = 0.72470660\n",
            "Iteration 9905, loss = 0.72468155\n",
            "Iteration 9906, loss = 0.72465651\n",
            "Iteration 9907, loss = 0.72463148\n",
            "Iteration 9908, loss = 0.72460645\n",
            "Iteration 9909, loss = 0.72458143\n",
            "Iteration 9910, loss = 0.72455642\n",
            "Iteration 9911, loss = 0.72453142\n",
            "Iteration 9912, loss = 0.72450643\n",
            "Iteration 9913, loss = 0.72448144\n",
            "Iteration 9914, loss = 0.72445646\n",
            "Iteration 9915, loss = 0.72443150\n",
            "Iteration 9916, loss = 0.72440654\n",
            "Iteration 9917, loss = 0.72438158\n",
            "Iteration 9918, loss = 0.72435664\n",
            "Iteration 9919, loss = 0.72433171\n",
            "Iteration 9920, loss = 0.72430678\n",
            "Iteration 9921, loss = 0.72428186\n",
            "Iteration 9922, loss = 0.72425695\n",
            "Iteration 9923, loss = 0.72423205\n",
            "Iteration 9924, loss = 0.72420715\n",
            "Iteration 9925, loss = 0.72418227\n",
            "Iteration 9926, loss = 0.72415739\n",
            "Iteration 9927, loss = 0.72413252\n",
            "Iteration 9928, loss = 0.72410766\n",
            "Iteration 9929, loss = 0.72408281\n",
            "Iteration 9930, loss = 0.72405797\n",
            "Iteration 9931, loss = 0.72403313\n",
            "Iteration 9932, loss = 0.72400830\n",
            "Iteration 9933, loss = 0.72398348\n",
            "Iteration 9934, loss = 0.72395867\n",
            "Iteration 9935, loss = 0.72393387\n",
            "Iteration 9936, loss = 0.72390907\n",
            "Iteration 9937, loss = 0.72388429\n",
            "Iteration 9938, loss = 0.72385951\n",
            "Iteration 9939, loss = 0.72383474\n",
            "Iteration 9940, loss = 0.72380997\n",
            "Iteration 9941, loss = 0.72378522\n",
            "Iteration 9942, loss = 0.72376047\n",
            "Iteration 9943, loss = 0.72373574\n",
            "Iteration 9944, loss = 0.72371101\n",
            "Iteration 9945, loss = 0.72368629\n",
            "Iteration 9946, loss = 0.72366157\n",
            "Iteration 9947, loss = 0.72363687\n",
            "Iteration 9948, loss = 0.72361217\n",
            "Iteration 9949, loss = 0.72358748\n",
            "Iteration 9950, loss = 0.72356280\n",
            "Iteration 9951, loss = 0.72353813\n",
            "Iteration 9952, loss = 0.72351347\n",
            "Iteration 9953, loss = 0.72348881\n",
            "Iteration 9954, loss = 0.72346416\n",
            "Iteration 9955, loss = 0.72343952\n",
            "Iteration 9956, loss = 0.72341489\n",
            "Iteration 9957, loss = 0.72339027\n",
            "Iteration 9958, loss = 0.72336565\n",
            "Iteration 9959, loss = 0.72334105\n",
            "Iteration 9960, loss = 0.72331645\n",
            "Iteration 9961, loss = 0.72329186\n",
            "Iteration 9962, loss = 0.72326727\n",
            "Iteration 9963, loss = 0.72324270\n",
            "Iteration 9964, loss = 0.72321813\n",
            "Iteration 9965, loss = 0.72319357\n",
            "Iteration 9966, loss = 0.72316902\n",
            "Iteration 9967, loss = 0.72314448\n",
            "Iteration 9968, loss = 0.72311995\n",
            "Iteration 9969, loss = 0.72309542\n",
            "Iteration 9970, loss = 0.72307090\n",
            "Iteration 9971, loss = 0.72304639\n",
            "Iteration 9972, loss = 0.72302189\n",
            "Iteration 9973, loss = 0.72299740\n",
            "Iteration 9974, loss = 0.72297291\n",
            "Iteration 9975, loss = 0.72294843\n",
            "Iteration 9976, loss = 0.72292396\n",
            "Iteration 9977, loss = 0.72289950\n",
            "Iteration 9978, loss = 0.72287505\n",
            "Iteration 9979, loss = 0.72285060\n",
            "Iteration 9980, loss = 0.72282616\n",
            "Iteration 9981, loss = 0.72280173\n",
            "Iteration 9982, loss = 0.72277731\n",
            "Iteration 9983, loss = 0.72275290\n",
            "Iteration 9984, loss = 0.72272849\n",
            "Iteration 9985, loss = 0.72270410\n",
            "Iteration 9986, loss = 0.72267971\n",
            "Iteration 9987, loss = 0.72265533\n",
            "Iteration 9988, loss = 0.72263095\n",
            "Iteration 9989, loss = 0.72260659\n",
            "Iteration 9990, loss = 0.72258223\n",
            "Iteration 9991, loss = 0.72255788\n",
            "Iteration 9992, loss = 0.72253354\n",
            "Iteration 9993, loss = 0.72250921\n",
            "Iteration 9994, loss = 0.72248488\n",
            "Iteration 9995, loss = 0.72246056\n",
            "Iteration 9996, loss = 0.72243625\n",
            "Iteration 9997, loss = 0.72241195\n",
            "Iteration 9998, loss = 0.72238766\n",
            "Iteration 9999, loss = 0.72236337\n",
            "Iteration 10000, loss = 0.72233910\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='tanh', hidden_layer_sizes=(2, 40),\n",
              "              learning_rate_init=0.0001, max_iter=10000, tol=1e-06,\n",
              "              verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resultado da acurácia\n",
        "# Modelo tanh foi o que obteve maior acurácia\n",
        "# significa que o acerto da previsão do modelo acerta 45% dos resultados (Infelizmente é melhor utilizar uma moeda)\n",
        "\n",
        "redeneural_1.score(X_test,Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoQbWv_Z9TLj",
        "outputId": "baabc957-f894-40ce-f154-7eff9fefcc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear retificada\n",
        "\n",
        "redeneural_4 = MLPClassifier(verbose=True,\n",
        "                           max_iter=10000,\n",
        "                           hidden_layer_sizes=(2,40),\n",
        "                           tol=0.000001,\n",
        "                           activation='relu',\n",
        "                           learning_rate_init = 0.0001)"
      ],
      "metadata": {
        "id": "HiVE2ssh-E66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando o modelo\n",
        "redeneural_4.fit(X_train,Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ4Cj5-MAkfW",
        "outputId": "7e55ce07-48a3-42a6-b913-7aced18f44e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "Iteration 5001, loss = 0.99397794\n",
            "Iteration 5002, loss = 0.99389775\n",
            "Iteration 5003, loss = 0.99381694\n",
            "Iteration 5004, loss = 0.99373756\n",
            "Iteration 5005, loss = 0.99365786\n",
            "Iteration 5006, loss = 0.99357850\n",
            "Iteration 5007, loss = 0.99349822\n",
            "Iteration 5008, loss = 0.99341724\n",
            "Iteration 5009, loss = 0.99333743\n",
            "Iteration 5010, loss = 0.99325772\n",
            "Iteration 5011, loss = 0.99317780\n",
            "Iteration 5012, loss = 0.99309750\n",
            "Iteration 5013, loss = 0.99301753\n",
            "Iteration 5014, loss = 0.99293854\n",
            "Iteration 5015, loss = 0.99285859\n",
            "Iteration 5016, loss = 0.99278036\n",
            "Iteration 5017, loss = 0.99269905\n",
            "Iteration 5018, loss = 0.99262084\n",
            "Iteration 5019, loss = 0.99254180\n",
            "Iteration 5020, loss = 0.99246176\n",
            "Iteration 5021, loss = 0.99238138\n",
            "Iteration 5022, loss = 0.99230288\n",
            "Iteration 5023, loss = 0.99222364\n",
            "Iteration 5024, loss = 0.99214482\n",
            "Iteration 5025, loss = 0.99206537\n",
            "Iteration 5026, loss = 0.99198572\n",
            "Iteration 5027, loss = 0.99190706\n",
            "Iteration 5028, loss = 0.99182848\n",
            "Iteration 5029, loss = 0.99174866\n",
            "Iteration 5030, loss = 0.99167054\n",
            "Iteration 5031, loss = 0.99159211\n",
            "Iteration 5032, loss = 0.99151313\n",
            "Iteration 5033, loss = 0.99143366\n",
            "Iteration 5034, loss = 0.99135404\n",
            "Iteration 5035, loss = 0.99127630\n",
            "Iteration 5036, loss = 0.99119765\n",
            "Iteration 5037, loss = 0.99111890\n",
            "Iteration 5038, loss = 0.99103966\n",
            "Iteration 5039, loss = 0.99096101\n",
            "Iteration 5040, loss = 0.99088321\n",
            "Iteration 5041, loss = 0.99080468\n",
            "Iteration 5042, loss = 0.99072550\n",
            "Iteration 5043, loss = 0.99064737\n",
            "Iteration 5044, loss = 0.99056879\n",
            "Iteration 5045, loss = 0.99049021\n",
            "Iteration 5046, loss = 0.99041203\n",
            "Iteration 5047, loss = 0.99033416\n",
            "Iteration 5048, loss = 0.99025599\n",
            "Iteration 5049, loss = 0.99017728\n",
            "Iteration 5050, loss = 0.99009851\n",
            "Iteration 5051, loss = 0.99002086\n",
            "Iteration 5052, loss = 0.98994396\n",
            "Iteration 5053, loss = 0.98986473\n",
            "Iteration 5054, loss = 0.98978677\n",
            "Iteration 5055, loss = 0.98970917\n",
            "Iteration 5056, loss = 0.98963141\n",
            "Iteration 5057, loss = 0.98955347\n",
            "Iteration 5058, loss = 0.98947523\n",
            "Iteration 5059, loss = 0.98939733\n",
            "Iteration 5060, loss = 0.98932072\n",
            "Iteration 5061, loss = 0.98924281\n",
            "Iteration 5062, loss = 0.98916523\n",
            "Iteration 5063, loss = 0.98908746\n",
            "Iteration 5064, loss = 0.98901057\n",
            "Iteration 5065, loss = 0.98893291\n",
            "Iteration 5066, loss = 0.98885511\n",
            "Iteration 5067, loss = 0.98877787\n",
            "Iteration 5068, loss = 0.98870035\n",
            "Iteration 5069, loss = 0.98862310\n",
            "Iteration 5070, loss = 0.98854585\n",
            "Iteration 5071, loss = 0.98846899\n",
            "Iteration 5072, loss = 0.98839166\n",
            "Iteration 5073, loss = 0.98831453\n",
            "Iteration 5074, loss = 0.98823723\n",
            "Iteration 5075, loss = 0.98816005\n",
            "Iteration 5076, loss = 0.98808296\n",
            "Iteration 5077, loss = 0.98800636\n",
            "Iteration 5078, loss = 0.98792930\n",
            "Iteration 5079, loss = 0.98785349\n",
            "Iteration 5080, loss = 0.98777547\n",
            "Iteration 5081, loss = 0.98769880\n",
            "Iteration 5082, loss = 0.98762186\n",
            "Iteration 5083, loss = 0.98754521\n",
            "Iteration 5084, loss = 0.98746898\n",
            "Iteration 5085, loss = 0.98739220\n",
            "Iteration 5086, loss = 0.98731526\n",
            "Iteration 5087, loss = 0.98723864\n",
            "Iteration 5088, loss = 0.98716210\n",
            "Iteration 5089, loss = 0.98708628\n",
            "Iteration 5090, loss = 0.98700959\n",
            "Iteration 5091, loss = 0.98693386\n",
            "Iteration 5092, loss = 0.98685722\n",
            "Iteration 5093, loss = 0.98678053\n",
            "Iteration 5094, loss = 0.98670496\n",
            "Iteration 5095, loss = 0.98662880\n",
            "Iteration 5096, loss = 0.98655243\n",
            "Iteration 5097, loss = 0.98647592\n",
            "Iteration 5098, loss = 0.98640082\n",
            "Iteration 5099, loss = 0.98632475\n",
            "Iteration 5100, loss = 0.98624858\n",
            "Iteration 5101, loss = 0.98617355\n",
            "Iteration 5102, loss = 0.98609613\n",
            "Iteration 5103, loss = 0.98601979\n",
            "Iteration 5104, loss = 0.98594467\n",
            "Iteration 5105, loss = 0.98586888\n",
            "Iteration 5106, loss = 0.98579267\n",
            "Iteration 5107, loss = 0.98571744\n",
            "Iteration 5108, loss = 0.98564169\n",
            "Iteration 5109, loss = 0.98556661\n",
            "Iteration 5110, loss = 0.98549136\n",
            "Iteration 5111, loss = 0.98541476\n",
            "Iteration 5112, loss = 0.98533872\n",
            "Iteration 5113, loss = 0.98526269\n",
            "Iteration 5114, loss = 0.98518644\n",
            "Iteration 5115, loss = 0.98510877\n",
            "Iteration 5116, loss = 0.98503089\n",
            "Iteration 5117, loss = 0.98495326\n",
            "Iteration 5118, loss = 0.98487550\n",
            "Iteration 5119, loss = 0.98479718\n",
            "Iteration 5120, loss = 0.98471843\n",
            "Iteration 5121, loss = 0.98463963\n",
            "Iteration 5122, loss = 0.98456177\n",
            "Iteration 5123, loss = 0.98448340\n",
            "Iteration 5124, loss = 0.98440367\n",
            "Iteration 5125, loss = 0.98432499\n",
            "Iteration 5126, loss = 0.98424591\n",
            "Iteration 5127, loss = 0.98416725\n",
            "Iteration 5128, loss = 0.98408765\n",
            "Iteration 5129, loss = 0.98400939\n",
            "Iteration 5130, loss = 0.98393257\n",
            "Iteration 5131, loss = 0.98385243\n",
            "Iteration 5132, loss = 0.98377253\n",
            "Iteration 5133, loss = 0.98369331\n",
            "Iteration 5134, loss = 0.98361654\n",
            "Iteration 5135, loss = 0.98354003\n",
            "Iteration 5136, loss = 0.98346370\n",
            "Iteration 5137, loss = 0.98338668\n",
            "Iteration 5138, loss = 0.98330903\n",
            "Iteration 5139, loss = 0.98323289\n",
            "Iteration 5140, loss = 0.98315715\n",
            "Iteration 5141, loss = 0.98308095\n",
            "Iteration 5142, loss = 0.98300415\n",
            "Iteration 5143, loss = 0.98292671\n",
            "Iteration 5144, loss = 0.98284961\n",
            "Iteration 5145, loss = 0.98277551\n",
            "Iteration 5146, loss = 0.98269928\n",
            "Iteration 5147, loss = 0.98262259\n",
            "Iteration 5148, loss = 0.98254579\n",
            "Iteration 5149, loss = 0.98246976\n",
            "Iteration 5150, loss = 0.98239484\n",
            "Iteration 5151, loss = 0.98231908\n",
            "Iteration 5152, loss = 0.98224304\n",
            "Iteration 5153, loss = 0.98216680\n",
            "Iteration 5154, loss = 0.98209135\n",
            "Iteration 5155, loss = 0.98201547\n",
            "Iteration 5156, loss = 0.98193978\n",
            "Iteration 5157, loss = 0.98186448\n",
            "Iteration 5158, loss = 0.98178879\n",
            "Iteration 5159, loss = 0.98171318\n",
            "Iteration 5160, loss = 0.98163775\n",
            "Iteration 5161, loss = 0.98156225\n",
            "Iteration 5162, loss = 0.98148708\n",
            "Iteration 5163, loss = 0.98141170\n",
            "Iteration 5164, loss = 0.98133657\n",
            "Iteration 5165, loss = 0.98126088\n",
            "Iteration 5166, loss = 0.98118650\n",
            "Iteration 5167, loss = 0.98111144\n",
            "Iteration 5168, loss = 0.98103798\n",
            "Iteration 5169, loss = 0.98096213\n",
            "Iteration 5170, loss = 0.98088641\n",
            "Iteration 5171, loss = 0.98081113\n",
            "Iteration 5172, loss = 0.98073698\n",
            "Iteration 5173, loss = 0.98066340\n",
            "Iteration 5174, loss = 0.98058846\n",
            "Iteration 5175, loss = 0.98051256\n",
            "Iteration 5176, loss = 0.98043909\n",
            "Iteration 5177, loss = 0.98036314\n",
            "Iteration 5178, loss = 0.98028853\n",
            "Iteration 5179, loss = 0.98021387\n",
            "Iteration 5180, loss = 0.98013943\n",
            "Iteration 5181, loss = 0.98006485\n",
            "Iteration 5182, loss = 0.97999084\n",
            "Iteration 5183, loss = 0.97991679\n",
            "Iteration 5184, loss = 0.97984154\n",
            "Iteration 5185, loss = 0.97976756\n",
            "Iteration 5186, loss = 0.97969362\n",
            "Iteration 5187, loss = 0.97961870\n",
            "Iteration 5188, loss = 0.97954317\n",
            "Iteration 5189, loss = 0.97946960\n",
            "Iteration 5190, loss = 0.97939488\n",
            "Iteration 5191, loss = 0.97932159\n",
            "Iteration 5192, loss = 0.97924777\n",
            "Iteration 5193, loss = 0.97917333\n",
            "Iteration 5194, loss = 0.97909941\n",
            "Iteration 5195, loss = 0.97902466\n",
            "Iteration 5196, loss = 0.97895209\n",
            "Iteration 5197, loss = 0.97887895\n",
            "Iteration 5198, loss = 0.97880429\n",
            "Iteration 5199, loss = 0.97872969\n",
            "Iteration 5200, loss = 0.97865657\n",
            "Iteration 5201, loss = 0.97858248\n",
            "Iteration 5202, loss = 0.97850842\n",
            "Iteration 5203, loss = 0.97843570\n",
            "Iteration 5204, loss = 0.97836270\n",
            "Iteration 5205, loss = 0.97828799\n",
            "Iteration 5206, loss = 0.97821410\n",
            "Iteration 5207, loss = 0.97814044\n",
            "Iteration 5208, loss = 0.97806698\n",
            "Iteration 5209, loss = 0.97799328\n",
            "Iteration 5210, loss = 0.97792052\n",
            "Iteration 5211, loss = 0.97784725\n",
            "Iteration 5212, loss = 0.97777333\n",
            "Iteration 5213, loss = 0.97769955\n",
            "Iteration 5214, loss = 0.97762694\n",
            "Iteration 5215, loss = 0.97755409\n",
            "Iteration 5216, loss = 0.97748054\n",
            "Iteration 5217, loss = 0.97740746\n",
            "Iteration 5218, loss = 0.97733443\n",
            "Iteration 5219, loss = 0.97726205\n",
            "Iteration 5220, loss = 0.97718818\n",
            "Iteration 5221, loss = 0.97711594\n",
            "Iteration 5222, loss = 0.97704342\n",
            "Iteration 5223, loss = 0.97697047\n",
            "Iteration 5224, loss = 0.97689644\n",
            "Iteration 5225, loss = 0.97682388\n",
            "Iteration 5226, loss = 0.97675057\n",
            "Iteration 5227, loss = 0.97667653\n",
            "Iteration 5228, loss = 0.97660399\n",
            "Iteration 5229, loss = 0.97653071\n",
            "Iteration 5230, loss = 0.97645694\n",
            "Iteration 5231, loss = 0.97638306\n",
            "Iteration 5232, loss = 0.97630910\n",
            "Iteration 5233, loss = 0.97623498\n",
            "Iteration 5234, loss = 0.97616237\n",
            "Iteration 5235, loss = 0.97608928\n",
            "Iteration 5236, loss = 0.97601556\n",
            "Iteration 5237, loss = 0.97594211\n",
            "Iteration 5238, loss = 0.97586886\n",
            "Iteration 5239, loss = 0.97579606\n",
            "Iteration 5240, loss = 0.97572259\n",
            "Iteration 5241, loss = 0.97564929\n",
            "Iteration 5242, loss = 0.97557628\n",
            "Iteration 5243, loss = 0.97550301\n",
            "Iteration 5244, loss = 0.97543058\n",
            "Iteration 5245, loss = 0.97535729\n",
            "Iteration 5246, loss = 0.97528434\n",
            "Iteration 5247, loss = 0.97521204\n",
            "Iteration 5248, loss = 0.97513961\n",
            "Iteration 5249, loss = 0.97506650\n",
            "Iteration 5250, loss = 0.97499457\n",
            "Iteration 5251, loss = 0.97492198\n",
            "Iteration 5252, loss = 0.97484956\n",
            "Iteration 5253, loss = 0.97477683\n",
            "Iteration 5254, loss = 0.97470340\n",
            "Iteration 5255, loss = 0.97463183\n",
            "Iteration 5256, loss = 0.97455897\n",
            "Iteration 5257, loss = 0.97448684\n",
            "Iteration 5258, loss = 0.97441466\n",
            "Iteration 5259, loss = 0.97434144\n",
            "Iteration 5260, loss = 0.97427184\n",
            "Iteration 5261, loss = 0.97419891\n",
            "Iteration 5262, loss = 0.97412762\n",
            "Iteration 5263, loss = 0.97405656\n",
            "Iteration 5264, loss = 0.97398595\n",
            "Iteration 5265, loss = 0.97391492\n",
            "Iteration 5266, loss = 0.97384471\n",
            "Iteration 5267, loss = 0.97377426\n",
            "Iteration 5268, loss = 0.97370289\n",
            "Iteration 5269, loss = 0.97363284\n",
            "Iteration 5270, loss = 0.97356255\n",
            "Iteration 5271, loss = 0.97349123\n",
            "Iteration 5272, loss = 0.97342090\n",
            "Iteration 5273, loss = 0.97335116\n",
            "Iteration 5274, loss = 0.97328023\n",
            "Iteration 5275, loss = 0.97320960\n",
            "Iteration 5276, loss = 0.97314039\n",
            "Iteration 5277, loss = 0.97306981\n",
            "Iteration 5278, loss = 0.97300023\n",
            "Iteration 5279, loss = 0.97292924\n",
            "Iteration 5280, loss = 0.97286043\n",
            "Iteration 5281, loss = 0.97279123\n",
            "Iteration 5282, loss = 0.97272096\n",
            "Iteration 5283, loss = 0.97264984\n",
            "Iteration 5284, loss = 0.97257940\n",
            "Iteration 5285, loss = 0.97251004\n",
            "Iteration 5286, loss = 0.97244074\n",
            "Iteration 5287, loss = 0.97237055\n",
            "Iteration 5288, loss = 0.97230001\n",
            "Iteration 5289, loss = 0.97223140\n",
            "Iteration 5290, loss = 0.97216270\n",
            "Iteration 5291, loss = 0.97209400\n",
            "Iteration 5292, loss = 0.97202453\n",
            "Iteration 5293, loss = 0.97195598\n",
            "Iteration 5294, loss = 0.97188795\n",
            "Iteration 5295, loss = 0.97181914\n",
            "Iteration 5296, loss = 0.97175124\n",
            "Iteration 5297, loss = 0.97168235\n",
            "Iteration 5298, loss = 0.97161395\n",
            "Iteration 5299, loss = 0.97154654\n",
            "Iteration 5300, loss = 0.97147729\n",
            "Iteration 5301, loss = 0.97140832\n",
            "Iteration 5302, loss = 0.97134099\n",
            "Iteration 5303, loss = 0.97127416\n",
            "Iteration 5304, loss = 0.97120398\n",
            "Iteration 5305, loss = 0.97113631\n",
            "Iteration 5306, loss = 0.97106807\n",
            "Iteration 5307, loss = 0.97099977\n",
            "Iteration 5308, loss = 0.97093173\n",
            "Iteration 5309, loss = 0.97086421\n",
            "Iteration 5310, loss = 0.97079594\n",
            "Iteration 5311, loss = 0.97072814\n",
            "Iteration 5312, loss = 0.97066110\n",
            "Iteration 5313, loss = 0.97059307\n",
            "Iteration 5314, loss = 0.97052506\n",
            "Iteration 5315, loss = 0.97045944\n",
            "Iteration 5316, loss = 0.97038997\n",
            "Iteration 5317, loss = 0.97032286\n",
            "Iteration 5318, loss = 0.97025500\n",
            "Iteration 5319, loss = 0.97018791\n",
            "Iteration 5320, loss = 0.97011969\n",
            "Iteration 5321, loss = 0.97005193\n",
            "Iteration 5322, loss = 0.96998439\n",
            "Iteration 5323, loss = 0.96991735\n",
            "Iteration 5324, loss = 0.96984943\n",
            "Iteration 5325, loss = 0.96978152\n",
            "Iteration 5326, loss = 0.96971338\n",
            "Iteration 5327, loss = 0.96964536\n",
            "Iteration 5328, loss = 0.96957920\n",
            "Iteration 5329, loss = 0.96951294\n",
            "Iteration 5330, loss = 0.96944518\n",
            "Iteration 5331, loss = 0.96937784\n",
            "Iteration 5332, loss = 0.96931018\n",
            "Iteration 5333, loss = 0.96924417\n",
            "Iteration 5334, loss = 0.96917774\n",
            "Iteration 5335, loss = 0.96911082\n",
            "Iteration 5336, loss = 0.96904337\n",
            "Iteration 5337, loss = 0.96897625\n",
            "Iteration 5338, loss = 0.96890921\n",
            "Iteration 5339, loss = 0.96884257\n",
            "Iteration 5340, loss = 0.96877578\n",
            "Iteration 5341, loss = 0.96870960\n",
            "Iteration 5342, loss = 0.96864314\n",
            "Iteration 5343, loss = 0.96857481\n",
            "Iteration 5344, loss = 0.96850978\n",
            "Iteration 5345, loss = 0.96844393\n",
            "Iteration 5346, loss = 0.96837723\n",
            "Iteration 5347, loss = 0.96830966\n",
            "Iteration 5348, loss = 0.96824151\n",
            "Iteration 5349, loss = 0.96817540\n",
            "Iteration 5350, loss = 0.96810887\n",
            "Iteration 5351, loss = 0.96804219\n",
            "Iteration 5352, loss = 0.96797547\n",
            "Iteration 5353, loss = 0.96790880\n",
            "Iteration 5354, loss = 0.96784223\n",
            "Iteration 5355, loss = 0.96777675\n",
            "Iteration 5356, loss = 0.96771059\n",
            "Iteration 5357, loss = 0.96764405\n",
            "Iteration 5358, loss = 0.96757736\n",
            "Iteration 5359, loss = 0.96751148\n",
            "Iteration 5360, loss = 0.96744554\n",
            "Iteration 5361, loss = 0.96738015\n",
            "Iteration 5362, loss = 0.96731400\n",
            "Iteration 5363, loss = 0.96724701\n",
            "Iteration 5364, loss = 0.96718016\n",
            "Iteration 5365, loss = 0.96711408\n",
            "Iteration 5366, loss = 0.96704918\n",
            "Iteration 5367, loss = 0.96698342\n",
            "Iteration 5368, loss = 0.96691474\n",
            "Iteration 5369, loss = 0.96684173\n",
            "Iteration 5370, loss = 0.96676928\n",
            "Iteration 5371, loss = 0.96669549\n",
            "Iteration 5372, loss = 0.96662222\n",
            "Iteration 5373, loss = 0.96654616\n",
            "Iteration 5374, loss = 0.96646798\n",
            "Iteration 5375, loss = 0.96639095\n",
            "Iteration 5376, loss = 0.96631209\n",
            "Iteration 5377, loss = 0.96623213\n",
            "Iteration 5378, loss = 0.96615206\n",
            "Iteration 5379, loss = 0.96607110\n",
            "Iteration 5380, loss = 0.96598933\n",
            "Iteration 5381, loss = 0.96590781\n",
            "Iteration 5382, loss = 0.96582532\n",
            "Iteration 5383, loss = 0.96574214\n",
            "Iteration 5384, loss = 0.96566016\n",
            "Iteration 5385, loss = 0.96557744\n",
            "Iteration 5386, loss = 0.96549420\n",
            "Iteration 5387, loss = 0.96541039\n",
            "Iteration 5388, loss = 0.96532979\n",
            "Iteration 5389, loss = 0.96524661\n",
            "Iteration 5390, loss = 0.96516312\n",
            "Iteration 5391, loss = 0.96507858\n",
            "Iteration 5392, loss = 0.96499619\n",
            "Iteration 5393, loss = 0.96491346\n",
            "Iteration 5394, loss = 0.96483028\n",
            "Iteration 5395, loss = 0.96474670\n",
            "Iteration 5396, loss = 0.96466422\n",
            "Iteration 5397, loss = 0.96457967\n",
            "Iteration 5398, loss = 0.96449592\n",
            "Iteration 5399, loss = 0.96441231\n",
            "Iteration 5400, loss = 0.96433068\n",
            "Iteration 5401, loss = 0.96424824\n",
            "Iteration 5402, loss = 0.96416501\n",
            "Iteration 5403, loss = 0.96408082\n",
            "Iteration 5404, loss = 0.96399853\n",
            "Iteration 5405, loss = 0.96391612\n",
            "Iteration 5406, loss = 0.96383334\n",
            "Iteration 5407, loss = 0.96375024\n",
            "Iteration 5408, loss = 0.96366691\n",
            "Iteration 5409, loss = 0.96358334\n",
            "Iteration 5410, loss = 0.96350021\n",
            "Iteration 5411, loss = 0.96341793\n",
            "Iteration 5412, loss = 0.96333456\n",
            "Iteration 5413, loss = 0.96325194\n",
            "Iteration 5414, loss = 0.96316953\n",
            "Iteration 5415, loss = 0.96308760\n",
            "Iteration 5416, loss = 0.96300540\n",
            "Iteration 5417, loss = 0.96292290\n",
            "Iteration 5418, loss = 0.96284189\n",
            "Iteration 5419, loss = 0.96275960\n",
            "Iteration 5420, loss = 0.96267684\n",
            "Iteration 5421, loss = 0.96259578\n",
            "Iteration 5422, loss = 0.96251433\n",
            "Iteration 5423, loss = 0.96243297\n",
            "Iteration 5424, loss = 0.96235132\n",
            "Iteration 5425, loss = 0.96226994\n",
            "Iteration 5426, loss = 0.96218846\n",
            "Iteration 5427, loss = 0.96210787\n",
            "Iteration 5428, loss = 0.96202608\n",
            "Iteration 5429, loss = 0.96194434\n",
            "Iteration 5430, loss = 0.96186289\n",
            "Iteration 5431, loss = 0.96178178\n",
            "Iteration 5432, loss = 0.96170136\n",
            "Iteration 5433, loss = 0.96162016\n",
            "Iteration 5434, loss = 0.96153917\n",
            "Iteration 5435, loss = 0.96145799\n",
            "Iteration 5436, loss = 0.96137707\n",
            "Iteration 5437, loss = 0.96129703\n",
            "Iteration 5438, loss = 0.96121657\n",
            "Iteration 5439, loss = 0.96113600\n",
            "Iteration 5440, loss = 0.96105544\n",
            "Iteration 5441, loss = 0.96097455\n",
            "Iteration 5442, loss = 0.96089382\n",
            "Iteration 5443, loss = 0.96081454\n",
            "Iteration 5444, loss = 0.96073505\n",
            "Iteration 5445, loss = 0.96065444\n",
            "Iteration 5446, loss = 0.96057291\n",
            "Iteration 5447, loss = 0.96049383\n",
            "Iteration 5448, loss = 0.96041359\n",
            "Iteration 5449, loss = 0.96033380\n",
            "Iteration 5450, loss = 0.96025377\n",
            "Iteration 5451, loss = 0.96017372\n",
            "Iteration 5452, loss = 0.96009346\n",
            "Iteration 5453, loss = 0.96001349\n",
            "Iteration 5454, loss = 0.95993341\n",
            "Iteration 5455, loss = 0.95985405\n",
            "Iteration 5456, loss = 0.95977413\n",
            "Iteration 5457, loss = 0.95969471\n",
            "Iteration 5458, loss = 0.95961620\n",
            "Iteration 5459, loss = 0.95953684\n",
            "Iteration 5460, loss = 0.95945731\n",
            "Iteration 5461, loss = 0.95937743\n",
            "Iteration 5462, loss = 0.95929874\n",
            "Iteration 5463, loss = 0.95921955\n",
            "Iteration 5464, loss = 0.95914017\n",
            "Iteration 5465, loss = 0.95906216\n",
            "Iteration 5466, loss = 0.95898192\n",
            "Iteration 5467, loss = 0.95890329\n",
            "Iteration 5468, loss = 0.95882414\n",
            "Iteration 5469, loss = 0.95874487\n",
            "Iteration 5470, loss = 0.95866642\n",
            "Iteration 5471, loss = 0.95858761\n",
            "Iteration 5472, loss = 0.95850891\n",
            "Iteration 5473, loss = 0.95843048\n",
            "Iteration 5474, loss = 0.95835170\n",
            "Iteration 5475, loss = 0.95827343\n",
            "Iteration 5476, loss = 0.95819558\n",
            "Iteration 5477, loss = 0.95811632\n",
            "Iteration 5478, loss = 0.95803843\n",
            "Iteration 5479, loss = 0.95796078\n",
            "Iteration 5480, loss = 0.95788132\n",
            "Iteration 5481, loss = 0.95780332\n",
            "Iteration 5482, loss = 0.95772552\n",
            "Iteration 5483, loss = 0.95764739\n",
            "Iteration 5484, loss = 0.95756893\n",
            "Iteration 5485, loss = 0.95749156\n",
            "Iteration 5486, loss = 0.95741333\n",
            "Iteration 5487, loss = 0.95733411\n",
            "Iteration 5488, loss = 0.95725710\n",
            "Iteration 5489, loss = 0.95718037\n",
            "Iteration 5490, loss = 0.95710276\n",
            "Iteration 5491, loss = 0.95702387\n",
            "Iteration 5492, loss = 0.95694648\n",
            "Iteration 5493, loss = 0.95686891\n",
            "Iteration 5494, loss = 0.95679099\n",
            "Iteration 5495, loss = 0.95671434\n",
            "Iteration 5496, loss = 0.95663724\n",
            "Iteration 5497, loss = 0.95655902\n",
            "Iteration 5498, loss = 0.95648050\n",
            "Iteration 5499, loss = 0.95640340\n",
            "Iteration 5500, loss = 0.95632548\n",
            "Iteration 5501, loss = 0.95624757\n",
            "Iteration 5502, loss = 0.95617040\n",
            "Iteration 5503, loss = 0.95609353\n",
            "Iteration 5504, loss = 0.95601523\n",
            "Iteration 5505, loss = 0.95594003\n",
            "Iteration 5506, loss = 0.95586232\n",
            "Iteration 5507, loss = 0.95578404\n",
            "Iteration 5508, loss = 0.95570774\n",
            "Iteration 5509, loss = 0.95563132\n",
            "Iteration 5510, loss = 0.95555333\n",
            "Iteration 5511, loss = 0.95547738\n",
            "Iteration 5512, loss = 0.95540100\n",
            "Iteration 5513, loss = 0.95532423\n",
            "Iteration 5514, loss = 0.95524711\n",
            "Iteration 5515, loss = 0.95516969\n",
            "Iteration 5516, loss = 0.95509198\n",
            "Iteration 5517, loss = 0.95501541\n",
            "Iteration 5518, loss = 0.95493930\n",
            "Iteration 5519, loss = 0.95486255\n",
            "Iteration 5520, loss = 0.95478607\n",
            "Iteration 5521, loss = 0.95470974\n",
            "Iteration 5522, loss = 0.95463229\n",
            "Iteration 5523, loss = 0.95455611\n",
            "Iteration 5524, loss = 0.95448068\n",
            "Iteration 5525, loss = 0.95440433\n",
            "Iteration 5526, loss = 0.95432793\n",
            "Iteration 5527, loss = 0.95425120\n",
            "Iteration 5528, loss = 0.95417416\n",
            "Iteration 5529, loss = 0.95409700\n",
            "Iteration 5530, loss = 0.95402066\n",
            "Iteration 5531, loss = 0.95394359\n",
            "Iteration 5532, loss = 0.95386750\n",
            "Iteration 5533, loss = 0.95379088\n",
            "Iteration 5534, loss = 0.95371514\n",
            "Iteration 5535, loss = 0.95363901\n",
            "Iteration 5536, loss = 0.95356319\n",
            "Iteration 5537, loss = 0.95348681\n",
            "Iteration 5538, loss = 0.95341062\n",
            "Iteration 5539, loss = 0.95333513\n",
            "Iteration 5540, loss = 0.95325936\n",
            "Iteration 5541, loss = 0.95318340\n",
            "Iteration 5542, loss = 0.95310711\n",
            "Iteration 5543, loss = 0.95303051\n",
            "Iteration 5544, loss = 0.95295412\n",
            "Iteration 5545, loss = 0.95288050\n",
            "Iteration 5546, loss = 0.95280483\n",
            "Iteration 5547, loss = 0.95272876\n",
            "Iteration 5548, loss = 0.95265183\n",
            "Iteration 5549, loss = 0.95257666\n",
            "Iteration 5550, loss = 0.95250184\n",
            "Iteration 5551, loss = 0.95242660\n",
            "Iteration 5552, loss = 0.95235097\n",
            "Iteration 5553, loss = 0.95227499\n",
            "Iteration 5554, loss = 0.95219949\n",
            "Iteration 5555, loss = 0.95212306\n",
            "Iteration 5556, loss = 0.95204727\n",
            "Iteration 5557, loss = 0.95197237\n",
            "Iteration 5558, loss = 0.95189741\n",
            "Iteration 5559, loss = 0.95182130\n",
            "Iteration 5560, loss = 0.95174714\n",
            "Iteration 5561, loss = 0.95167140\n",
            "Iteration 5562, loss = 0.95159469\n",
            "Iteration 5563, loss = 0.95152001\n",
            "Iteration 5564, loss = 0.95144567\n",
            "Iteration 5565, loss = 0.95136988\n",
            "Iteration 5566, loss = 0.95129543\n",
            "Iteration 5567, loss = 0.95122072\n",
            "Iteration 5568, loss = 0.95114563\n",
            "Iteration 5569, loss = 0.95107019\n",
            "Iteration 5570, loss = 0.95099445\n",
            "Iteration 5571, loss = 0.95091844\n",
            "Iteration 5572, loss = 0.95084485\n",
            "Iteration 5573, loss = 0.95076993\n",
            "Iteration 5574, loss = 0.95069448\n",
            "Iteration 5575, loss = 0.95061915\n",
            "Iteration 5576, loss = 0.95054326\n",
            "Iteration 5577, loss = 0.95046962\n",
            "Iteration 5578, loss = 0.95039501\n",
            "Iteration 5579, loss = 0.95032061\n",
            "Iteration 5580, loss = 0.95024584\n",
            "Iteration 5581, loss = 0.95017083\n",
            "Iteration 5582, loss = 0.95009579\n",
            "Iteration 5583, loss = 0.95002122\n",
            "Iteration 5584, loss = 0.94994652\n",
            "Iteration 5585, loss = 0.94987177\n",
            "Iteration 5586, loss = 0.94979688\n",
            "Iteration 5587, loss = 0.94972169\n",
            "Iteration 5588, loss = 0.94964656\n",
            "Iteration 5589, loss = 0.94957293\n",
            "Iteration 5590, loss = 0.94949909\n",
            "Iteration 5591, loss = 0.94942379\n",
            "Iteration 5592, loss = 0.94934884\n",
            "Iteration 5593, loss = 0.94927514\n",
            "Iteration 5594, loss = 0.94920102\n",
            "Iteration 5595, loss = 0.94912653\n",
            "Iteration 5596, loss = 0.94905182\n",
            "Iteration 5597, loss = 0.94897680\n",
            "Iteration 5598, loss = 0.94890435\n",
            "Iteration 5599, loss = 0.94882993\n",
            "Iteration 5600, loss = 0.94875462\n",
            "Iteration 5601, loss = 0.94868071\n",
            "Iteration 5602, loss = 0.94860868\n",
            "Iteration 5603, loss = 0.94853398\n",
            "Iteration 5604, loss = 0.94845929\n",
            "Iteration 5605, loss = 0.94838496\n",
            "Iteration 5606, loss = 0.94831078\n",
            "Iteration 5607, loss = 0.94823705\n",
            "Iteration 5608, loss = 0.94816222\n",
            "Iteration 5609, loss = 0.94808829\n",
            "Iteration 5610, loss = 0.94801393\n",
            "Iteration 5611, loss = 0.94793984\n",
            "Iteration 5612, loss = 0.94786598\n",
            "Iteration 5613, loss = 0.94779205\n",
            "Iteration 5614, loss = 0.94771779\n",
            "Iteration 5615, loss = 0.94764421\n",
            "Iteration 5616, loss = 0.94756997\n",
            "Iteration 5617, loss = 0.94749705\n",
            "Iteration 5618, loss = 0.94742300\n",
            "Iteration 5619, loss = 0.94734841\n",
            "Iteration 5620, loss = 0.94727527\n",
            "Iteration 5621, loss = 0.94720231\n",
            "Iteration 5622, loss = 0.94712893\n",
            "Iteration 5623, loss = 0.94705575\n",
            "Iteration 5624, loss = 0.94698193\n",
            "Iteration 5625, loss = 0.94690791\n",
            "Iteration 5626, loss = 0.94683396\n",
            "Iteration 5627, loss = 0.94676002\n",
            "Iteration 5628, loss = 0.94668578\n",
            "Iteration 5629, loss = 0.94661204\n",
            "Iteration 5630, loss = 0.94653986\n",
            "Iteration 5631, loss = 0.94646757\n",
            "Iteration 5632, loss = 0.94639399\n",
            "Iteration 5633, loss = 0.94631923\n",
            "Iteration 5634, loss = 0.94624420\n",
            "Iteration 5635, loss = 0.94617161\n",
            "Iteration 5636, loss = 0.94609901\n",
            "Iteration 5637, loss = 0.94602605\n",
            "Iteration 5638, loss = 0.94595340\n",
            "Iteration 5639, loss = 0.94588038\n",
            "Iteration 5640, loss = 0.94580698\n",
            "Iteration 5641, loss = 0.94573325\n",
            "Iteration 5642, loss = 0.94565922\n",
            "Iteration 5643, loss = 0.94558499\n",
            "Iteration 5644, loss = 0.94551185\n",
            "Iteration 5645, loss = 0.94543873\n",
            "Iteration 5646, loss = 0.94536440\n",
            "Iteration 5647, loss = 0.94529471\n",
            "Iteration 5648, loss = 0.94522192\n",
            "Iteration 5649, loss = 0.94514606\n",
            "Iteration 5650, loss = 0.94507294\n",
            "Iteration 5651, loss = 0.94500054\n",
            "Iteration 5652, loss = 0.94492806\n",
            "Iteration 5653, loss = 0.94485507\n",
            "Iteration 5654, loss = 0.94478256\n",
            "Iteration 5655, loss = 0.94470966\n",
            "Iteration 5656, loss = 0.94463642\n",
            "Iteration 5657, loss = 0.94456326\n",
            "Iteration 5658, loss = 0.94448975\n",
            "Iteration 5659, loss = 0.94441665\n",
            "Iteration 5660, loss = 0.94434376\n",
            "Iteration 5661, loss = 0.94427084\n",
            "Iteration 5662, loss = 0.94419810\n",
            "Iteration 5663, loss = 0.94412442\n",
            "Iteration 5664, loss = 0.94405184\n",
            "Iteration 5665, loss = 0.94397978\n",
            "Iteration 5666, loss = 0.94390677\n",
            "Iteration 5667, loss = 0.94383412\n",
            "Iteration 5668, loss = 0.94376196\n",
            "Iteration 5669, loss = 0.94369012\n",
            "Iteration 5670, loss = 0.94361793\n",
            "Iteration 5671, loss = 0.94354535\n",
            "Iteration 5672, loss = 0.94347262\n",
            "Iteration 5673, loss = 0.94339999\n",
            "Iteration 5674, loss = 0.94332731\n",
            "Iteration 5675, loss = 0.94325458\n",
            "Iteration 5676, loss = 0.94318239\n",
            "Iteration 5677, loss = 0.94310945\n",
            "Iteration 5678, loss = 0.94303783\n",
            "Iteration 5679, loss = 0.94296518\n",
            "Iteration 5680, loss = 0.94289448\n",
            "Iteration 5681, loss = 0.94282250\n",
            "Iteration 5682, loss = 0.94274912\n",
            "Iteration 5683, loss = 0.94267725\n",
            "Iteration 5684, loss = 0.94260500\n",
            "Iteration 5685, loss = 0.94253242\n",
            "Iteration 5686, loss = 0.94246107\n",
            "Iteration 5687, loss = 0.94239061\n",
            "Iteration 5688, loss = 0.94231756\n",
            "Iteration 5689, loss = 0.94224437\n",
            "Iteration 5690, loss = 0.94217285\n",
            "Iteration 5691, loss = 0.94210142\n",
            "Iteration 5692, loss = 0.94202945\n",
            "Iteration 5693, loss = 0.94195730\n",
            "Iteration 5694, loss = 0.94188518\n",
            "Iteration 5695, loss = 0.94181292\n",
            "Iteration 5696, loss = 0.94174098\n",
            "Iteration 5697, loss = 0.94166970\n",
            "Iteration 5698, loss = 0.94159725\n",
            "Iteration 5699, loss = 0.94152487\n",
            "Iteration 5700, loss = 0.94145341\n",
            "Iteration 5701, loss = 0.94138069\n",
            "Iteration 5702, loss = 0.94130847\n",
            "Iteration 5703, loss = 0.94123628\n",
            "Iteration 5704, loss = 0.94116477\n",
            "Iteration 5705, loss = 0.94109233\n",
            "Iteration 5706, loss = 0.94102167\n",
            "Iteration 5707, loss = 0.94094877\n",
            "Iteration 5708, loss = 0.94087611\n",
            "Iteration 5709, loss = 0.94080547\n",
            "Iteration 5710, loss = 0.94073370\n",
            "Iteration 5711, loss = 0.94066052\n",
            "Iteration 5712, loss = 0.94058881\n",
            "Iteration 5713, loss = 0.94051669\n",
            "Iteration 5714, loss = 0.94044444\n",
            "Iteration 5715, loss = 0.94037229\n",
            "Iteration 5716, loss = 0.94029987\n",
            "Iteration 5717, loss = 0.94022761\n",
            "Iteration 5718, loss = 0.94015578\n",
            "Iteration 5719, loss = 0.94008353\n",
            "Iteration 5720, loss = 0.94001151\n",
            "Iteration 5721, loss = 0.93994039\n",
            "Iteration 5722, loss = 0.93986821\n",
            "Iteration 5723, loss = 0.93979741\n",
            "Iteration 5724, loss = 0.93972639\n",
            "Iteration 5725, loss = 0.93965471\n",
            "Iteration 5726, loss = 0.93958297\n",
            "Iteration 5727, loss = 0.93951088\n",
            "Iteration 5728, loss = 0.93943845\n",
            "Iteration 5729, loss = 0.93936708\n",
            "Iteration 5730, loss = 0.93929555\n",
            "Iteration 5731, loss = 0.93922290\n",
            "Iteration 5732, loss = 0.93915029\n",
            "Iteration 5733, loss = 0.93908032\n",
            "Iteration 5734, loss = 0.93900875\n",
            "Iteration 5735, loss = 0.93893598\n",
            "Iteration 5736, loss = 0.93886447\n",
            "Iteration 5737, loss = 0.93879311\n",
            "Iteration 5738, loss = 0.93872170\n",
            "Iteration 5739, loss = 0.93865007\n",
            "Iteration 5740, loss = 0.93857858\n",
            "Iteration 5741, loss = 0.93850692\n",
            "Iteration 5742, loss = 0.93843506\n",
            "Iteration 5743, loss = 0.93836465\n",
            "Iteration 5744, loss = 0.93829305\n",
            "Iteration 5745, loss = 0.93822327\n",
            "Iteration 5746, loss = 0.93815106\n",
            "Iteration 5747, loss = 0.93807883\n",
            "Iteration 5748, loss = 0.93800819\n",
            "Iteration 5749, loss = 0.93793714\n",
            "Iteration 5750, loss = 0.93786572\n",
            "Iteration 5751, loss = 0.93779398\n",
            "Iteration 5752, loss = 0.93772240\n",
            "Iteration 5753, loss = 0.93765162\n",
            "Iteration 5754, loss = 0.93758038\n",
            "Iteration 5755, loss = 0.93750984\n",
            "Iteration 5756, loss = 0.93743867\n",
            "Iteration 5757, loss = 0.93736843\n",
            "Iteration 5758, loss = 0.93729777\n",
            "Iteration 5759, loss = 0.93722674\n",
            "Iteration 5760, loss = 0.93715538\n",
            "Iteration 5761, loss = 0.93708371\n",
            "Iteration 5762, loss = 0.93701214\n",
            "Iteration 5763, loss = 0.93694177\n",
            "Iteration 5764, loss = 0.93687061\n",
            "Iteration 5765, loss = 0.93680031\n",
            "Iteration 5766, loss = 0.93672938\n",
            "Iteration 5767, loss = 0.93665920\n",
            "Iteration 5768, loss = 0.93658857\n",
            "Iteration 5769, loss = 0.93651755\n",
            "Iteration 5770, loss = 0.93644618\n",
            "Iteration 5771, loss = 0.93637605\n",
            "Iteration 5772, loss = 0.93630489\n",
            "Iteration 5773, loss = 0.93623355\n",
            "Iteration 5774, loss = 0.93616391\n",
            "Iteration 5775, loss = 0.93609281\n",
            "Iteration 5776, loss = 0.93602315\n",
            "Iteration 5777, loss = 0.93595212\n",
            "Iteration 5778, loss = 0.93588139\n",
            "Iteration 5779, loss = 0.93581030\n",
            "Iteration 5780, loss = 0.93573962\n",
            "Iteration 5781, loss = 0.93566888\n",
            "Iteration 5782, loss = 0.93559757\n",
            "Iteration 5783, loss = 0.93552886\n",
            "Iteration 5784, loss = 0.93545786\n",
            "Iteration 5785, loss = 0.93538669\n",
            "Iteration 5786, loss = 0.93531692\n",
            "Iteration 5787, loss = 0.93524578\n",
            "Iteration 5788, loss = 0.93517579\n",
            "Iteration 5789, loss = 0.93510540\n",
            "Iteration 5790, loss = 0.93503464\n",
            "Iteration 5791, loss = 0.93496356\n",
            "Iteration 5792, loss = 0.93489382\n",
            "Iteration 5793, loss = 0.93482431\n",
            "Iteration 5794, loss = 0.93475402\n",
            "Iteration 5795, loss = 0.93468258\n",
            "Iteration 5796, loss = 0.93461253\n",
            "Iteration 5797, loss = 0.93454283\n",
            "Iteration 5798, loss = 0.93447271\n",
            "Iteration 5799, loss = 0.93440221\n",
            "Iteration 5800, loss = 0.93433137\n",
            "Iteration 5801, loss = 0.93426054\n",
            "Iteration 5802, loss = 0.93419130\n",
            "Iteration 5803, loss = 0.93412015\n",
            "Iteration 5804, loss = 0.93405073\n",
            "Iteration 5805, loss = 0.93398030\n",
            "Iteration 5806, loss = 0.93391066\n",
            "Iteration 5807, loss = 0.93384088\n",
            "Iteration 5808, loss = 0.93377072\n",
            "Iteration 5809, loss = 0.93370018\n",
            "Iteration 5810, loss = 0.93363072\n",
            "Iteration 5811, loss = 0.93356026\n",
            "Iteration 5812, loss = 0.93349034\n",
            "Iteration 5813, loss = 0.93341961\n",
            "Iteration 5814, loss = 0.93334987\n",
            "Iteration 5815, loss = 0.93328065\n",
            "Iteration 5816, loss = 0.93321005\n",
            "Iteration 5817, loss = 0.93314085\n",
            "Iteration 5818, loss = 0.93307139\n",
            "Iteration 5819, loss = 0.93300149\n",
            "Iteration 5820, loss = 0.93293120\n",
            "Iteration 5821, loss = 0.93286189\n",
            "Iteration 5822, loss = 0.93278990\n",
            "Iteration 5823, loss = 0.93272122\n",
            "Iteration 5824, loss = 0.93265180\n",
            "Iteration 5825, loss = 0.93258098\n",
            "Iteration 5826, loss = 0.93251081\n",
            "Iteration 5827, loss = 0.93244240\n",
            "Iteration 5828, loss = 0.93237121\n",
            "Iteration 5829, loss = 0.93230225\n",
            "Iteration 5830, loss = 0.93223260\n",
            "Iteration 5831, loss = 0.93216289\n",
            "Iteration 5832, loss = 0.93209284\n",
            "Iteration 5833, loss = 0.93202245\n",
            "Iteration 5834, loss = 0.93195176\n",
            "Iteration 5835, loss = 0.93188311\n",
            "Iteration 5836, loss = 0.93181238\n",
            "Iteration 5837, loss = 0.93174234\n",
            "Iteration 5838, loss = 0.93167264\n",
            "Iteration 5839, loss = 0.93160341\n",
            "Iteration 5840, loss = 0.93153433\n",
            "Iteration 5841, loss = 0.93146424\n",
            "Iteration 5842, loss = 0.93139433\n",
            "Iteration 5843, loss = 0.93132528\n",
            "Iteration 5844, loss = 0.93125513\n",
            "Iteration 5845, loss = 0.93118525\n",
            "Iteration 5846, loss = 0.93111530\n",
            "Iteration 5847, loss = 0.93104650\n",
            "Iteration 5848, loss = 0.93097616\n",
            "Iteration 5849, loss = 0.93090747\n",
            "Iteration 5850, loss = 0.93083810\n",
            "Iteration 5851, loss = 0.93076843\n",
            "Iteration 5852, loss = 0.93069941\n",
            "Iteration 5853, loss = 0.93062996\n",
            "Iteration 5854, loss = 0.93056031\n",
            "Iteration 5855, loss = 0.93049179\n",
            "Iteration 5856, loss = 0.93042213\n",
            "Iteration 5857, loss = 0.93035154\n",
            "Iteration 5858, loss = 0.93028196\n",
            "Iteration 5859, loss = 0.93021281\n",
            "Iteration 5860, loss = 0.93014447\n",
            "Iteration 5861, loss = 0.93007495\n",
            "Iteration 5862, loss = 0.93000559\n",
            "Iteration 5863, loss = 0.92993723\n",
            "Iteration 5864, loss = 0.92986757\n",
            "Iteration 5865, loss = 0.92979813\n",
            "Iteration 5866, loss = 0.92972963\n",
            "Iteration 5867, loss = 0.92966128\n",
            "Iteration 5868, loss = 0.92959184\n",
            "Iteration 5869, loss = 0.92952271\n",
            "Iteration 5870, loss = 0.92945351\n",
            "Iteration 5871, loss = 0.92938468\n",
            "Iteration 5872, loss = 0.92931561\n",
            "Iteration 5873, loss = 0.92924763\n",
            "Iteration 5874, loss = 0.92917879\n",
            "Iteration 5875, loss = 0.92911009\n",
            "Iteration 5876, loss = 0.92904179\n",
            "Iteration 5877, loss = 0.92897340\n",
            "Iteration 5878, loss = 0.92890506\n",
            "Iteration 5879, loss = 0.92883638\n",
            "Iteration 5880, loss = 0.92876802\n",
            "Iteration 5881, loss = 0.92869931\n",
            "Iteration 5882, loss = 0.92863101\n",
            "Iteration 5883, loss = 0.92856183\n",
            "Iteration 5884, loss = 0.92849314\n",
            "Iteration 5885, loss = 0.92842472\n",
            "Iteration 5886, loss = 0.92835606\n",
            "Iteration 5887, loss = 0.92828738\n",
            "Iteration 5888, loss = 0.92822093\n",
            "Iteration 5889, loss = 0.92815187\n",
            "Iteration 5890, loss = 0.92808209\n",
            "Iteration 5891, loss = 0.92801458\n",
            "Iteration 5892, loss = 0.92794507\n",
            "Iteration 5893, loss = 0.92787760\n",
            "Iteration 5894, loss = 0.92780914\n",
            "Iteration 5895, loss = 0.92774114\n",
            "Iteration 5896, loss = 0.92767151\n",
            "Iteration 5897, loss = 0.92760326\n",
            "Iteration 5898, loss = 0.92753573\n",
            "Iteration 5899, loss = 0.92746729\n",
            "Iteration 5900, loss = 0.92739889\n",
            "Iteration 5901, loss = 0.92733052\n",
            "Iteration 5902, loss = 0.92726314\n",
            "Iteration 5903, loss = 0.92719483\n",
            "Iteration 5904, loss = 0.92712760\n",
            "Iteration 5905, loss = 0.92705879\n",
            "Iteration 5906, loss = 0.92699090\n",
            "Iteration 5907, loss = 0.92692348\n",
            "Iteration 5908, loss = 0.92685548\n",
            "Iteration 5909, loss = 0.92678854\n",
            "Iteration 5910, loss = 0.92672018\n",
            "Iteration 5911, loss = 0.92665250\n",
            "Iteration 5912, loss = 0.92658440\n",
            "Iteration 5913, loss = 0.92651592\n",
            "Iteration 5914, loss = 0.92644778\n",
            "Iteration 5915, loss = 0.92637878\n",
            "Iteration 5916, loss = 0.92631254\n",
            "Iteration 5917, loss = 0.92624576\n",
            "Iteration 5918, loss = 0.92617680\n",
            "Iteration 5919, loss = 0.92610807\n",
            "Iteration 5920, loss = 0.92604131\n",
            "Iteration 5921, loss = 0.92597305\n",
            "Iteration 5922, loss = 0.92590594\n",
            "Iteration 5923, loss = 0.92583812\n",
            "Iteration 5924, loss = 0.92577229\n",
            "Iteration 5925, loss = 0.92570372\n",
            "Iteration 5926, loss = 0.92563537\n",
            "Iteration 5927, loss = 0.92557005\n",
            "Iteration 5928, loss = 0.92550148\n",
            "Iteration 5929, loss = 0.92543366\n",
            "Iteration 5930, loss = 0.92536797\n",
            "Iteration 5931, loss = 0.92530228\n",
            "Iteration 5932, loss = 0.92523279\n",
            "Iteration 5933, loss = 0.92516586\n",
            "Iteration 5934, loss = 0.92509899\n",
            "Iteration 5935, loss = 0.92503230\n",
            "Iteration 5936, loss = 0.92496495\n",
            "Iteration 5937, loss = 0.92489696\n",
            "Iteration 5938, loss = 0.92483062\n",
            "Iteration 5939, loss = 0.92476310\n",
            "Iteration 5940, loss = 0.92469568\n",
            "Iteration 5941, loss = 0.92462817\n",
            "Iteration 5942, loss = 0.92456093\n",
            "Iteration 5943, loss = 0.92449300\n",
            "Iteration 5944, loss = 0.92442663\n",
            "Iteration 5945, loss = 0.92436062\n",
            "Iteration 5946, loss = 0.92429280\n",
            "Iteration 5947, loss = 0.92422561\n",
            "Iteration 5948, loss = 0.92415986\n",
            "Iteration 5949, loss = 0.92409248\n",
            "Iteration 5950, loss = 0.92402530\n",
            "Iteration 5951, loss = 0.92395840\n",
            "Iteration 5952, loss = 0.92389186\n",
            "Iteration 5953, loss = 0.92382405\n",
            "Iteration 5954, loss = 0.92375665\n",
            "Iteration 5955, loss = 0.92368988\n",
            "Iteration 5956, loss = 0.92362262\n",
            "Iteration 5957, loss = 0.92355500\n",
            "Iteration 5958, loss = 0.92349005\n",
            "Iteration 5959, loss = 0.92342267\n",
            "Iteration 5960, loss = 0.92335577\n",
            "Iteration 5961, loss = 0.92329037\n",
            "Iteration 5962, loss = 0.92322452\n",
            "Iteration 5963, loss = 0.92315811\n",
            "Iteration 5964, loss = 0.92309139\n",
            "Iteration 5965, loss = 0.92302468\n",
            "Iteration 5966, loss = 0.92295884\n",
            "Iteration 5967, loss = 0.92289271\n",
            "Iteration 5968, loss = 0.92282631\n",
            "Iteration 5969, loss = 0.92275990\n",
            "Iteration 5970, loss = 0.92269283\n",
            "Iteration 5971, loss = 0.92262727\n",
            "Iteration 5972, loss = 0.92256041\n",
            "Iteration 5973, loss = 0.92249465\n",
            "Iteration 5974, loss = 0.92242800\n",
            "Iteration 5975, loss = 0.92236197\n",
            "Iteration 5976, loss = 0.92229633\n",
            "Iteration 5977, loss = 0.92223024\n",
            "Iteration 5978, loss = 0.92216390\n",
            "Iteration 5979, loss = 0.92209720\n",
            "Iteration 5980, loss = 0.92203362\n",
            "Iteration 5981, loss = 0.92196778\n",
            "Iteration 5982, loss = 0.92189880\n",
            "Iteration 5983, loss = 0.92183378\n",
            "Iteration 5984, loss = 0.92177089\n",
            "Iteration 5985, loss = 0.92170508\n",
            "Iteration 5986, loss = 0.92163786\n",
            "Iteration 5987, loss = 0.92157285\n",
            "Iteration 5988, loss = 0.92150742\n",
            "Iteration 5989, loss = 0.92144154\n",
            "Iteration 5990, loss = 0.92137606\n",
            "Iteration 5991, loss = 0.92131009\n",
            "Iteration 5992, loss = 0.92124368\n",
            "Iteration 5993, loss = 0.92117688\n",
            "Iteration 5994, loss = 0.92111102\n",
            "Iteration 5995, loss = 0.92104677\n",
            "Iteration 5996, loss = 0.92098174\n",
            "Iteration 5997, loss = 0.92091471\n",
            "Iteration 5998, loss = 0.92084987\n",
            "Iteration 5999, loss = 0.92078525\n",
            "Iteration 6000, loss = 0.92071969\n",
            "Iteration 6001, loss = 0.92065373\n",
            "Iteration 6002, loss = 0.92058885\n",
            "Iteration 6003, loss = 0.92052352\n",
            "Iteration 6004, loss = 0.92045857\n",
            "Iteration 6005, loss = 0.92039276\n",
            "Iteration 6006, loss = 0.92032718\n",
            "Iteration 6007, loss = 0.92026154\n",
            "Iteration 6008, loss = 0.92019549\n",
            "Iteration 6009, loss = 0.92013066\n",
            "Iteration 6010, loss = 0.92006534\n",
            "Iteration 6011, loss = 0.91999868\n",
            "Iteration 6012, loss = 0.91993349\n",
            "Iteration 6013, loss = 0.91986880\n",
            "Iteration 6014, loss = 0.91980380\n",
            "Iteration 6015, loss = 0.91973827\n",
            "Iteration 6016, loss = 0.91967297\n",
            "Iteration 6017, loss = 0.91960873\n",
            "Iteration 6018, loss = 0.91954336\n",
            "Iteration 6019, loss = 0.91947912\n",
            "Iteration 6020, loss = 0.91941361\n",
            "Iteration 6021, loss = 0.91934836\n",
            "Iteration 6022, loss = 0.91928453\n",
            "Iteration 6023, loss = 0.91921954\n",
            "Iteration 6024, loss = 0.91915308\n",
            "Iteration 6025, loss = 0.91908807\n",
            "Iteration 6026, loss = 0.91902446\n",
            "Iteration 6027, loss = 0.91895929\n",
            "Iteration 6028, loss = 0.91889388\n",
            "Iteration 6029, loss = 0.91882858\n",
            "Iteration 6030, loss = 0.91876621\n",
            "Iteration 6031, loss = 0.91870157\n",
            "Iteration 6032, loss = 0.91863548\n",
            "Iteration 6033, loss = 0.91857157\n",
            "Iteration 6034, loss = 0.91850735\n",
            "Iteration 6035, loss = 0.91844471\n",
            "Iteration 6036, loss = 0.91837885\n",
            "Iteration 6037, loss = 0.91831402\n",
            "Iteration 6038, loss = 0.91824953\n",
            "Iteration 6039, loss = 0.91818369\n",
            "Iteration 6040, loss = 0.91811921\n",
            "Iteration 6041, loss = 0.91805496\n",
            "Iteration 6042, loss = 0.91798992\n",
            "Iteration 6043, loss = 0.91792587\n",
            "Iteration 6044, loss = 0.91786072\n",
            "Iteration 6045, loss = 0.91779555\n",
            "Iteration 6046, loss = 0.91773212\n",
            "Iteration 6047, loss = 0.91766922\n",
            "Iteration 6048, loss = 0.91760514\n",
            "Iteration 6049, loss = 0.91753876\n",
            "Iteration 6050, loss = 0.91747386\n",
            "Iteration 6051, loss = 0.91741038\n",
            "Iteration 6052, loss = 0.91734751\n",
            "Iteration 6053, loss = 0.91728233\n",
            "Iteration 6054, loss = 0.91721786\n",
            "Iteration 6055, loss = 0.91715337\n",
            "Iteration 6056, loss = 0.91708844\n",
            "Iteration 6057, loss = 0.91702417\n",
            "Iteration 6058, loss = 0.91695981\n",
            "Iteration 6059, loss = 0.91689692\n",
            "Iteration 6060, loss = 0.91683320\n",
            "Iteration 6061, loss = 0.91676756\n",
            "Iteration 6062, loss = 0.91670477\n",
            "Iteration 6063, loss = 0.91663984\n",
            "Iteration 6064, loss = 0.91657705\n",
            "Iteration 6065, loss = 0.91651276\n",
            "Iteration 6066, loss = 0.91644885\n",
            "Iteration 6067, loss = 0.91638421\n",
            "Iteration 6068, loss = 0.91632195\n",
            "Iteration 6069, loss = 0.91625764\n",
            "Iteration 6070, loss = 0.91619329\n",
            "Iteration 6071, loss = 0.91612938\n",
            "Iteration 6072, loss = 0.91606497\n",
            "Iteration 6073, loss = 0.91600182\n",
            "Iteration 6074, loss = 0.91593765\n",
            "Iteration 6075, loss = 0.91587396\n",
            "Iteration 6076, loss = 0.91581026\n",
            "Iteration 6077, loss = 0.91574771\n",
            "Iteration 6078, loss = 0.91568258\n",
            "Iteration 6079, loss = 0.91561826\n",
            "Iteration 6080, loss = 0.91555662\n",
            "Iteration 6081, loss = 0.91549317\n",
            "Iteration 6082, loss = 0.91542859\n",
            "Iteration 6083, loss = 0.91536520\n",
            "Iteration 6084, loss = 0.91530197\n",
            "Iteration 6085, loss = 0.91523825\n",
            "Iteration 6086, loss = 0.91517410\n",
            "Iteration 6087, loss = 0.91511049\n",
            "Iteration 6088, loss = 0.91504715\n",
            "Iteration 6089, loss = 0.91498380\n",
            "Iteration 6090, loss = 0.91491905\n",
            "Iteration 6091, loss = 0.91485564\n",
            "Iteration 6092, loss = 0.91479470\n",
            "Iteration 6093, loss = 0.91473091\n",
            "Iteration 6094, loss = 0.91466780\n",
            "Iteration 6095, loss = 0.91460369\n",
            "Iteration 6096, loss = 0.91454116\n",
            "Iteration 6097, loss = 0.91447748\n",
            "Iteration 6098, loss = 0.91441393\n",
            "Iteration 6099, loss = 0.91434994\n",
            "Iteration 6100, loss = 0.91428612\n",
            "Iteration 6101, loss = 0.91422245\n",
            "Iteration 6102, loss = 0.91415885\n",
            "Iteration 6103, loss = 0.91409780\n",
            "Iteration 6104, loss = 0.91403351\n",
            "Iteration 6105, loss = 0.91397103\n",
            "Iteration 6106, loss = 0.91390763\n",
            "Iteration 6107, loss = 0.91384478\n",
            "Iteration 6108, loss = 0.91378229\n",
            "Iteration 6109, loss = 0.91371884\n",
            "Iteration 6110, loss = 0.91365481\n",
            "Iteration 6111, loss = 0.91359411\n",
            "Iteration 6112, loss = 0.91352928\n",
            "Iteration 6113, loss = 0.91346597\n",
            "Iteration 6114, loss = 0.91340500\n",
            "Iteration 6115, loss = 0.91334102\n",
            "Iteration 6116, loss = 0.91327772\n",
            "Iteration 6117, loss = 0.91321482\n",
            "Iteration 6118, loss = 0.91315345\n",
            "Iteration 6119, loss = 0.91309039\n",
            "Iteration 6120, loss = 0.91302542\n",
            "Iteration 6121, loss = 0.91296227\n",
            "Iteration 6122, loss = 0.91290029\n",
            "Iteration 6123, loss = 0.91283774\n",
            "Iteration 6124, loss = 0.91277542\n",
            "Iteration 6125, loss = 0.91271202\n",
            "Iteration 6126, loss = 0.91264962\n",
            "Iteration 6127, loss = 0.91258672\n",
            "Iteration 6128, loss = 0.91252447\n",
            "Iteration 6129, loss = 0.91246011\n",
            "Iteration 6130, loss = 0.91239826\n",
            "Iteration 6131, loss = 0.91233611\n",
            "Iteration 6132, loss = 0.91227326\n",
            "Iteration 6133, loss = 0.91221088\n",
            "Iteration 6134, loss = 0.91214871\n",
            "Iteration 6135, loss = 0.91208523\n",
            "Iteration 6136, loss = 0.91202300\n",
            "Iteration 6137, loss = 0.91196067\n",
            "Iteration 6138, loss = 0.91189782\n",
            "Iteration 6139, loss = 0.91183450\n",
            "Iteration 6140, loss = 0.91177134\n",
            "Iteration 6141, loss = 0.91170913\n",
            "Iteration 6142, loss = 0.91164732\n",
            "Iteration 6143, loss = 0.91158490\n",
            "Iteration 6144, loss = 0.91152217\n",
            "Iteration 6145, loss = 0.91145913\n",
            "Iteration 6146, loss = 0.91139756\n",
            "Iteration 6147, loss = 0.91133574\n",
            "Iteration 6148, loss = 0.91127390\n",
            "Iteration 6149, loss = 0.91121194\n",
            "Iteration 6150, loss = 0.91114982\n",
            "Iteration 6151, loss = 0.91108717\n",
            "Iteration 6152, loss = 0.91102405\n",
            "Iteration 6153, loss = 0.91096172\n",
            "Iteration 6154, loss = 0.91089950\n",
            "Iteration 6155, loss = 0.91083679\n",
            "Iteration 6156, loss = 0.91077784\n",
            "Iteration 6157, loss = 0.91071568\n",
            "Iteration 6158, loss = 0.91064916\n",
            "Iteration 6159, loss = 0.91058824\n",
            "Iteration 6160, loss = 0.91052672\n",
            "Iteration 6161, loss = 0.91046403\n",
            "Iteration 6162, loss = 0.91040228\n",
            "Iteration 6163, loss = 0.91033996\n",
            "Iteration 6164, loss = 0.91027816\n",
            "Iteration 6165, loss = 0.91021434\n",
            "Iteration 6166, loss = 0.91015134\n",
            "Iteration 6167, loss = 0.91009010\n",
            "Iteration 6168, loss = 0.91002959\n",
            "Iteration 6169, loss = 0.90996743\n",
            "Iteration 6170, loss = 0.90990406\n",
            "Iteration 6171, loss = 0.90984204\n",
            "Iteration 6172, loss = 0.90978084\n",
            "Iteration 6173, loss = 0.90972022\n",
            "Iteration 6174, loss = 0.90965781\n",
            "Iteration 6175, loss = 0.90959558\n",
            "Iteration 6176, loss = 0.90953285\n",
            "Iteration 6177, loss = 0.90947137\n",
            "Iteration 6178, loss = 0.90940902\n",
            "Iteration 6179, loss = 0.90934887\n",
            "Iteration 6180, loss = 0.90928766\n",
            "Iteration 6181, loss = 0.90922492\n",
            "Iteration 6182, loss = 0.90916071\n",
            "Iteration 6183, loss = 0.90909978\n",
            "Iteration 6184, loss = 0.90903956\n",
            "Iteration 6185, loss = 0.90897796\n",
            "Iteration 6186, loss = 0.90891582\n",
            "Iteration 6187, loss = 0.90885460\n",
            "Iteration 6188, loss = 0.90879316\n",
            "Iteration 6189, loss = 0.90873077\n",
            "Iteration 6190, loss = 0.90866919\n",
            "Iteration 6191, loss = 0.90860699\n",
            "Iteration 6192, loss = 0.90854505\n",
            "Iteration 6193, loss = 0.90848286\n",
            "Iteration 6194, loss = 0.90842204\n",
            "Iteration 6195, loss = 0.90835996\n",
            "Iteration 6196, loss = 0.90829807\n",
            "Iteration 6197, loss = 0.90823745\n",
            "Iteration 6198, loss = 0.90817663\n",
            "Iteration 6199, loss = 0.90811485\n",
            "Iteration 6200, loss = 0.90805327\n",
            "Iteration 6201, loss = 0.90799290\n",
            "Iteration 6202, loss = 0.90793144\n",
            "Iteration 6203, loss = 0.90787025\n",
            "Iteration 6204, loss = 0.90780868\n",
            "Iteration 6205, loss = 0.90774656\n",
            "Iteration 6206, loss = 0.90768496\n",
            "Iteration 6207, loss = 0.90762226\n",
            "Iteration 6208, loss = 0.90756097\n",
            "Iteration 6209, loss = 0.90749983\n",
            "Iteration 6210, loss = 0.90743786\n",
            "Iteration 6211, loss = 0.90737639\n",
            "Iteration 6212, loss = 0.90731519\n",
            "Iteration 6213, loss = 0.90725416\n",
            "Iteration 6214, loss = 0.90719206\n",
            "Iteration 6215, loss = 0.90713213\n",
            "Iteration 6216, loss = 0.90707072\n",
            "Iteration 6217, loss = 0.90700928\n",
            "Iteration 6218, loss = 0.90694833\n",
            "Iteration 6219, loss = 0.90688720\n",
            "Iteration 6220, loss = 0.90682608\n",
            "Iteration 6221, loss = 0.90676518\n",
            "Iteration 6222, loss = 0.90670391\n",
            "Iteration 6223, loss = 0.90664346\n",
            "Iteration 6224, loss = 0.90658358\n",
            "Iteration 6225, loss = 0.90652194\n",
            "Iteration 6226, loss = 0.90645982\n",
            "Iteration 6227, loss = 0.90640063\n",
            "Iteration 6228, loss = 0.90633818\n",
            "Iteration 6229, loss = 0.90627622\n",
            "Iteration 6230, loss = 0.90621587\n",
            "Iteration 6231, loss = 0.90615420\n",
            "Iteration 6232, loss = 0.90609349\n",
            "Iteration 6233, loss = 0.90603463\n",
            "Iteration 6234, loss = 0.90597158\n",
            "Iteration 6235, loss = 0.90591056\n",
            "Iteration 6236, loss = 0.90585209\n",
            "Iteration 6237, loss = 0.90578845\n",
            "Iteration 6238, loss = 0.90572838\n",
            "Iteration 6239, loss = 0.90566824\n",
            "Iteration 6240, loss = 0.90560692\n",
            "Iteration 6241, loss = 0.90554624\n",
            "Iteration 6242, loss = 0.90548512\n",
            "Iteration 6243, loss = 0.90542342\n",
            "Iteration 6244, loss = 0.90536471\n",
            "Iteration 6245, loss = 0.90530355\n",
            "Iteration 6246, loss = 0.90523975\n",
            "Iteration 6247, loss = 0.90517916\n",
            "Iteration 6248, loss = 0.90511886\n",
            "Iteration 6249, loss = 0.90505844\n",
            "Iteration 6250, loss = 0.90499886\n",
            "Iteration 6251, loss = 0.90493593\n",
            "Iteration 6252, loss = 0.90487626\n",
            "Iteration 6253, loss = 0.90481566\n",
            "Iteration 6254, loss = 0.90475531\n",
            "Iteration 6255, loss = 0.90469383\n",
            "Iteration 6256, loss = 0.90463407\n",
            "Iteration 6257, loss = 0.90457296\n",
            "Iteration 6258, loss = 0.90451223\n",
            "Iteration 6259, loss = 0.90445166\n",
            "Iteration 6260, loss = 0.90439271\n",
            "Iteration 6261, loss = 0.90433221\n",
            "Iteration 6262, loss = 0.90427019\n",
            "Iteration 6263, loss = 0.90421028\n",
            "Iteration 6264, loss = 0.90414982\n",
            "Iteration 6265, loss = 0.90408952\n",
            "Iteration 6266, loss = 0.90403026\n",
            "Iteration 6267, loss = 0.90396840\n",
            "Iteration 6268, loss = 0.90390668\n",
            "Iteration 6269, loss = 0.90384715\n",
            "Iteration 6270, loss = 0.90378761\n",
            "Iteration 6271, loss = 0.90372672\n",
            "Iteration 6272, loss = 0.90366621\n",
            "Iteration 6273, loss = 0.90360521\n",
            "Iteration 6274, loss = 0.90354492\n",
            "Iteration 6275, loss = 0.90348398\n",
            "Iteration 6276, loss = 0.90342493\n",
            "Iteration 6277, loss = 0.90336523\n",
            "Iteration 6278, loss = 0.90330374\n",
            "Iteration 6279, loss = 0.90324411\n",
            "Iteration 6280, loss = 0.90318380\n",
            "Iteration 6281, loss = 0.90312440\n",
            "Iteration 6282, loss = 0.90306571\n",
            "Iteration 6283, loss = 0.90300529\n",
            "Iteration 6284, loss = 0.90294586\n",
            "Iteration 6285, loss = 0.90288582\n",
            "Iteration 6286, loss = 0.90282591\n",
            "Iteration 6287, loss = 0.90276429\n",
            "Iteration 6288, loss = 0.90270469\n",
            "Iteration 6289, loss = 0.90264421\n",
            "Iteration 6290, loss = 0.90258369\n",
            "Iteration 6291, loss = 0.90252319\n",
            "Iteration 6292, loss = 0.90246436\n",
            "Iteration 6293, loss = 0.90240327\n",
            "Iteration 6294, loss = 0.90234452\n",
            "Iteration 6295, loss = 0.90228261\n",
            "Iteration 6296, loss = 0.90222245\n",
            "Iteration 6297, loss = 0.90216428\n",
            "Iteration 6298, loss = 0.90210422\n",
            "Iteration 6299, loss = 0.90204416\n",
            "Iteration 6300, loss = 0.90198406\n",
            "Iteration 6301, loss = 0.90192387\n",
            "Iteration 6302, loss = 0.90186415\n",
            "Iteration 6303, loss = 0.90180465\n",
            "Iteration 6304, loss = 0.90174718\n",
            "Iteration 6305, loss = 0.90168806\n",
            "Iteration 6306, loss = 0.90162609\n",
            "Iteration 6307, loss = 0.90156717\n",
            "Iteration 6308, loss = 0.90150841\n",
            "Iteration 6309, loss = 0.90144758\n",
            "Iteration 6310, loss = 0.90138924\n",
            "Iteration 6311, loss = 0.90133000\n",
            "Iteration 6312, loss = 0.90127050\n",
            "Iteration 6313, loss = 0.90121041\n",
            "Iteration 6314, loss = 0.90115130\n",
            "Iteration 6315, loss = 0.90109231\n",
            "Iteration 6316, loss = 0.90103419\n",
            "Iteration 6317, loss = 0.90097278\n",
            "Iteration 6318, loss = 0.90091451\n",
            "Iteration 6319, loss = 0.90085699\n",
            "Iteration 6320, loss = 0.90079623\n",
            "Iteration 6321, loss = 0.90073848\n",
            "Iteration 6322, loss = 0.90067818\n",
            "Iteration 6323, loss = 0.90061887\n",
            "Iteration 6324, loss = 0.90056128\n",
            "Iteration 6325, loss = 0.90050283\n",
            "Iteration 6326, loss = 0.90044406\n",
            "Iteration 6327, loss = 0.90038459\n",
            "Iteration 6328, loss = 0.90032580\n",
            "Iteration 6329, loss = 0.90026665\n",
            "Iteration 6330, loss = 0.90020943\n",
            "Iteration 6331, loss = 0.90015092\n",
            "Iteration 6332, loss = 0.90009204\n",
            "Iteration 6333, loss = 0.90003274\n",
            "Iteration 6334, loss = 0.89997418\n",
            "Iteration 6335, loss = 0.89991610\n",
            "Iteration 6336, loss = 0.89985757\n",
            "Iteration 6337, loss = 0.89979910\n",
            "Iteration 6338, loss = 0.89974029\n",
            "Iteration 6339, loss = 0.89968141\n",
            "Iteration 6340, loss = 0.89962251\n",
            "Iteration 6341, loss = 0.89956432\n",
            "Iteration 6342, loss = 0.89950587\n",
            "Iteration 6343, loss = 0.89944675\n",
            "Iteration 6344, loss = 0.89938937\n",
            "Iteration 6345, loss = 0.89933164\n",
            "Iteration 6346, loss = 0.89927217\n",
            "Iteration 6347, loss = 0.89921424\n",
            "Iteration 6348, loss = 0.89915636\n",
            "Iteration 6349, loss = 0.89909674\n",
            "Iteration 6350, loss = 0.89903915\n",
            "Iteration 6351, loss = 0.89898025\n",
            "Iteration 6352, loss = 0.89892102\n",
            "Iteration 6353, loss = 0.89886459\n",
            "Iteration 6354, loss = 0.89880491\n",
            "Iteration 6355, loss = 0.89874704\n",
            "Iteration 6356, loss = 0.89868966\n",
            "Iteration 6357, loss = 0.89863130\n",
            "Iteration 6358, loss = 0.89857315\n",
            "Iteration 6359, loss = 0.89851575\n",
            "Iteration 6360, loss = 0.89845733\n",
            "Iteration 6361, loss = 0.89839875\n",
            "Iteration 6362, loss = 0.89834018\n",
            "Iteration 6363, loss = 0.89828081\n",
            "Iteration 6364, loss = 0.89822382\n",
            "Iteration 6365, loss = 0.89816487\n",
            "Iteration 6366, loss = 0.89810713\n",
            "Iteration 6367, loss = 0.89805058\n",
            "Iteration 6368, loss = 0.89799161\n",
            "Iteration 6369, loss = 0.89793316\n",
            "Iteration 6370, loss = 0.89787494\n",
            "Iteration 6371, loss = 0.89781842\n",
            "Iteration 6372, loss = 0.89775827\n",
            "Iteration 6373, loss = 0.89770019\n",
            "Iteration 6374, loss = 0.89764258\n",
            "Iteration 6375, loss = 0.89758410\n",
            "Iteration 6376, loss = 0.89752487\n",
            "Iteration 6377, loss = 0.89746629\n",
            "Iteration 6378, loss = 0.89740916\n",
            "Iteration 6379, loss = 0.89735167\n",
            "Iteration 6380, loss = 0.89729274\n",
            "Iteration 6381, loss = 0.89723405\n",
            "Iteration 6382, loss = 0.89717831\n",
            "Iteration 6383, loss = 0.89711921\n",
            "Iteration 6384, loss = 0.89706229\n",
            "Iteration 6385, loss = 0.89700512\n",
            "Iteration 6386, loss = 0.89694701\n",
            "Iteration 6387, loss = 0.89688987\n",
            "Iteration 6388, loss = 0.89683189\n",
            "Iteration 6389, loss = 0.89677360\n",
            "Iteration 6390, loss = 0.89671381\n",
            "Iteration 6391, loss = 0.89665555\n",
            "Iteration 6392, loss = 0.89659647\n",
            "Iteration 6393, loss = 0.89653910\n",
            "Iteration 6394, loss = 0.89648332\n",
            "Iteration 6395, loss = 0.89642616\n",
            "Iteration 6396, loss = 0.89636708\n",
            "Iteration 6397, loss = 0.89630867\n",
            "Iteration 6398, loss = 0.89625038\n",
            "Iteration 6399, loss = 0.89619357\n",
            "Iteration 6400, loss = 0.89613672\n",
            "Iteration 6401, loss = 0.89607893\n",
            "Iteration 6402, loss = 0.89602055\n",
            "Iteration 6403, loss = 0.89596273\n",
            "Iteration 6404, loss = 0.89590495\n",
            "Iteration 6405, loss = 0.89584632\n",
            "Iteration 6406, loss = 0.89578904\n",
            "Iteration 6407, loss = 0.89573032\n",
            "Iteration 6408, loss = 0.89567402\n",
            "Iteration 6409, loss = 0.89561683\n",
            "Iteration 6410, loss = 0.89555894\n",
            "Iteration 6411, loss = 0.89550212\n",
            "Iteration 6412, loss = 0.89544437\n",
            "Iteration 6413, loss = 0.89538581\n",
            "Iteration 6414, loss = 0.89532783\n",
            "Iteration 6415, loss = 0.89527026\n",
            "Iteration 6416, loss = 0.89521292\n",
            "Iteration 6417, loss = 0.89515601\n",
            "Iteration 6418, loss = 0.89509835\n",
            "Iteration 6419, loss = 0.89504138\n",
            "Iteration 6420, loss = 0.89498433\n",
            "Iteration 6421, loss = 0.89492664\n",
            "Iteration 6422, loss = 0.89486908\n",
            "Iteration 6423, loss = 0.89481086\n",
            "Iteration 6424, loss = 0.89475331\n",
            "Iteration 6425, loss = 0.89469770\n",
            "Iteration 6426, loss = 0.89464144\n",
            "Iteration 6427, loss = 0.89458332\n",
            "Iteration 6428, loss = 0.89452464\n",
            "Iteration 6429, loss = 0.89446919\n",
            "Iteration 6430, loss = 0.89441296\n",
            "Iteration 6431, loss = 0.89435771\n",
            "Iteration 6432, loss = 0.89429928\n",
            "Iteration 6433, loss = 0.89424213\n",
            "Iteration 6434, loss = 0.89418483\n",
            "Iteration 6435, loss = 0.89412684\n",
            "Iteration 6436, loss = 0.89406901\n",
            "Iteration 6437, loss = 0.89401070\n",
            "Iteration 6438, loss = 0.89395301\n",
            "Iteration 6439, loss = 0.89389779\n",
            "Iteration 6440, loss = 0.89384119\n",
            "Iteration 6441, loss = 0.89378380\n",
            "Iteration 6442, loss = 0.89372523\n",
            "Iteration 6443, loss = 0.89366855\n",
            "Iteration 6444, loss = 0.89361263\n",
            "Iteration 6445, loss = 0.89355472\n",
            "Iteration 6446, loss = 0.89349747\n",
            "Iteration 6447, loss = 0.89344091\n",
            "Iteration 6448, loss = 0.89338337\n",
            "Iteration 6449, loss = 0.89332676\n",
            "Iteration 6450, loss = 0.89326981\n",
            "Iteration 6451, loss = 0.89321422\n",
            "Iteration 6452, loss = 0.89315568\n",
            "Iteration 6453, loss = 0.89309899\n",
            "Iteration 6454, loss = 0.89304229\n",
            "Iteration 6455, loss = 0.89298481\n",
            "Iteration 6456, loss = 0.89292873\n",
            "Iteration 6457, loss = 0.89287272\n",
            "Iteration 6458, loss = 0.89281545\n",
            "Iteration 6459, loss = 0.89276016\n",
            "Iteration 6460, loss = 0.89270235\n",
            "Iteration 6461, loss = 0.89264606\n",
            "Iteration 6462, loss = 0.89258927\n",
            "Iteration 6463, loss = 0.89253205\n",
            "Iteration 6464, loss = 0.89247535\n",
            "Iteration 6465, loss = 0.89241766\n",
            "Iteration 6466, loss = 0.89236177\n",
            "Iteration 6467, loss = 0.89230566\n",
            "Iteration 6468, loss = 0.89224949\n",
            "Iteration 6469, loss = 0.89219243\n",
            "Iteration 6470, loss = 0.89213617\n",
            "Iteration 6471, loss = 0.89207956\n",
            "Iteration 6472, loss = 0.89202425\n",
            "Iteration 6473, loss = 0.89196740\n",
            "Iteration 6474, loss = 0.89191056\n",
            "Iteration 6475, loss = 0.89185357\n",
            "Iteration 6476, loss = 0.89179819\n",
            "Iteration 6477, loss = 0.89174073\n",
            "Iteration 6478, loss = 0.89168396\n",
            "Iteration 6479, loss = 0.89162937\n",
            "Iteration 6480, loss = 0.89157095\n",
            "Iteration 6481, loss = 0.89151500\n",
            "Iteration 6482, loss = 0.89145964\n",
            "Iteration 6483, loss = 0.89140344\n",
            "Iteration 6484, loss = 0.89134679\n",
            "Iteration 6485, loss = 0.89128894\n",
            "Iteration 6486, loss = 0.89123739\n",
            "Iteration 6487, loss = 0.89119147\n",
            "Iteration 6488, loss = 0.89114335\n",
            "Iteration 6489, loss = 0.89109117\n",
            "Iteration 6490, loss = 0.89103881\n",
            "Iteration 6491, loss = 0.89098661\n",
            "Iteration 6492, loss = 0.89093471\n",
            "Iteration 6493, loss = 0.89088098\n",
            "Iteration 6494, loss = 0.89082700\n",
            "Iteration 6495, loss = 0.89077275\n",
            "Iteration 6496, loss = 0.89071564\n",
            "Iteration 6497, loss = 0.89066178\n",
            "Iteration 6498, loss = 0.89061264\n",
            "Iteration 6499, loss = 0.89056227\n",
            "Iteration 6500, loss = 0.89051223\n",
            "Iteration 6501, loss = 0.89046093\n",
            "Iteration 6502, loss = 0.89040845\n",
            "Iteration 6503, loss = 0.89035961\n",
            "Iteration 6504, loss = 0.89030613\n",
            "Iteration 6505, loss = 0.89025300\n",
            "Iteration 6506, loss = 0.89020073\n",
            "Iteration 6507, loss = 0.89014829\n",
            "Iteration 6508, loss = 0.89009478\n",
            "Iteration 6509, loss = 0.89004461\n",
            "Iteration 6510, loss = 0.88999233\n",
            "Iteration 6511, loss = 0.88993564\n",
            "Iteration 6512, loss = 0.88988128\n",
            "Iteration 6513, loss = 0.88983468\n",
            "Iteration 6514, loss = 0.88978063\n",
            "Iteration 6515, loss = 0.88972500\n",
            "Iteration 6516, loss = 0.88967698\n",
            "Iteration 6517, loss = 0.88962878\n",
            "Iteration 6518, loss = 0.88957337\n",
            "Iteration 6519, loss = 0.88951790\n",
            "Iteration 6520, loss = 0.88946814\n",
            "Iteration 6521, loss = 0.88941856\n",
            "Iteration 6522, loss = 0.88936462\n",
            "Iteration 6523, loss = 0.88930840\n",
            "Iteration 6524, loss = 0.88926113\n",
            "Iteration 6525, loss = 0.88921228\n",
            "Iteration 6526, loss = 0.88915582\n",
            "Iteration 6527, loss = 0.88910172\n",
            "Iteration 6528, loss = 0.88905476\n",
            "Iteration 6529, loss = 0.88900193\n",
            "Iteration 6530, loss = 0.88895016\n",
            "Iteration 6531, loss = 0.88889598\n",
            "Iteration 6532, loss = 0.88884607\n",
            "Iteration 6533, loss = 0.88879267\n",
            "Iteration 6534, loss = 0.88874087\n",
            "Iteration 6535, loss = 0.88868983\n",
            "Iteration 6536, loss = 0.88863672\n",
            "Iteration 6537, loss = 0.88858495\n",
            "Iteration 6538, loss = 0.88853163\n",
            "Iteration 6539, loss = 0.88848146\n",
            "Iteration 6540, loss = 0.88843053\n",
            "Iteration 6541, loss = 0.88838264\n",
            "Iteration 6542, loss = 0.88832924\n",
            "Iteration 6543, loss = 0.88827499\n",
            "Iteration 6544, loss = 0.88822384\n",
            "Iteration 6545, loss = 0.88817337\n",
            "Iteration 6546, loss = 0.88812195\n",
            "Iteration 6547, loss = 0.88807101\n",
            "Iteration 6548, loss = 0.88801964\n",
            "Iteration 6549, loss = 0.88796814\n",
            "Iteration 6550, loss = 0.88791796\n",
            "Iteration 6551, loss = 0.88786384\n",
            "Iteration 6552, loss = 0.88781445\n",
            "Iteration 6553, loss = 0.88776382\n",
            "Iteration 6554, loss = 0.88771101\n",
            "Iteration 6555, loss = 0.88765917\n",
            "Iteration 6556, loss = 0.88760885\n",
            "Iteration 6557, loss = 0.88755789\n",
            "Iteration 6558, loss = 0.88750661\n",
            "Iteration 6559, loss = 0.88745423\n",
            "Iteration 6560, loss = 0.88740320\n",
            "Iteration 6561, loss = 0.88735149\n",
            "Iteration 6562, loss = 0.88730005\n",
            "Iteration 6563, loss = 0.88724951\n",
            "Iteration 6564, loss = 0.88719807\n",
            "Iteration 6565, loss = 0.88714673\n",
            "Iteration 6566, loss = 0.88709773\n",
            "Iteration 6567, loss = 0.88704544\n",
            "Iteration 6568, loss = 0.88699329\n",
            "Iteration 6569, loss = 0.88694417\n",
            "Iteration 6570, loss = 0.88689251\n",
            "Iteration 6571, loss = 0.88684183\n",
            "Iteration 6572, loss = 0.88678967\n",
            "Iteration 6573, loss = 0.88673700\n",
            "Iteration 6574, loss = 0.88668900\n",
            "Iteration 6575, loss = 0.88663883\n",
            "Iteration 6576, loss = 0.88658541\n",
            "Iteration 6577, loss = 0.88653439\n",
            "Iteration 6578, loss = 0.88648637\n",
            "Iteration 6579, loss = 0.88643452\n",
            "Iteration 6580, loss = 0.88638344\n",
            "Iteration 6581, loss = 0.88633196\n",
            "Iteration 6582, loss = 0.88628309\n",
            "Iteration 6583, loss = 0.88623302\n",
            "Iteration 6584, loss = 0.88618070\n",
            "Iteration 6585, loss = 0.88612943\n",
            "Iteration 6586, loss = 0.88607856\n",
            "Iteration 6587, loss = 0.88602843\n",
            "Iteration 6588, loss = 0.88597790\n",
            "Iteration 6589, loss = 0.88592695\n",
            "Iteration 6590, loss = 0.88587766\n",
            "Iteration 6591, loss = 0.88582613\n",
            "Iteration 6592, loss = 0.88577514\n",
            "Iteration 6593, loss = 0.88572410\n",
            "Iteration 6594, loss = 0.88567178\n",
            "Iteration 6595, loss = 0.88562055\n",
            "Iteration 6596, loss = 0.88557197\n",
            "Iteration 6597, loss = 0.88552285\n",
            "Iteration 6598, loss = 0.88547072\n",
            "Iteration 6599, loss = 0.88541976\n",
            "Iteration 6600, loss = 0.88536755\n",
            "Iteration 6601, loss = 0.88531853\n",
            "Iteration 6602, loss = 0.88526825\n",
            "Iteration 6603, loss = 0.88521911\n",
            "Iteration 6604, loss = 0.88516742\n",
            "Iteration 6605, loss = 0.88511749\n",
            "Iteration 6606, loss = 0.88506727\n",
            "Iteration 6607, loss = 0.88501513\n",
            "Iteration 6608, loss = 0.88496519\n",
            "Iteration 6609, loss = 0.88491535\n",
            "Iteration 6610, loss = 0.88486236\n",
            "Iteration 6611, loss = 0.88481336\n",
            "Iteration 6612, loss = 0.88476435\n",
            "Iteration 6613, loss = 0.88471421\n",
            "Iteration 6614, loss = 0.88466300\n",
            "Iteration 6615, loss = 0.88461224\n",
            "Iteration 6616, loss = 0.88456157\n",
            "Iteration 6617, loss = 0.88451369\n",
            "Iteration 6618, loss = 0.88446228\n",
            "Iteration 6619, loss = 0.88441037\n",
            "Iteration 6620, loss = 0.88436185\n",
            "Iteration 6621, loss = 0.88431213\n",
            "Iteration 6622, loss = 0.88426139\n",
            "Iteration 6623, loss = 0.88420987\n",
            "Iteration 6624, loss = 0.88415895\n",
            "Iteration 6625, loss = 0.88410949\n",
            "Iteration 6626, loss = 0.88405930\n",
            "Iteration 6627, loss = 0.88400934\n",
            "Iteration 6628, loss = 0.88395968\n",
            "Iteration 6629, loss = 0.88390862\n",
            "Iteration 6630, loss = 0.88385854\n",
            "Iteration 6631, loss = 0.88380704\n",
            "Iteration 6632, loss = 0.88375956\n",
            "Iteration 6633, loss = 0.88370942\n",
            "Iteration 6634, loss = 0.88365799\n",
            "Iteration 6635, loss = 0.88360762\n",
            "Iteration 6636, loss = 0.88355545\n",
            "Iteration 6637, loss = 0.88350499\n",
            "Iteration 6638, loss = 0.88345584\n",
            "Iteration 6639, loss = 0.88340520\n",
            "Iteration 6640, loss = 0.88335601\n",
            "Iteration 6641, loss = 0.88330616\n",
            "Iteration 6642, loss = 0.88325534\n",
            "Iteration 6643, loss = 0.88320480\n",
            "Iteration 6644, loss = 0.88315368\n",
            "Iteration 6645, loss = 0.88310353\n",
            "Iteration 6646, loss = 0.88305506\n",
            "Iteration 6647, loss = 0.88300455\n",
            "Iteration 6648, loss = 0.88295465\n",
            "Iteration 6649, loss = 0.88290450\n",
            "Iteration 6650, loss = 0.88285542\n",
            "Iteration 6651, loss = 0.88280471\n",
            "Iteration 6652, loss = 0.88275580\n",
            "Iteration 6653, loss = 0.88270456\n",
            "Iteration 6654, loss = 0.88265339\n",
            "Iteration 6655, loss = 0.88260598\n",
            "Iteration 6656, loss = 0.88255483\n",
            "Iteration 6657, loss = 0.88250548\n",
            "Iteration 6658, loss = 0.88245461\n",
            "Iteration 6659, loss = 0.88240811\n",
            "Iteration 6660, loss = 0.88235543\n",
            "Iteration 6661, loss = 0.88230561\n",
            "Iteration 6662, loss = 0.88225799\n",
            "Iteration 6663, loss = 0.88220595\n",
            "Iteration 6664, loss = 0.88215723\n",
            "Iteration 6665, loss = 0.88210956\n",
            "Iteration 6666, loss = 0.88205897\n",
            "Iteration 6667, loss = 0.88200719\n",
            "Iteration 6668, loss = 0.88195796\n",
            "Iteration 6669, loss = 0.88190941\n",
            "Iteration 6670, loss = 0.88185758\n",
            "Iteration 6671, loss = 0.88180884\n",
            "Iteration 6672, loss = 0.88175959\n",
            "Iteration 6673, loss = 0.88170951\n",
            "Iteration 6674, loss = 0.88165947\n",
            "Iteration 6675, loss = 0.88160901\n",
            "Iteration 6676, loss = 0.88156040\n",
            "Iteration 6677, loss = 0.88151058\n",
            "Iteration 6678, loss = 0.88146056\n",
            "Iteration 6679, loss = 0.88141117\n",
            "Iteration 6680, loss = 0.88136190\n",
            "Iteration 6681, loss = 0.88131300\n",
            "Iteration 6682, loss = 0.88126233\n",
            "Iteration 6683, loss = 0.88121139\n",
            "Iteration 6684, loss = 0.88116102\n",
            "Iteration 6685, loss = 0.88111108\n",
            "Iteration 6686, loss = 0.88106392\n",
            "Iteration 6687, loss = 0.88101350\n",
            "Iteration 6688, loss = 0.88096461\n",
            "Iteration 6689, loss = 0.88091551\n",
            "Iteration 6690, loss = 0.88086643\n",
            "Iteration 6691, loss = 0.88081946\n",
            "Iteration 6692, loss = 0.88077035\n",
            "Iteration 6693, loss = 0.88071993\n",
            "Iteration 6694, loss = 0.88067002\n",
            "Iteration 6695, loss = 0.88062050\n",
            "Iteration 6696, loss = 0.88056884\n",
            "Iteration 6697, loss = 0.88052015\n",
            "Iteration 6698, loss = 0.88046957\n",
            "Iteration 6699, loss = 0.88042146\n",
            "Iteration 6700, loss = 0.88037199\n",
            "Iteration 6701, loss = 0.88032337\n",
            "Iteration 6702, loss = 0.88027284\n",
            "Iteration 6703, loss = 0.88022205\n",
            "Iteration 6704, loss = 0.88017296\n",
            "Iteration 6705, loss = 0.88012433\n",
            "Iteration 6706, loss = 0.88007680\n",
            "Iteration 6707, loss = 0.88002771\n",
            "Iteration 6708, loss = 0.87997797\n",
            "Iteration 6709, loss = 0.87992726\n",
            "Iteration 6710, loss = 0.87987773\n",
            "Iteration 6711, loss = 0.87982977\n",
            "Iteration 6712, loss = 0.87978041\n",
            "Iteration 6713, loss = 0.87973061\n",
            "Iteration 6714, loss = 0.87968123\n",
            "Iteration 6715, loss = 0.87963129\n",
            "Iteration 6716, loss = 0.87958166\n",
            "Iteration 6717, loss = 0.87953364\n",
            "Iteration 6718, loss = 0.87948502\n",
            "Iteration 6719, loss = 0.87943500\n",
            "Iteration 6720, loss = 0.87938570\n",
            "Iteration 6721, loss = 0.87933691\n",
            "Iteration 6722, loss = 0.87928848\n",
            "Iteration 6723, loss = 0.87924067\n",
            "Iteration 6724, loss = 0.87919070\n",
            "Iteration 6725, loss = 0.87914230\n",
            "Iteration 6726, loss = 0.87909281\n",
            "Iteration 6727, loss = 0.87904205\n",
            "Iteration 6728, loss = 0.87899371\n",
            "Iteration 6729, loss = 0.87894541\n",
            "Iteration 6730, loss = 0.87889597\n",
            "Iteration 6731, loss = 0.87884956\n",
            "Iteration 6732, loss = 0.87879882\n",
            "Iteration 6733, loss = 0.87874826\n",
            "Iteration 6734, loss = 0.87870135\n",
            "Iteration 6735, loss = 0.87865292\n",
            "Iteration 6736, loss = 0.87860230\n",
            "Iteration 6737, loss = 0.87855382\n",
            "Iteration 6738, loss = 0.87850475\n",
            "Iteration 6739, loss = 0.87845662\n",
            "Iteration 6740, loss = 0.87840802\n",
            "Iteration 6741, loss = 0.87835953\n",
            "Iteration 6742, loss = 0.87831048\n",
            "Iteration 6743, loss = 0.87826202\n",
            "Iteration 6744, loss = 0.87821270\n",
            "Iteration 6745, loss = 0.87816337\n",
            "Iteration 6746, loss = 0.87811706\n",
            "Iteration 6747, loss = 0.87806547\n",
            "Iteration 6748, loss = 0.87801678\n",
            "Iteration 6749, loss = 0.87796885\n",
            "Iteration 6750, loss = 0.87791941\n",
            "Iteration 6751, loss = 0.87787271\n",
            "Iteration 6752, loss = 0.87782158\n",
            "Iteration 6753, loss = 0.87777349\n",
            "Iteration 6754, loss = 0.87772557\n",
            "Iteration 6755, loss = 0.87767503\n",
            "Iteration 6756, loss = 0.87762648\n",
            "Iteration 6757, loss = 0.87757759\n",
            "Iteration 6758, loss = 0.87752931\n",
            "Iteration 6759, loss = 0.87747937\n",
            "Iteration 6760, loss = 0.87743340\n",
            "Iteration 6761, loss = 0.87738405\n",
            "Iteration 6762, loss = 0.87733449\n",
            "Iteration 6763, loss = 0.87728541\n",
            "Iteration 6764, loss = 0.87723772\n",
            "Iteration 6765, loss = 0.87718900\n",
            "Iteration 6766, loss = 0.87714522\n",
            "Iteration 6767, loss = 0.87709464\n",
            "Iteration 6768, loss = 0.87704367\n",
            "Iteration 6769, loss = 0.87699641\n",
            "Iteration 6770, loss = 0.87694732\n",
            "Iteration 6771, loss = 0.87689886\n",
            "Iteration 6772, loss = 0.87685210\n",
            "Iteration 6773, loss = 0.87680365\n",
            "Iteration 6774, loss = 0.87675537\n",
            "Iteration 6775, loss = 0.87670676\n",
            "Iteration 6776, loss = 0.87665780\n",
            "Iteration 6777, loss = 0.87660885\n",
            "Iteration 6778, loss = 0.87656052\n",
            "Iteration 6779, loss = 0.87651133\n",
            "Iteration 6780, loss = 0.87646268\n",
            "Iteration 6781, loss = 0.87641333\n",
            "Iteration 6782, loss = 0.87636391\n",
            "Iteration 6783, loss = 0.87631798\n",
            "Iteration 6784, loss = 0.87627052\n",
            "Iteration 6785, loss = 0.87621887\n",
            "Iteration 6786, loss = 0.87617150\n",
            "Iteration 6787, loss = 0.87612294\n",
            "Iteration 6788, loss = 0.87607488\n",
            "Iteration 6789, loss = 0.87602678\n",
            "Iteration 6790, loss = 0.87597810\n",
            "Iteration 6791, loss = 0.87592978\n",
            "Iteration 6792, loss = 0.87588127\n",
            "Iteration 6793, loss = 0.87583215\n",
            "Iteration 6794, loss = 0.87578393\n",
            "Iteration 6795, loss = 0.87573494\n",
            "Iteration 6796, loss = 0.87568881\n",
            "Iteration 6797, loss = 0.87563879\n",
            "Iteration 6798, loss = 0.87559236\n",
            "Iteration 6799, loss = 0.87554217\n",
            "Iteration 6800, loss = 0.87549559\n",
            "Iteration 6801, loss = 0.87544669\n",
            "Iteration 6802, loss = 0.87539966\n",
            "Iteration 6803, loss = 0.87535303\n",
            "Iteration 6804, loss = 0.87530389\n",
            "Iteration 6805, loss = 0.87525692\n",
            "Iteration 6806, loss = 0.87520873\n",
            "Iteration 6807, loss = 0.87516071\n",
            "Iteration 6808, loss = 0.87511373\n",
            "Iteration 6809, loss = 0.87506520\n",
            "Iteration 6810, loss = 0.87501538\n",
            "Iteration 6811, loss = 0.87496701\n",
            "Iteration 6812, loss = 0.87491957\n",
            "Iteration 6813, loss = 0.87487150\n",
            "Iteration 6814, loss = 0.87482849\n",
            "Iteration 6815, loss = 0.87477852\n",
            "Iteration 6816, loss = 0.87473093\n",
            "Iteration 6817, loss = 0.87468368\n",
            "Iteration 6818, loss = 0.87463404\n",
            "Iteration 6819, loss = 0.87458736\n",
            "Iteration 6820, loss = 0.87454073\n",
            "Iteration 6821, loss = 0.87449214\n",
            "Iteration 6822, loss = 0.87444512\n",
            "Iteration 6823, loss = 0.87439706\n",
            "Iteration 6824, loss = 0.87435035\n",
            "Iteration 6825, loss = 0.87430169\n",
            "Iteration 6826, loss = 0.87425439\n",
            "Iteration 6827, loss = 0.87420642\n",
            "Iteration 6828, loss = 0.87415827\n",
            "Iteration 6829, loss = 0.87410998\n",
            "Iteration 6830, loss = 0.87406249\n",
            "Iteration 6831, loss = 0.87401487\n",
            "Iteration 6832, loss = 0.87396907\n",
            "Iteration 6833, loss = 0.87392147\n",
            "Iteration 6834, loss = 0.87387432\n",
            "Iteration 6835, loss = 0.87382578\n",
            "Iteration 6836, loss = 0.87378009\n",
            "Iteration 6837, loss = 0.87373255\n",
            "Iteration 6838, loss = 0.87368333\n",
            "Iteration 6839, loss = 0.87363517\n",
            "Iteration 6840, loss = 0.87358637\n",
            "Iteration 6841, loss = 0.87353958\n",
            "Iteration 6842, loss = 0.87349269\n",
            "Iteration 6843, loss = 0.87344561\n",
            "Iteration 6844, loss = 0.87339851\n",
            "Iteration 6845, loss = 0.87335160\n",
            "Iteration 6846, loss = 0.87330423\n",
            "Iteration 6847, loss = 0.87325560\n",
            "Iteration 6848, loss = 0.87320926\n",
            "Iteration 6849, loss = 0.87316281\n",
            "Iteration 6850, loss = 0.87311534\n",
            "Iteration 6851, loss = 0.87306763\n",
            "Iteration 6852, loss = 0.87302100\n",
            "Iteration 6853, loss = 0.87297398\n",
            "Iteration 6854, loss = 0.87292604\n",
            "Iteration 6855, loss = 0.87287879\n",
            "Iteration 6856, loss = 0.87283091\n",
            "Iteration 6857, loss = 0.87278504\n",
            "Iteration 6858, loss = 0.87273923\n",
            "Iteration 6859, loss = 0.87269128\n",
            "Iteration 6860, loss = 0.87264395\n",
            "Iteration 6861, loss = 0.87259561\n",
            "Iteration 6862, loss = 0.87254857\n",
            "Iteration 6863, loss = 0.87250196\n",
            "Iteration 6864, loss = 0.87245499\n",
            "Iteration 6865, loss = 0.87240862\n",
            "Iteration 6866, loss = 0.87236185\n",
            "Iteration 6867, loss = 0.87231497\n",
            "Iteration 6868, loss = 0.87226670\n",
            "Iteration 6869, loss = 0.87221748\n",
            "Iteration 6870, loss = 0.87217265\n",
            "Iteration 6871, loss = 0.87212719\n",
            "Iteration 6872, loss = 0.87208054\n",
            "Iteration 6873, loss = 0.87203287\n",
            "Iteration 6874, loss = 0.87198505\n",
            "Iteration 6875, loss = 0.87193872\n",
            "Iteration 6876, loss = 0.87189212\n",
            "Iteration 6877, loss = 0.87184514\n",
            "Iteration 6878, loss = 0.87179919\n",
            "Iteration 6879, loss = 0.87175186\n",
            "Iteration 6880, loss = 0.87170412\n",
            "Iteration 6881, loss = 0.87165629\n",
            "Iteration 6882, loss = 0.87160743\n",
            "Iteration 6883, loss = 0.87156148\n",
            "Iteration 6884, loss = 0.87151577\n",
            "Iteration 6885, loss = 0.87146957\n",
            "Iteration 6886, loss = 0.87142183\n",
            "Iteration 6887, loss = 0.87137579\n",
            "Iteration 6888, loss = 0.87132874\n",
            "Iteration 6889, loss = 0.87128296\n",
            "Iteration 6890, loss = 0.87123685\n",
            "Iteration 6891, loss = 0.87119084\n",
            "Iteration 6892, loss = 0.87114232\n",
            "Iteration 6893, loss = 0.87109408\n",
            "Iteration 6894, loss = 0.87104970\n",
            "Iteration 6895, loss = 0.87100371\n",
            "Iteration 6896, loss = 0.87095718\n",
            "Iteration 6897, loss = 0.87090879\n",
            "Iteration 6898, loss = 0.87086320\n",
            "Iteration 6899, loss = 0.87081555\n",
            "Iteration 6900, loss = 0.87076749\n",
            "Iteration 6901, loss = 0.87072278\n",
            "Iteration 6902, loss = 0.87067605\n",
            "Iteration 6903, loss = 0.87062990\n",
            "Iteration 6904, loss = 0.87058250\n",
            "Iteration 6905, loss = 0.87053437\n",
            "Iteration 6906, loss = 0.87048789\n",
            "Iteration 6907, loss = 0.87044131\n",
            "Iteration 6908, loss = 0.87039580\n",
            "Iteration 6909, loss = 0.87034893\n",
            "Iteration 6910, loss = 0.87030320\n",
            "Iteration 6911, loss = 0.87025498\n",
            "Iteration 6912, loss = 0.87020812\n",
            "Iteration 6913, loss = 0.87016508\n",
            "Iteration 6914, loss = 0.87011711\n",
            "Iteration 6915, loss = 0.87006901\n",
            "Iteration 6916, loss = 0.87002385\n",
            "Iteration 6917, loss = 0.86997567\n",
            "Iteration 6918, loss = 0.86993013\n",
            "Iteration 6919, loss = 0.86988480\n",
            "Iteration 6920, loss = 0.86983773\n",
            "Iteration 6921, loss = 0.86979232\n",
            "Iteration 6922, loss = 0.86974575\n",
            "Iteration 6923, loss = 0.86969780\n",
            "Iteration 6924, loss = 0.86965377\n",
            "Iteration 6925, loss = 0.86960700\n",
            "Iteration 6926, loss = 0.86956060\n",
            "Iteration 6927, loss = 0.86951351\n",
            "Iteration 6928, loss = 0.86946904\n",
            "Iteration 6929, loss = 0.86942254\n",
            "Iteration 6930, loss = 0.86937491\n",
            "Iteration 6931, loss = 0.86933183\n",
            "Iteration 6932, loss = 0.86928432\n",
            "Iteration 6933, loss = 0.86923645\n",
            "Iteration 6934, loss = 0.86919242\n",
            "Iteration 6935, loss = 0.86914641\n",
            "Iteration 6936, loss = 0.86909906\n",
            "Iteration 6937, loss = 0.86905472\n",
            "Iteration 6938, loss = 0.86900781\n",
            "Iteration 6939, loss = 0.86896070\n",
            "Iteration 6940, loss = 0.86891259\n",
            "Iteration 6941, loss = 0.86886911\n",
            "Iteration 6942, loss = 0.86882028\n",
            "Iteration 6943, loss = 0.86877757\n",
            "Iteration 6944, loss = 0.86873243\n",
            "Iteration 6945, loss = 0.86868399\n",
            "Iteration 6946, loss = 0.86863931\n",
            "Iteration 6947, loss = 0.86859457\n",
            "Iteration 6948, loss = 0.86854784\n",
            "Iteration 6949, loss = 0.86849924\n",
            "Iteration 6950, loss = 0.86845226\n",
            "Iteration 6951, loss = 0.86840695\n",
            "Iteration 6952, loss = 0.86836008\n",
            "Iteration 6953, loss = 0.86831427\n",
            "Iteration 6954, loss = 0.86826962\n",
            "Iteration 6955, loss = 0.86822561\n",
            "Iteration 6956, loss = 0.86817881\n",
            "Iteration 6957, loss = 0.86813344\n",
            "Iteration 6958, loss = 0.86808527\n",
            "Iteration 6959, loss = 0.86803918\n",
            "Iteration 6960, loss = 0.86799541\n",
            "Iteration 6961, loss = 0.86794987\n",
            "Iteration 6962, loss = 0.86790467\n",
            "Iteration 6963, loss = 0.86785833\n",
            "Iteration 6964, loss = 0.86781128\n",
            "Iteration 6965, loss = 0.86776598\n",
            "Iteration 6966, loss = 0.86771928\n",
            "Iteration 6967, loss = 0.86767442\n",
            "Iteration 6968, loss = 0.86763095\n",
            "Iteration 6969, loss = 0.86758448\n",
            "Iteration 6970, loss = 0.86753651\n",
            "Iteration 6971, loss = 0.86748989\n",
            "Iteration 6972, loss = 0.86744312\n",
            "Iteration 6973, loss = 0.86739925\n",
            "Iteration 6974, loss = 0.86735551\n",
            "Iteration 6975, loss = 0.86730736\n",
            "Iteration 6976, loss = 0.86726126\n",
            "Iteration 6977, loss = 0.86721673\n",
            "Iteration 6978, loss = 0.86717041\n",
            "Iteration 6979, loss = 0.86712310\n",
            "Iteration 6980, loss = 0.86707920\n",
            "Iteration 6981, loss = 0.86703538\n",
            "Iteration 6982, loss = 0.86698646\n",
            "Iteration 6983, loss = 0.86693976\n",
            "Iteration 6984, loss = 0.86689402\n",
            "Iteration 6985, loss = 0.86684847\n",
            "Iteration 6986, loss = 0.86680181\n",
            "Iteration 6987, loss = 0.86675639\n",
            "Iteration 6988, loss = 0.86671015\n",
            "Iteration 6989, loss = 0.86666505\n",
            "Iteration 6990, loss = 0.86661904\n",
            "Iteration 6991, loss = 0.86657387\n",
            "Iteration 6992, loss = 0.86652847\n",
            "Iteration 6993, loss = 0.86648205\n",
            "Iteration 6994, loss = 0.86643748\n",
            "Iteration 6995, loss = 0.86639352\n",
            "Iteration 6996, loss = 0.86634774\n",
            "Iteration 6997, loss = 0.86630050\n",
            "Iteration 6998, loss = 0.86625524\n",
            "Iteration 6999, loss = 0.86621036\n",
            "Iteration 7000, loss = 0.86616448\n",
            "Iteration 7001, loss = 0.86611874\n",
            "Iteration 7002, loss = 0.86607335\n",
            "Iteration 7003, loss = 0.86602952\n",
            "Iteration 7004, loss = 0.86598472\n",
            "Iteration 7005, loss = 0.86593699\n",
            "Iteration 7006, loss = 0.86589300\n",
            "Iteration 7007, loss = 0.86584785\n",
            "Iteration 7008, loss = 0.86580359\n",
            "Iteration 7009, loss = 0.86575806\n",
            "Iteration 7010, loss = 0.86571171\n",
            "Iteration 7011, loss = 0.86566353\n",
            "Iteration 7012, loss = 0.86561965\n",
            "Iteration 7013, loss = 0.86557488\n",
            "Iteration 7014, loss = 0.86552914\n",
            "Iteration 7015, loss = 0.86548429\n",
            "Iteration 7016, loss = 0.86543919\n",
            "Iteration 7017, loss = 0.86539385\n",
            "Iteration 7018, loss = 0.86534901\n",
            "Iteration 7019, loss = 0.86530413\n",
            "Iteration 7020, loss = 0.86525668\n",
            "Iteration 7021, loss = 0.86521210\n",
            "Iteration 7022, loss = 0.86516775\n",
            "Iteration 7023, loss = 0.86512262\n",
            "Iteration 7024, loss = 0.86507645\n",
            "Iteration 7025, loss = 0.86502902\n",
            "Iteration 7026, loss = 0.86498522\n",
            "Iteration 7027, loss = 0.86494101\n",
            "Iteration 7028, loss = 0.86489648\n",
            "Iteration 7029, loss = 0.86485090\n",
            "Iteration 7030, loss = 0.86480689\n",
            "Iteration 7031, loss = 0.86476138\n",
            "Iteration 7032, loss = 0.86471643\n",
            "Iteration 7033, loss = 0.86467063\n",
            "Iteration 7034, loss = 0.86462478\n",
            "Iteration 7035, loss = 0.86457823\n",
            "Iteration 7036, loss = 0.86453461\n",
            "Iteration 7037, loss = 0.86448945\n",
            "Iteration 7038, loss = 0.86444413\n",
            "Iteration 7039, loss = 0.86439810\n",
            "Iteration 7040, loss = 0.86435350\n",
            "Iteration 7041, loss = 0.86430756\n",
            "Iteration 7042, loss = 0.86426405\n",
            "Iteration 7043, loss = 0.86421978\n",
            "Iteration 7044, loss = 0.86417472\n",
            "Iteration 7045, loss = 0.86412902\n",
            "Iteration 7046, loss = 0.86408372\n",
            "Iteration 7047, loss = 0.86403770\n",
            "Iteration 7048, loss = 0.86399388\n",
            "Iteration 7049, loss = 0.86394939\n",
            "Iteration 7050, loss = 0.86390307\n",
            "Iteration 7051, loss = 0.86385854\n",
            "Iteration 7052, loss = 0.86381359\n",
            "Iteration 7053, loss = 0.86376845\n",
            "Iteration 7054, loss = 0.86372378\n",
            "Iteration 7055, loss = 0.86368000\n",
            "Iteration 7056, loss = 0.86363488\n",
            "Iteration 7057, loss = 0.86358963\n",
            "Iteration 7058, loss = 0.86354272\n",
            "Iteration 7059, loss = 0.86350002\n",
            "Iteration 7060, loss = 0.86345548\n",
            "Iteration 7061, loss = 0.86340924\n",
            "Iteration 7062, loss = 0.86336557\n",
            "Iteration 7063, loss = 0.86332095\n",
            "Iteration 7064, loss = 0.86327353\n",
            "Iteration 7065, loss = 0.86322878\n",
            "Iteration 7066, loss = 0.86318493\n",
            "Iteration 7067, loss = 0.86314013\n",
            "Iteration 7068, loss = 0.86309305\n",
            "Iteration 7069, loss = 0.86304909\n",
            "Iteration 7070, loss = 0.86300613\n",
            "Iteration 7071, loss = 0.86296099\n",
            "Iteration 7072, loss = 0.86291465\n",
            "Iteration 7073, loss = 0.86286966\n",
            "Iteration 7074, loss = 0.86282336\n",
            "Iteration 7075, loss = 0.86277977\n",
            "Iteration 7076, loss = 0.86273680\n",
            "Iteration 7077, loss = 0.86268888\n",
            "Iteration 7078, loss = 0.86264689\n",
            "Iteration 7079, loss = 0.86260231\n",
            "Iteration 7080, loss = 0.86255601\n",
            "Iteration 7081, loss = 0.86251433\n",
            "Iteration 7082, loss = 0.86247060\n",
            "Iteration 7083, loss = 0.86242521\n",
            "Iteration 7084, loss = 0.86237780\n",
            "Iteration 7085, loss = 0.86233607\n",
            "Iteration 7086, loss = 0.86229490\n",
            "Iteration 7087, loss = 0.86224789\n",
            "Iteration 7088, loss = 0.86220046\n",
            "Iteration 7089, loss = 0.86215533\n",
            "Iteration 7090, loss = 0.86211131\n",
            "Iteration 7091, loss = 0.86206708\n",
            "Iteration 7092, loss = 0.86202109\n",
            "Iteration 7093, loss = 0.86197543\n",
            "Iteration 7094, loss = 0.86193283\n",
            "Iteration 7095, loss = 0.86188587\n",
            "Iteration 7096, loss = 0.86184033\n",
            "Iteration 7097, loss = 0.86179787\n",
            "Iteration 7098, loss = 0.86175249\n",
            "Iteration 7099, loss = 0.86171006\n",
            "Iteration 7100, loss = 0.86166496\n",
            "Iteration 7101, loss = 0.86162104\n",
            "Iteration 7102, loss = 0.86157652\n",
            "Iteration 7103, loss = 0.86153143\n",
            "Iteration 7104, loss = 0.86148625\n",
            "Iteration 7105, loss = 0.86143970\n",
            "Iteration 7106, loss = 0.86139626\n",
            "Iteration 7107, loss = 0.86135245\n",
            "Iteration 7108, loss = 0.86130789\n",
            "Iteration 7109, loss = 0.86126145\n",
            "Iteration 7110, loss = 0.86121688\n",
            "Iteration 7111, loss = 0.86117299\n",
            "Iteration 7112, loss = 0.86112902\n",
            "Iteration 7113, loss = 0.86108401\n",
            "Iteration 7114, loss = 0.86104019\n",
            "Iteration 7115, loss = 0.86099466\n",
            "Iteration 7116, loss = 0.86094956\n",
            "Iteration 7117, loss = 0.86090651\n",
            "Iteration 7118, loss = 0.86086379\n",
            "Iteration 7119, loss = 0.86082022\n",
            "Iteration 7120, loss = 0.86077513\n",
            "Iteration 7121, loss = 0.86072911\n",
            "Iteration 7122, loss = 0.86068435\n",
            "Iteration 7123, loss = 0.86063998\n",
            "Iteration 7124, loss = 0.86059598\n",
            "Iteration 7125, loss = 0.86055220\n",
            "Iteration 7126, loss = 0.86050785\n",
            "Iteration 7127, loss = 0.86046291\n",
            "Iteration 7128, loss = 0.86041797\n",
            "Iteration 7129, loss = 0.86037402\n",
            "Iteration 7130, loss = 0.86033025\n",
            "Iteration 7131, loss = 0.86028557\n",
            "Iteration 7132, loss = 0.86024036\n",
            "Iteration 7133, loss = 0.86019540\n",
            "Iteration 7134, loss = 0.86014992\n",
            "Iteration 7135, loss = 0.86010760\n",
            "Iteration 7136, loss = 0.86006207\n",
            "Iteration 7137, loss = 0.86001776\n",
            "Iteration 7138, loss = 0.85997289\n",
            "Iteration 7139, loss = 0.85992916\n",
            "Iteration 7140, loss = 0.85988593\n",
            "Iteration 7141, loss = 0.85984117\n",
            "Iteration 7142, loss = 0.85979722\n",
            "Iteration 7143, loss = 0.85975402\n",
            "Iteration 7144, loss = 0.85970850\n",
            "Iteration 7145, loss = 0.85966437\n",
            "Iteration 7146, loss = 0.85961981\n",
            "Iteration 7147, loss = 0.85957619\n",
            "Iteration 7148, loss = 0.85953211\n",
            "Iteration 7149, loss = 0.85948865\n",
            "Iteration 7150, loss = 0.85944346\n",
            "Iteration 7151, loss = 0.85939923\n",
            "Iteration 7152, loss = 0.85935447\n",
            "Iteration 7153, loss = 0.85931035\n",
            "Iteration 7154, loss = 0.85926712\n",
            "Iteration 7155, loss = 0.85922304\n",
            "Iteration 7156, loss = 0.85917810\n",
            "Iteration 7157, loss = 0.85913458\n",
            "Iteration 7158, loss = 0.85909178\n",
            "Iteration 7159, loss = 0.85904755\n",
            "Iteration 7160, loss = 0.85900279\n",
            "Iteration 7161, loss = 0.85895811\n",
            "Iteration 7162, loss = 0.85891401\n",
            "Iteration 7163, loss = 0.85887028\n",
            "Iteration 7164, loss = 0.85882677\n",
            "Iteration 7165, loss = 0.85878229\n",
            "Iteration 7166, loss = 0.85873849\n",
            "Iteration 7167, loss = 0.85869412\n",
            "Iteration 7168, loss = 0.85865007\n",
            "Iteration 7169, loss = 0.85860733\n",
            "Iteration 7170, loss = 0.85856245\n",
            "Iteration 7171, loss = 0.85851782\n",
            "Iteration 7172, loss = 0.85847459\n",
            "Iteration 7173, loss = 0.85843211\n",
            "Iteration 7174, loss = 0.85838776\n",
            "Iteration 7175, loss = 0.85834249\n",
            "Iteration 7176, loss = 0.85829954\n",
            "Iteration 7177, loss = 0.85825687\n",
            "Iteration 7178, loss = 0.85821344\n",
            "Iteration 7179, loss = 0.85816800\n",
            "Iteration 7180, loss = 0.85812388\n",
            "Iteration 7181, loss = 0.85808014\n",
            "Iteration 7182, loss = 0.85803548\n",
            "Iteration 7183, loss = 0.85799268\n",
            "Iteration 7184, loss = 0.85794941\n",
            "Iteration 7185, loss = 0.85790476\n",
            "Iteration 7186, loss = 0.85786092\n",
            "Iteration 7187, loss = 0.85781703\n",
            "Iteration 7188, loss = 0.85777520\n",
            "Iteration 7189, loss = 0.85772901\n",
            "Iteration 7190, loss = 0.85768630\n",
            "Iteration 7191, loss = 0.85764298\n",
            "Iteration 7192, loss = 0.85760023\n",
            "Iteration 7193, loss = 0.85755588\n",
            "Iteration 7194, loss = 0.85751216\n",
            "Iteration 7195, loss = 0.85746797\n",
            "Iteration 7196, loss = 0.85742519\n",
            "Iteration 7197, loss = 0.85738049\n",
            "Iteration 7198, loss = 0.85733867\n",
            "Iteration 7199, loss = 0.85729510\n",
            "Iteration 7200, loss = 0.85725055\n",
            "Iteration 7201, loss = 0.85720809\n",
            "Iteration 7202, loss = 0.85716666\n",
            "Iteration 7203, loss = 0.85712083\n",
            "Iteration 7204, loss = 0.85707721\n",
            "Iteration 7205, loss = 0.85703300\n",
            "Iteration 7206, loss = 0.85698909\n",
            "Iteration 7207, loss = 0.85694709\n",
            "Iteration 7208, loss = 0.85690391\n",
            "Iteration 7209, loss = 0.85685975\n",
            "Iteration 7210, loss = 0.85681505\n",
            "Iteration 7211, loss = 0.85677264\n",
            "Iteration 7212, loss = 0.85672987\n",
            "Iteration 7213, loss = 0.85668718\n",
            "Iteration 7214, loss = 0.85664276\n",
            "Iteration 7215, loss = 0.85659889\n",
            "Iteration 7216, loss = 0.85655662\n",
            "Iteration 7217, loss = 0.85651391\n",
            "Iteration 7218, loss = 0.85646889\n",
            "Iteration 7219, loss = 0.85642720\n",
            "Iteration 7220, loss = 0.85638321\n",
            "Iteration 7221, loss = 0.85634017\n",
            "Iteration 7222, loss = 0.85629892\n",
            "Iteration 7223, loss = 0.85625596\n",
            "Iteration 7224, loss = 0.85621184\n",
            "Iteration 7225, loss = 0.85616771\n",
            "Iteration 7226, loss = 0.85612350\n",
            "Iteration 7227, loss = 0.85608178\n",
            "Iteration 7228, loss = 0.85604058\n",
            "Iteration 7229, loss = 0.85599607\n",
            "Iteration 7230, loss = 0.85595150\n",
            "Iteration 7231, loss = 0.85590674\n",
            "Iteration 7232, loss = 0.85586460\n",
            "Iteration 7233, loss = 0.85581873\n",
            "Iteration 7234, loss = 0.85577637\n",
            "Iteration 7235, loss = 0.85573347\n",
            "Iteration 7236, loss = 0.85569015\n",
            "Iteration 7237, loss = 0.85564738\n",
            "Iteration 7238, loss = 0.85560297\n",
            "Iteration 7239, loss = 0.85556183\n",
            "Iteration 7240, loss = 0.85551981\n",
            "Iteration 7241, loss = 0.85547558\n",
            "Iteration 7242, loss = 0.85543150\n",
            "Iteration 7243, loss = 0.85538923\n",
            "Iteration 7244, loss = 0.85534758\n",
            "Iteration 7245, loss = 0.85530249\n",
            "Iteration 7246, loss = 0.85525845\n",
            "Iteration 7247, loss = 0.85521526\n",
            "Iteration 7248, loss = 0.85517176\n",
            "Iteration 7249, loss = 0.85512837\n",
            "Iteration 7250, loss = 0.85508587\n",
            "Iteration 7251, loss = 0.85504279\n",
            "Iteration 7252, loss = 0.85499975\n",
            "Iteration 7253, loss = 0.85495571\n",
            "Iteration 7254, loss = 0.85491338\n",
            "Iteration 7255, loss = 0.85487028\n",
            "Iteration 7256, loss = 0.85482660\n",
            "Iteration 7257, loss = 0.85478332\n",
            "Iteration 7258, loss = 0.85474280\n",
            "Iteration 7259, loss = 0.85470035\n",
            "Iteration 7260, loss = 0.85465547\n",
            "Iteration 7261, loss = 0.85461155\n",
            "Iteration 7262, loss = 0.85456925\n",
            "Iteration 7263, loss = 0.85452681\n",
            "Iteration 7264, loss = 0.85448377\n",
            "Iteration 7265, loss = 0.85444117\n",
            "Iteration 7266, loss = 0.85439869\n",
            "Iteration 7267, loss = 0.85435340\n",
            "Iteration 7268, loss = 0.85431166\n",
            "Iteration 7269, loss = 0.85426891\n",
            "Iteration 7270, loss = 0.85422523\n",
            "Iteration 7271, loss = 0.85418226\n",
            "Iteration 7272, loss = 0.85413919\n",
            "Iteration 7273, loss = 0.85409636\n",
            "Iteration 7274, loss = 0.85405205\n",
            "Iteration 7275, loss = 0.85400834\n",
            "Iteration 7276, loss = 0.85396822\n",
            "Iteration 7277, loss = 0.85392318\n",
            "Iteration 7278, loss = 0.85388481\n",
            "Iteration 7279, loss = 0.85384131\n",
            "Iteration 7280, loss = 0.85379475\n",
            "Iteration 7281, loss = 0.85375520\n",
            "Iteration 7282, loss = 0.85371400\n",
            "Iteration 7283, loss = 0.85366846\n",
            "Iteration 7284, loss = 0.85362491\n",
            "Iteration 7285, loss = 0.85358520\n",
            "Iteration 7286, loss = 0.85354362\n",
            "Iteration 7287, loss = 0.85350004\n",
            "Iteration 7288, loss = 0.85345466\n",
            "Iteration 7289, loss = 0.85340993\n",
            "Iteration 7290, loss = 0.85337332\n",
            "Iteration 7291, loss = 0.85332908\n",
            "Iteration 7292, loss = 0.85328050\n",
            "Iteration 7293, loss = 0.85323996\n",
            "Iteration 7294, loss = 0.85319744\n",
            "Iteration 7295, loss = 0.85315226\n",
            "Iteration 7296, loss = 0.85311003\n",
            "Iteration 7297, loss = 0.85306641\n",
            "Iteration 7298, loss = 0.85302370\n",
            "Iteration 7299, loss = 0.85298189\n",
            "Iteration 7300, loss = 0.85293893\n",
            "Iteration 7301, loss = 0.85289583\n",
            "Iteration 7302, loss = 0.85285273\n",
            "Iteration 7303, loss = 0.85281120\n",
            "Iteration 7304, loss = 0.85276907\n",
            "Iteration 7305, loss = 0.85272525\n",
            "Iteration 7306, loss = 0.85268135\n",
            "Iteration 7307, loss = 0.85264219\n",
            "Iteration 7308, loss = 0.85259937\n",
            "Iteration 7309, loss = 0.85255400\n",
            "Iteration 7310, loss = 0.85251505\n",
            "Iteration 7311, loss = 0.85247074\n",
            "Iteration 7312, loss = 0.85242650\n",
            "Iteration 7313, loss = 0.85238570\n",
            "Iteration 7314, loss = 0.85234416\n",
            "Iteration 7315, loss = 0.85229962\n",
            "Iteration 7316, loss = 0.85225587\n",
            "Iteration 7317, loss = 0.85221371\n",
            "Iteration 7318, loss = 0.85217116\n",
            "Iteration 7319, loss = 0.85212945\n",
            "Iteration 7320, loss = 0.85208522\n",
            "Iteration 7321, loss = 0.85204302\n",
            "Iteration 7322, loss = 0.85200064\n",
            "Iteration 7323, loss = 0.85195898\n",
            "Iteration 7324, loss = 0.85191626\n",
            "Iteration 7325, loss = 0.85187373\n",
            "Iteration 7326, loss = 0.85183096\n",
            "Iteration 7327, loss = 0.85178842\n",
            "Iteration 7328, loss = 0.85174628\n",
            "Iteration 7329, loss = 0.85170276\n",
            "Iteration 7330, loss = 0.85166063\n",
            "Iteration 7331, loss = 0.85161801\n",
            "Iteration 7332, loss = 0.85157663\n",
            "Iteration 7333, loss = 0.85153479\n",
            "Iteration 7334, loss = 0.85149011\n",
            "Iteration 7335, loss = 0.85144954\n",
            "Iteration 7336, loss = 0.85140706\n",
            "Iteration 7337, loss = 0.85136420\n",
            "Iteration 7338, loss = 0.85132579\n",
            "Iteration 7339, loss = 0.85128131\n",
            "Iteration 7340, loss = 0.85123791\n",
            "Iteration 7341, loss = 0.85119733\n",
            "Iteration 7342, loss = 0.85115478\n",
            "Iteration 7343, loss = 0.85111258\n",
            "Iteration 7344, loss = 0.85107380\n",
            "Iteration 7345, loss = 0.85103265\n",
            "Iteration 7346, loss = 0.85098963\n",
            "Iteration 7347, loss = 0.85094479\n",
            "Iteration 7348, loss = 0.85090772\n",
            "Iteration 7349, loss = 0.85086536\n",
            "Iteration 7350, loss = 0.85082311\n",
            "Iteration 7351, loss = 0.85077731\n",
            "Iteration 7352, loss = 0.85073830\n",
            "Iteration 7353, loss = 0.85070038\n",
            "Iteration 7354, loss = 0.85065647\n",
            "Iteration 7355, loss = 0.85060955\n",
            "Iteration 7356, loss = 0.85057070\n",
            "Iteration 7357, loss = 0.85053186\n",
            "Iteration 7358, loss = 0.85048718\n",
            "Iteration 7359, loss = 0.85044172\n",
            "Iteration 7360, loss = 0.85040406\n",
            "Iteration 7361, loss = 0.85036384\n",
            "Iteration 7362, loss = 0.85032126\n",
            "Iteration 7363, loss = 0.85027672\n",
            "Iteration 7364, loss = 0.85023480\n",
            "Iteration 7365, loss = 0.85019689\n",
            "Iteration 7366, loss = 0.85015499\n",
            "Iteration 7367, loss = 0.85010855\n",
            "Iteration 7368, loss = 0.85006846\n",
            "Iteration 7369, loss = 0.85002707\n",
            "Iteration 7370, loss = 0.84998392\n",
            "Iteration 7371, loss = 0.84994344\n",
            "Iteration 7372, loss = 0.84990010\n",
            "Iteration 7373, loss = 0.84985561\n",
            "Iteration 7374, loss = 0.84981571\n",
            "Iteration 7375, loss = 0.84977366\n",
            "Iteration 7376, loss = 0.84973185\n",
            "Iteration 7377, loss = 0.84969103\n",
            "Iteration 7378, loss = 0.84964969\n",
            "Iteration 7379, loss = 0.84960647\n",
            "Iteration 7380, loss = 0.84956806\n",
            "Iteration 7381, loss = 0.84952693\n",
            "Iteration 7382, loss = 0.84948427\n",
            "Iteration 7383, loss = 0.84944197\n",
            "Iteration 7384, loss = 0.84940387\n",
            "Iteration 7385, loss = 0.84936309\n",
            "Iteration 7386, loss = 0.84931979\n",
            "Iteration 7387, loss = 0.84927760\n",
            "Iteration 7388, loss = 0.84923805\n",
            "Iteration 7389, loss = 0.84919672\n",
            "Iteration 7390, loss = 0.84915525\n",
            "Iteration 7391, loss = 0.84911242\n",
            "Iteration 7392, loss = 0.84907059\n",
            "Iteration 7393, loss = 0.84902951\n",
            "Iteration 7394, loss = 0.84899043\n",
            "Iteration 7395, loss = 0.84894960\n",
            "Iteration 7396, loss = 0.84890617\n",
            "Iteration 7397, loss = 0.84886647\n",
            "Iteration 7398, loss = 0.84882571\n",
            "Iteration 7399, loss = 0.84878300\n",
            "Iteration 7400, loss = 0.84874092\n",
            "Iteration 7401, loss = 0.84870203\n",
            "Iteration 7402, loss = 0.84866183\n",
            "Iteration 7403, loss = 0.84861900\n",
            "Iteration 7404, loss = 0.84857592\n",
            "Iteration 7405, loss = 0.84853438\n",
            "Iteration 7406, loss = 0.84849607\n",
            "Iteration 7407, loss = 0.84845496\n",
            "Iteration 7408, loss = 0.84841294\n",
            "Iteration 7409, loss = 0.84837023\n",
            "Iteration 7410, loss = 0.84833031\n",
            "Iteration 7411, loss = 0.84828854\n",
            "Iteration 7412, loss = 0.84824684\n",
            "Iteration 7413, loss = 0.84820786\n",
            "Iteration 7414, loss = 0.84816869\n",
            "Iteration 7415, loss = 0.84812783\n",
            "Iteration 7416, loss = 0.84808498\n",
            "Iteration 7417, loss = 0.84804316\n",
            "Iteration 7418, loss = 0.84800284\n",
            "Iteration 7419, loss = 0.84795955\n",
            "Iteration 7420, loss = 0.84792105\n",
            "Iteration 7421, loss = 0.84788101\n",
            "Iteration 7422, loss = 0.84783746\n",
            "Iteration 7423, loss = 0.84779696\n",
            "Iteration 7424, loss = 0.84775564\n",
            "Iteration 7425, loss = 0.84771502\n",
            "Iteration 7426, loss = 0.84767512\n",
            "Iteration 7427, loss = 0.84763473\n",
            "Iteration 7428, loss = 0.84759314\n",
            "Iteration 7429, loss = 0.84755353\n",
            "Iteration 7430, loss = 0.84751262\n",
            "Iteration 7431, loss = 0.84747235\n",
            "Iteration 7432, loss = 0.84743053\n",
            "Iteration 7433, loss = 0.84738793\n",
            "Iteration 7434, loss = 0.84734754\n",
            "Iteration 7435, loss = 0.84730620\n",
            "Iteration 7436, loss = 0.84726787\n",
            "Iteration 7437, loss = 0.84722702\n",
            "Iteration 7438, loss = 0.84718574\n",
            "Iteration 7439, loss = 0.84714408\n",
            "Iteration 7440, loss = 0.84710407\n",
            "Iteration 7441, loss = 0.84706388\n",
            "Iteration 7442, loss = 0.84702539\n",
            "Iteration 7443, loss = 0.84698526\n",
            "Iteration 7444, loss = 0.84694396\n",
            "Iteration 7445, loss = 0.84690165\n",
            "Iteration 7446, loss = 0.84686297\n",
            "Iteration 7447, loss = 0.84682396\n",
            "Iteration 7448, loss = 0.84678266\n",
            "Iteration 7449, loss = 0.84674017\n",
            "Iteration 7450, loss = 0.84669840\n",
            "Iteration 7451, loss = 0.84665950\n",
            "Iteration 7452, loss = 0.84661968\n",
            "Iteration 7453, loss = 0.84657785\n",
            "Iteration 7454, loss = 0.84653861\n",
            "Iteration 7455, loss = 0.84649744\n",
            "Iteration 7456, loss = 0.84645576\n",
            "Iteration 7457, loss = 0.84641666\n",
            "Iteration 7458, loss = 0.84637883\n",
            "Iteration 7459, loss = 0.84633544\n",
            "Iteration 7460, loss = 0.84629450\n",
            "Iteration 7461, loss = 0.84625482\n",
            "Iteration 7462, loss = 0.84621299\n",
            "Iteration 7463, loss = 0.84617333\n",
            "Iteration 7464, loss = 0.84613377\n",
            "Iteration 7465, loss = 0.84609009\n",
            "Iteration 7466, loss = 0.84604903\n",
            "Iteration 7467, loss = 0.84600615\n",
            "Iteration 7468, loss = 0.84596433\n",
            "Iteration 7469, loss = 0.84592238\n",
            "Iteration 7470, loss = 0.84587977\n",
            "Iteration 7471, loss = 0.84584001\n",
            "Iteration 7472, loss = 0.84579832\n",
            "Iteration 7473, loss = 0.84575591\n",
            "Iteration 7474, loss = 0.84571377\n",
            "Iteration 7475, loss = 0.84567070\n",
            "Iteration 7476, loss = 0.84563033\n",
            "Iteration 7477, loss = 0.84558612\n",
            "Iteration 7478, loss = 0.84554583\n",
            "Iteration 7479, loss = 0.84550127\n",
            "Iteration 7480, loss = 0.84546170\n",
            "Iteration 7481, loss = 0.84541884\n",
            "Iteration 7482, loss = 0.84537847\n",
            "Iteration 7483, loss = 0.84533738\n",
            "Iteration 7484, loss = 0.84529626\n",
            "Iteration 7485, loss = 0.84525377\n",
            "Iteration 7486, loss = 0.84521201\n",
            "Iteration 7487, loss = 0.84517453\n",
            "Iteration 7488, loss = 0.84513086\n",
            "Iteration 7489, loss = 0.84508639\n",
            "Iteration 7490, loss = 0.84504898\n",
            "Iteration 7491, loss = 0.84500867\n",
            "Iteration 7492, loss = 0.84496485\n",
            "Iteration 7493, loss = 0.84492277\n",
            "Iteration 7494, loss = 0.84488532\n",
            "Iteration 7495, loss = 0.84484291\n",
            "Iteration 7496, loss = 0.84479854\n",
            "Iteration 7497, loss = 0.84476002\n",
            "Iteration 7498, loss = 0.84471994\n",
            "Iteration 7499, loss = 0.84467991\n",
            "Iteration 7500, loss = 0.84463863\n",
            "Iteration 7501, loss = 0.84459675\n",
            "Iteration 7502, loss = 0.84455454\n",
            "Iteration 7503, loss = 0.84451335\n",
            "Iteration 7504, loss = 0.84447490\n",
            "Iteration 7505, loss = 0.84443678\n",
            "Iteration 7506, loss = 0.84439554\n",
            "Iteration 7507, loss = 0.84435342\n",
            "Iteration 7508, loss = 0.84431106\n",
            "Iteration 7509, loss = 0.84426916\n",
            "Iteration 7510, loss = 0.84423014\n",
            "Iteration 7511, loss = 0.84418932\n",
            "Iteration 7512, loss = 0.84414819\n",
            "Iteration 7513, loss = 0.84410681\n",
            "Iteration 7514, loss = 0.84406399\n",
            "Iteration 7515, loss = 0.84402906\n",
            "Iteration 7516, loss = 0.84398790\n",
            "Iteration 7517, loss = 0.84394567\n",
            "Iteration 7518, loss = 0.84390696\n",
            "Iteration 7519, loss = 0.84386592\n",
            "Iteration 7520, loss = 0.84382552\n",
            "Iteration 7521, loss = 0.84378717\n",
            "Iteration 7522, loss = 0.84374863\n",
            "Iteration 7523, loss = 0.84370822\n",
            "Iteration 7524, loss = 0.84366576\n",
            "Iteration 7525, loss = 0.84362341\n",
            "Iteration 7526, loss = 0.84358312\n",
            "Iteration 7527, loss = 0.84354351\n",
            "Iteration 7528, loss = 0.84350328\n",
            "Iteration 7529, loss = 0.84346290\n",
            "Iteration 7530, loss = 0.84342205\n",
            "Iteration 7531, loss = 0.84338093\n",
            "Iteration 7532, loss = 0.84334080\n",
            "Iteration 7533, loss = 0.84330068\n",
            "Iteration 7534, loss = 0.84326212\n",
            "Iteration 7535, loss = 0.84322079\n",
            "Iteration 7536, loss = 0.84317926\n",
            "Iteration 7537, loss = 0.84313893\n",
            "Iteration 7538, loss = 0.84309927\n",
            "Iteration 7539, loss = 0.84305927\n",
            "Iteration 7540, loss = 0.84301921\n",
            "Iteration 7541, loss = 0.84297851\n",
            "Iteration 7542, loss = 0.84293867\n",
            "Iteration 7543, loss = 0.84289787\n",
            "Iteration 7544, loss = 0.84285883\n",
            "Iteration 7545, loss = 0.84281863\n",
            "Iteration 7546, loss = 0.84277759\n",
            "Iteration 7547, loss = 0.84273686\n",
            "Iteration 7548, loss = 0.84269785\n",
            "Iteration 7549, loss = 0.84265745\n",
            "Iteration 7550, loss = 0.84261628\n",
            "Iteration 7551, loss = 0.84257663\n",
            "Iteration 7552, loss = 0.84253692\n",
            "Iteration 7553, loss = 0.84249777\n",
            "Iteration 7554, loss = 0.84245793\n",
            "Iteration 7555, loss = 0.84241697\n",
            "Iteration 7556, loss = 0.84237781\n",
            "Iteration 7557, loss = 0.84233737\n",
            "Iteration 7558, loss = 0.84229748\n",
            "Iteration 7559, loss = 0.84225786\n",
            "Iteration 7560, loss = 0.84221794\n",
            "Iteration 7561, loss = 0.84217806\n",
            "Iteration 7562, loss = 0.84213953\n",
            "Iteration 7563, loss = 0.84210188\n",
            "Iteration 7564, loss = 0.84206165\n",
            "Iteration 7565, loss = 0.84202312\n",
            "Iteration 7566, loss = 0.84198392\n",
            "Iteration 7567, loss = 0.84194235\n",
            "Iteration 7568, loss = 0.84190441\n",
            "Iteration 7569, loss = 0.84186657\n",
            "Iteration 7570, loss = 0.84182891\n",
            "Iteration 7571, loss = 0.84178586\n",
            "Iteration 7572, loss = 0.84174894\n",
            "Iteration 7573, loss = 0.84170981\n",
            "Iteration 7574, loss = 0.84167160\n",
            "Iteration 7575, loss = 0.84163008\n",
            "Iteration 7576, loss = 0.84158718\n",
            "Iteration 7577, loss = 0.84155117\n",
            "Iteration 7578, loss = 0.84151266\n",
            "Iteration 7579, loss = 0.84147487\n",
            "Iteration 7580, loss = 0.84143580\n",
            "Iteration 7581, loss = 0.84139503\n",
            "Iteration 7582, loss = 0.84135351\n",
            "Iteration 7583, loss = 0.84131452\n",
            "Iteration 7584, loss = 0.84127660\n",
            "Iteration 7585, loss = 0.84123732\n",
            "Iteration 7586, loss = 0.84119840\n",
            "Iteration 7587, loss = 0.84115943\n",
            "Iteration 7588, loss = 0.84111914\n",
            "Iteration 7589, loss = 0.84108007\n",
            "Iteration 7590, loss = 0.84103945\n",
            "Iteration 7591, loss = 0.84100129\n",
            "Iteration 7592, loss = 0.84096116\n",
            "Iteration 7593, loss = 0.84092330\n",
            "Iteration 7594, loss = 0.84088399\n",
            "Iteration 7595, loss = 0.84084363\n",
            "Iteration 7596, loss = 0.84080550\n",
            "Iteration 7597, loss = 0.84076659\n",
            "Iteration 7598, loss = 0.84072650\n",
            "Iteration 7599, loss = 0.84068841\n",
            "Iteration 7600, loss = 0.84064942\n",
            "Iteration 7601, loss = 0.84060982\n",
            "Iteration 7602, loss = 0.84057036\n",
            "Iteration 7603, loss = 0.84053324\n",
            "Iteration 7604, loss = 0.84049282\n",
            "Iteration 7605, loss = 0.84045446\n",
            "Iteration 7606, loss = 0.84041487\n",
            "Iteration 7607, loss = 0.84037696\n",
            "Iteration 7608, loss = 0.84033894\n",
            "Iteration 7609, loss = 0.84029926\n",
            "Iteration 7610, loss = 0.84026027\n",
            "Iteration 7611, loss = 0.84022148\n",
            "Iteration 7612, loss = 0.84018254\n",
            "Iteration 7613, loss = 0.84014434\n",
            "Iteration 7614, loss = 0.84010565\n",
            "Iteration 7615, loss = 0.84006559\n",
            "Iteration 7616, loss = 0.84002917\n",
            "Iteration 7617, loss = 0.83999013\n",
            "Iteration 7618, loss = 0.83995065\n",
            "Iteration 7619, loss = 0.83991113\n",
            "Iteration 7620, loss = 0.83987322\n",
            "Iteration 7621, loss = 0.83983362\n",
            "Iteration 7622, loss = 0.83979572\n",
            "Iteration 7623, loss = 0.83975803\n",
            "Iteration 7624, loss = 0.83971716\n",
            "Iteration 7625, loss = 0.83967906\n",
            "Iteration 7626, loss = 0.83964422\n",
            "Iteration 7627, loss = 0.83960214\n",
            "Iteration 7628, loss = 0.83956288\n",
            "Iteration 7629, loss = 0.83952569\n",
            "Iteration 7630, loss = 0.83948856\n",
            "Iteration 7631, loss = 0.83944820\n",
            "Iteration 7632, loss = 0.83941149\n",
            "Iteration 7633, loss = 0.83937385\n",
            "Iteration 7634, loss = 0.83933457\n",
            "Iteration 7635, loss = 0.83929683\n",
            "Iteration 7636, loss = 0.83925841\n",
            "Iteration 7637, loss = 0.83921860\n",
            "Iteration 7638, loss = 0.83917765\n",
            "Iteration 7639, loss = 0.83913687\n",
            "Iteration 7640, loss = 0.83910013\n",
            "Iteration 7641, loss = 0.83906177\n",
            "Iteration 7642, loss = 0.83902432\n",
            "Iteration 7643, loss = 0.83898349\n",
            "Iteration 7644, loss = 0.83894378\n",
            "Iteration 7645, loss = 0.83890654\n",
            "Iteration 7646, loss = 0.83886862\n",
            "Iteration 7647, loss = 0.83883027\n",
            "Iteration 7648, loss = 0.83879276\n",
            "Iteration 7649, loss = 0.83875442\n",
            "Iteration 7650, loss = 0.83871597\n",
            "Iteration 7651, loss = 0.83867599\n",
            "Iteration 7652, loss = 0.83863855\n",
            "Iteration 7653, loss = 0.83860204\n",
            "Iteration 7654, loss = 0.83856381\n",
            "Iteration 7655, loss = 0.83852595\n",
            "Iteration 7656, loss = 0.83848683\n",
            "Iteration 7657, loss = 0.83844661\n",
            "Iteration 7658, loss = 0.83840797\n",
            "Iteration 7659, loss = 0.83836664\n",
            "Iteration 7660, loss = 0.83832845\n",
            "Iteration 7661, loss = 0.83829142\n",
            "Iteration 7662, loss = 0.83825404\n",
            "Iteration 7663, loss = 0.83821558\n",
            "Iteration 7664, loss = 0.83817749\n",
            "Iteration 7665, loss = 0.83813860\n",
            "Iteration 7666, loss = 0.83809848\n",
            "Iteration 7667, loss = 0.83805996\n",
            "Iteration 7668, loss = 0.83802154\n",
            "Iteration 7669, loss = 0.83798488\n",
            "Iteration 7670, loss = 0.83794667\n",
            "Iteration 7671, loss = 0.83790855\n",
            "Iteration 7672, loss = 0.83787039\n",
            "Iteration 7673, loss = 0.83783250\n",
            "Iteration 7674, loss = 0.83779545\n",
            "Iteration 7675, loss = 0.83775674\n",
            "Iteration 7676, loss = 0.83771699\n",
            "Iteration 7677, loss = 0.83767928\n",
            "Iteration 7678, loss = 0.83764071\n",
            "Iteration 7679, loss = 0.83760112\n",
            "Iteration 7680, loss = 0.83756290\n",
            "Iteration 7681, loss = 0.83752572\n",
            "Iteration 7682, loss = 0.83748675\n",
            "Iteration 7683, loss = 0.83744818\n",
            "Iteration 7684, loss = 0.83741010\n",
            "Iteration 7685, loss = 0.83736997\n",
            "Iteration 7686, loss = 0.83733337\n",
            "Iteration 7687, loss = 0.83729469\n",
            "Iteration 7688, loss = 0.83725585\n",
            "Iteration 7689, loss = 0.83721891\n",
            "Iteration 7690, loss = 0.83718112\n",
            "Iteration 7691, loss = 0.83714360\n",
            "Iteration 7692, loss = 0.83710557\n",
            "Iteration 7693, loss = 0.83706666\n",
            "Iteration 7694, loss = 0.83702921\n",
            "Iteration 7695, loss = 0.83699082\n",
            "Iteration 7696, loss = 0.83695414\n",
            "Iteration 7697, loss = 0.83691642\n",
            "Iteration 7698, loss = 0.83687877\n",
            "Iteration 7699, loss = 0.83684273\n",
            "Iteration 7700, loss = 0.83680383\n",
            "Iteration 7701, loss = 0.83676629\n",
            "Iteration 7702, loss = 0.83672773\n",
            "Iteration 7703, loss = 0.83668907\n",
            "Iteration 7704, loss = 0.83665323\n",
            "Iteration 7705, loss = 0.83661481\n",
            "Iteration 7706, loss = 0.83657576\n",
            "Iteration 7707, loss = 0.83654338\n",
            "Iteration 7708, loss = 0.83650479\n",
            "Iteration 7709, loss = 0.83646269\n",
            "Iteration 7710, loss = 0.83642824\n",
            "Iteration 7711, loss = 0.83638984\n",
            "Iteration 7712, loss = 0.83635011\n",
            "Iteration 7713, loss = 0.83631403\n",
            "Iteration 7714, loss = 0.83627694\n",
            "Iteration 7715, loss = 0.83623834\n",
            "Iteration 7716, loss = 0.83620039\n",
            "Iteration 7717, loss = 0.83616353\n",
            "Iteration 7718, loss = 0.83612415\n",
            "Iteration 7719, loss = 0.83608487\n",
            "Iteration 7720, loss = 0.83604819\n",
            "Iteration 7721, loss = 0.83601280\n",
            "Iteration 7722, loss = 0.83597675\n",
            "Iteration 7723, loss = 0.83593905\n",
            "Iteration 7724, loss = 0.83589857\n",
            "Iteration 7725, loss = 0.83586066\n",
            "Iteration 7726, loss = 0.83582255\n",
            "Iteration 7727, loss = 0.83578558\n",
            "Iteration 7728, loss = 0.83574757\n",
            "Iteration 7729, loss = 0.83571303\n",
            "Iteration 7730, loss = 0.83567286\n",
            "Iteration 7731, loss = 0.83563577\n",
            "Iteration 7732, loss = 0.83560132\n",
            "Iteration 7733, loss = 0.83556150\n",
            "Iteration 7734, loss = 0.83552475\n",
            "Iteration 7735, loss = 0.83548932\n",
            "Iteration 7736, loss = 0.83545238\n",
            "Iteration 7737, loss = 0.83541386\n",
            "Iteration 7738, loss = 0.83537426\n",
            "Iteration 7739, loss = 0.83533690\n",
            "Iteration 7740, loss = 0.83529811\n",
            "Iteration 7741, loss = 0.83525933\n",
            "Iteration 7742, loss = 0.83522120\n",
            "Iteration 7743, loss = 0.83518552\n",
            "Iteration 7744, loss = 0.83514772\n",
            "Iteration 7745, loss = 0.83511110\n",
            "Iteration 7746, loss = 0.83507171\n",
            "Iteration 7747, loss = 0.83503555\n",
            "Iteration 7748, loss = 0.83499847\n",
            "Iteration 7749, loss = 0.83495988\n",
            "Iteration 7750, loss = 0.83492634\n",
            "Iteration 7751, loss = 0.83488902\n",
            "Iteration 7752, loss = 0.83484792\n",
            "Iteration 7753, loss = 0.83481676\n",
            "Iteration 7754, loss = 0.83477964\n",
            "Iteration 7755, loss = 0.83473766\n",
            "Iteration 7756, loss = 0.83469984\n",
            "Iteration 7757, loss = 0.83466603\n",
            "Iteration 7758, loss = 0.83462892\n",
            "Iteration 7759, loss = 0.83458907\n",
            "Iteration 7760, loss = 0.83454961\n",
            "Iteration 7761, loss = 0.83451382\n",
            "Iteration 7762, loss = 0.83447641\n",
            "Iteration 7763, loss = 0.83443937\n",
            "Iteration 7764, loss = 0.83440147\n",
            "Iteration 7765, loss = 0.83436249\n",
            "Iteration 7766, loss = 0.83432732\n",
            "Iteration 7767, loss = 0.83428883\n",
            "Iteration 7768, loss = 0.83425190\n",
            "Iteration 7769, loss = 0.83421394\n",
            "Iteration 7770, loss = 0.83417496\n",
            "Iteration 7771, loss = 0.83413872\n",
            "Iteration 7772, loss = 0.83410288\n",
            "Iteration 7773, loss = 0.83406533\n",
            "Iteration 7774, loss = 0.83402689\n",
            "Iteration 7775, loss = 0.83398940\n",
            "Iteration 7776, loss = 0.83395068\n",
            "Iteration 7777, loss = 0.83391341\n",
            "Iteration 7778, loss = 0.83387394\n",
            "Iteration 7779, loss = 0.83384133\n",
            "Iteration 7780, loss = 0.83380336\n",
            "Iteration 7781, loss = 0.83376469\n",
            "Iteration 7782, loss = 0.83372832\n",
            "Iteration 7783, loss = 0.83368830\n",
            "Iteration 7784, loss = 0.83365211\n",
            "Iteration 7785, loss = 0.83361544\n",
            "Iteration 7786, loss = 0.83357842\n",
            "Iteration 7787, loss = 0.83354092\n",
            "Iteration 7788, loss = 0.83350261\n",
            "Iteration 7789, loss = 0.83346340\n",
            "Iteration 7790, loss = 0.83342944\n",
            "Iteration 7791, loss = 0.83339544\n",
            "Iteration 7792, loss = 0.83335780\n",
            "Iteration 7793, loss = 0.83331739\n",
            "Iteration 7794, loss = 0.83328012\n",
            "Iteration 7795, loss = 0.83324325\n",
            "Iteration 7796, loss = 0.83320727\n",
            "Iteration 7797, loss = 0.83317142\n",
            "Iteration 7798, loss = 0.83313329\n",
            "Iteration 7799, loss = 0.83309713\n",
            "Iteration 7800, loss = 0.83305944\n",
            "Iteration 7801, loss = 0.83302324\n",
            "Iteration 7802, loss = 0.83298604\n",
            "Iteration 7803, loss = 0.83294832\n",
            "Iteration 7804, loss = 0.83290998\n",
            "Iteration 7805, loss = 0.83287332\n",
            "Iteration 7806, loss = 0.83283527\n",
            "Iteration 7807, loss = 0.83279865\n",
            "Iteration 7808, loss = 0.83276074\n",
            "Iteration 7809, loss = 0.83272258\n",
            "Iteration 7810, loss = 0.83268488\n",
            "Iteration 7811, loss = 0.83264868\n",
            "Iteration 7812, loss = 0.83261297\n",
            "Iteration 7813, loss = 0.83257518\n",
            "Iteration 7814, loss = 0.83253816\n",
            "Iteration 7815, loss = 0.83250193\n",
            "Iteration 7816, loss = 0.83246428\n",
            "Iteration 7817, loss = 0.83242590\n",
            "Iteration 7818, loss = 0.83238878\n",
            "Iteration 7819, loss = 0.83235031\n",
            "Iteration 7820, loss = 0.83231787\n",
            "Iteration 7821, loss = 0.83228087\n",
            "Iteration 7822, loss = 0.83224129\n",
            "Iteration 7823, loss = 0.83220509\n",
            "Iteration 7824, loss = 0.83216842\n",
            "Iteration 7825, loss = 0.83213032\n",
            "Iteration 7826, loss = 0.83209472\n",
            "Iteration 7827, loss = 0.83205844\n",
            "Iteration 7828, loss = 0.83202063\n",
            "Iteration 7829, loss = 0.83198381\n",
            "Iteration 7830, loss = 0.83194548\n",
            "Iteration 7831, loss = 0.83191103\n",
            "Iteration 7832, loss = 0.83187280\n",
            "Iteration 7833, loss = 0.83183629\n",
            "Iteration 7834, loss = 0.83179945\n",
            "Iteration 7835, loss = 0.83176305\n",
            "Iteration 7836, loss = 0.83172718\n",
            "Iteration 7837, loss = 0.83168795\n",
            "Iteration 7838, loss = 0.83165161\n",
            "Iteration 7839, loss = 0.83161578\n",
            "Iteration 7840, loss = 0.83157968\n",
            "Iteration 7841, loss = 0.83154123\n",
            "Iteration 7842, loss = 0.83150436\n",
            "Iteration 7843, loss = 0.83146686\n",
            "Iteration 7844, loss = 0.83143114\n",
            "Iteration 7845, loss = 0.83139735\n",
            "Iteration 7846, loss = 0.83136013\n",
            "Iteration 7847, loss = 0.83131979\n",
            "Iteration 7848, loss = 0.83128662\n",
            "Iteration 7849, loss = 0.83124851\n",
            "Iteration 7850, loss = 0.83121443\n",
            "Iteration 7851, loss = 0.83117909\n",
            "Iteration 7852, loss = 0.83114226\n",
            "Iteration 7853, loss = 0.83110727\n",
            "Iteration 7854, loss = 0.83106690\n",
            "Iteration 7855, loss = 0.83102913\n",
            "Iteration 7856, loss = 0.83099479\n",
            "Iteration 7857, loss = 0.83095834\n",
            "Iteration 7858, loss = 0.83092124\n",
            "Iteration 7859, loss = 0.83088329\n",
            "Iteration 7860, loss = 0.83084803\n",
            "Iteration 7861, loss = 0.83081156\n",
            "Iteration 7862, loss = 0.83077368\n",
            "Iteration 7863, loss = 0.83073543\n",
            "Iteration 7864, loss = 0.83069883\n",
            "Iteration 7865, loss = 0.83066119\n",
            "Iteration 7866, loss = 0.83062333\n",
            "Iteration 7867, loss = 0.83058677\n",
            "Iteration 7868, loss = 0.83055061\n",
            "Iteration 7869, loss = 0.83051347\n",
            "Iteration 7870, loss = 0.83047771\n",
            "Iteration 7871, loss = 0.83044127\n",
            "Iteration 7872, loss = 0.83040438\n",
            "Iteration 7873, loss = 0.83036809\n",
            "Iteration 7874, loss = 0.83033123\n",
            "Iteration 7875, loss = 0.83029313\n",
            "Iteration 7876, loss = 0.83025571\n",
            "Iteration 7877, loss = 0.83022040\n",
            "Iteration 7878, loss = 0.83018487\n",
            "Iteration 7879, loss = 0.83014821\n",
            "Iteration 7880, loss = 0.83010987\n",
            "Iteration 7881, loss = 0.83007199\n",
            "Iteration 7882, loss = 0.83003734\n",
            "Iteration 7883, loss = 0.83000152\n",
            "Iteration 7884, loss = 0.82996582\n",
            "Iteration 7885, loss = 0.82992830\n",
            "Iteration 7886, loss = 0.82989168\n",
            "Iteration 7887, loss = 0.82985490\n",
            "Iteration 7888, loss = 0.82981975\n",
            "Iteration 7889, loss = 0.82978120\n",
            "Iteration 7890, loss = 0.82974359\n",
            "Iteration 7891, loss = 0.82970966\n",
            "Iteration 7892, loss = 0.82967213\n",
            "Iteration 7893, loss = 0.82963796\n",
            "Iteration 7894, loss = 0.82960076\n",
            "Iteration 7895, loss = 0.82956313\n",
            "Iteration 7896, loss = 0.82952754\n",
            "Iteration 7897, loss = 0.82949222\n",
            "Iteration 7898, loss = 0.82945610\n",
            "Iteration 7899, loss = 0.82941804\n",
            "Iteration 7900, loss = 0.82938111\n",
            "Iteration 7901, loss = 0.82934524\n",
            "Iteration 7902, loss = 0.82930833\n",
            "Iteration 7903, loss = 0.82927084\n",
            "Iteration 7904, loss = 0.82923532\n",
            "Iteration 7905, loss = 0.82920166\n",
            "Iteration 7906, loss = 0.82916503\n",
            "Iteration 7907, loss = 0.82912675\n",
            "Iteration 7908, loss = 0.82908716\n",
            "Iteration 7909, loss = 0.82905250\n",
            "Iteration 7910, loss = 0.82901768\n",
            "Iteration 7911, loss = 0.82898158\n",
            "Iteration 7912, loss = 0.82894488\n",
            "Iteration 7913, loss = 0.82890680\n",
            "Iteration 7914, loss = 0.82886855\n",
            "Iteration 7915, loss = 0.82883427\n",
            "Iteration 7916, loss = 0.82879734\n",
            "Iteration 7917, loss = 0.82876438\n",
            "Iteration 7918, loss = 0.82872861\n",
            "Iteration 7919, loss = 0.82868763\n",
            "Iteration 7920, loss = 0.82865214\n",
            "Iteration 7921, loss = 0.82861725\n",
            "Iteration 7922, loss = 0.82857854\n",
            "Iteration 7923, loss = 0.82854380\n",
            "Iteration 7924, loss = 0.82850789\n",
            "Iteration 7925, loss = 0.82847104\n",
            "Iteration 7926, loss = 0.82843548\n",
            "Iteration 7927, loss = 0.82840012\n",
            "Iteration 7928, loss = 0.82836182\n",
            "Iteration 7929, loss = 0.82832550\n",
            "Iteration 7930, loss = 0.82828893\n",
            "Iteration 7931, loss = 0.82825183\n",
            "Iteration 7932, loss = 0.82821454\n",
            "Iteration 7933, loss = 0.82817808\n",
            "Iteration 7934, loss = 0.82814096\n",
            "Iteration 7935, loss = 0.82810433\n",
            "Iteration 7936, loss = 0.82806880\n",
            "Iteration 7937, loss = 0.82803510\n",
            "Iteration 7938, loss = 0.82799992\n",
            "Iteration 7939, loss = 0.82796101\n",
            "Iteration 7940, loss = 0.82792501\n",
            "Iteration 7941, loss = 0.82789098\n",
            "Iteration 7942, loss = 0.82785484\n",
            "Iteration 7943, loss = 0.82781757\n",
            "Iteration 7944, loss = 0.82778175\n",
            "Iteration 7945, loss = 0.82774609\n",
            "Iteration 7946, loss = 0.82770759\n",
            "Iteration 7947, loss = 0.82767058\n",
            "Iteration 7948, loss = 0.82763549\n",
            "Iteration 7949, loss = 0.82759977\n",
            "Iteration 7950, loss = 0.82756288\n",
            "Iteration 7951, loss = 0.82752707\n",
            "Iteration 7952, loss = 0.82749218\n",
            "Iteration 7953, loss = 0.82745445\n",
            "Iteration 7954, loss = 0.82741894\n",
            "Iteration 7955, loss = 0.82738199\n",
            "Iteration 7956, loss = 0.82734450\n",
            "Iteration 7957, loss = 0.82730837\n",
            "Iteration 7958, loss = 0.82727309\n",
            "Iteration 7959, loss = 0.82723818\n",
            "Iteration 7960, loss = 0.82720273\n",
            "Iteration 7961, loss = 0.82716626\n",
            "Iteration 7962, loss = 0.82712951\n",
            "Iteration 7963, loss = 0.82709592\n",
            "Iteration 7964, loss = 0.82706016\n",
            "Iteration 7965, loss = 0.82702321\n",
            "Iteration 7966, loss = 0.82698584\n",
            "Iteration 7967, loss = 0.82694950\n",
            "Iteration 7968, loss = 0.82691303\n",
            "Iteration 7969, loss = 0.82687581\n",
            "Iteration 7970, loss = 0.82683843\n",
            "Iteration 7971, loss = 0.82680535\n",
            "Iteration 7972, loss = 0.82676854\n",
            "Iteration 7973, loss = 0.82673041\n",
            "Iteration 7974, loss = 0.82669725\n",
            "Iteration 7975, loss = 0.82666150\n",
            "Iteration 7976, loss = 0.82662593\n",
            "Iteration 7977, loss = 0.82659081\n",
            "Iteration 7978, loss = 0.82655437\n",
            "Iteration 7979, loss = 0.82652117\n",
            "Iteration 7980, loss = 0.82648425\n",
            "Iteration 7981, loss = 0.82644723\n",
            "Iteration 7982, loss = 0.82641302\n",
            "Iteration 7983, loss = 0.82637726\n",
            "Iteration 7984, loss = 0.82634008\n",
            "Iteration 7985, loss = 0.82630456\n",
            "Iteration 7986, loss = 0.82626818\n",
            "Iteration 7987, loss = 0.82623048\n",
            "Iteration 7988, loss = 0.82619175\n",
            "Iteration 7989, loss = 0.82615908\n",
            "Iteration 7990, loss = 0.82612368\n",
            "Iteration 7991, loss = 0.82608842\n",
            "Iteration 7992, loss = 0.82604938\n",
            "Iteration 7993, loss = 0.82601461\n",
            "Iteration 7994, loss = 0.82598032\n",
            "Iteration 7995, loss = 0.82594508\n",
            "Iteration 7996, loss = 0.82590775\n",
            "Iteration 7997, loss = 0.82587190\n",
            "Iteration 7998, loss = 0.82583591\n",
            "Iteration 7999, loss = 0.82579998\n",
            "Iteration 8000, loss = 0.82576336\n",
            "Iteration 8001, loss = 0.82572764\n",
            "Iteration 8002, loss = 0.82569192\n",
            "Iteration 8003, loss = 0.82565576\n",
            "Iteration 8004, loss = 0.82561880\n",
            "Iteration 8005, loss = 0.82558543\n",
            "Iteration 8006, loss = 0.82554812\n",
            "Iteration 8007, loss = 0.82551445\n",
            "Iteration 8008, loss = 0.82548035\n",
            "Iteration 8009, loss = 0.82544038\n",
            "Iteration 8010, loss = 0.82540656\n",
            "Iteration 8011, loss = 0.82537203\n",
            "Iteration 8012, loss = 0.82533632\n",
            "Iteration 8013, loss = 0.82529973\n",
            "Iteration 8014, loss = 0.82526266\n",
            "Iteration 8015, loss = 0.82522510\n",
            "Iteration 8016, loss = 0.82519018\n",
            "Iteration 8017, loss = 0.82515398\n",
            "Iteration 8018, loss = 0.82511662\n",
            "Iteration 8019, loss = 0.82508164\n",
            "Iteration 8020, loss = 0.82504754\n",
            "Iteration 8021, loss = 0.82501190\n",
            "Iteration 8022, loss = 0.82497598\n",
            "Iteration 8023, loss = 0.82494048\n",
            "Iteration 8024, loss = 0.82490365\n",
            "Iteration 8025, loss = 0.82486768\n",
            "Iteration 8026, loss = 0.82483235\n",
            "Iteration 8027, loss = 0.82479626\n",
            "Iteration 8028, loss = 0.82476529\n",
            "Iteration 8029, loss = 0.82472917\n",
            "Iteration 8030, loss = 0.82468988\n",
            "Iteration 8031, loss = 0.82465699\n",
            "Iteration 8032, loss = 0.82461915\n",
            "Iteration 8033, loss = 0.82458401\n",
            "Iteration 8034, loss = 0.82454908\n",
            "Iteration 8035, loss = 0.82451336\n",
            "Iteration 8036, loss = 0.82447720\n",
            "Iteration 8037, loss = 0.82444271\n",
            "Iteration 8038, loss = 0.82440593\n",
            "Iteration 8039, loss = 0.82437102\n",
            "Iteration 8040, loss = 0.82433924\n",
            "Iteration 8041, loss = 0.82430189\n",
            "Iteration 8042, loss = 0.82426613\n",
            "Iteration 8043, loss = 0.82423161\n",
            "Iteration 8044, loss = 0.82419423\n",
            "Iteration 8045, loss = 0.82416158\n",
            "Iteration 8046, loss = 0.82412849\n",
            "Iteration 8047, loss = 0.82409446\n",
            "Iteration 8048, loss = 0.82405773\n",
            "Iteration 8049, loss = 0.82402036\n",
            "Iteration 8050, loss = 0.82398247\n",
            "Iteration 8051, loss = 0.82394602\n",
            "Iteration 8052, loss = 0.82391076\n",
            "Iteration 8053, loss = 0.82387593\n",
            "Iteration 8054, loss = 0.82383955\n",
            "Iteration 8055, loss = 0.82380428\n",
            "Iteration 8056, loss = 0.82376972\n",
            "Iteration 8057, loss = 0.82373457\n",
            "Iteration 8058, loss = 0.82370023\n",
            "Iteration 8059, loss = 0.82366502\n",
            "Iteration 8060, loss = 0.82362765\n",
            "Iteration 8061, loss = 0.82359291\n",
            "Iteration 8062, loss = 0.82355886\n",
            "Iteration 8063, loss = 0.82352239\n",
            "Iteration 8064, loss = 0.82348902\n",
            "Iteration 8065, loss = 0.82345211\n",
            "Iteration 8066, loss = 0.82341485\n",
            "Iteration 8067, loss = 0.82338001\n",
            "Iteration 8068, loss = 0.82334746\n",
            "Iteration 8069, loss = 0.82331088\n",
            "Iteration 8070, loss = 0.82327473\n",
            "Iteration 8071, loss = 0.82324098\n",
            "Iteration 8072, loss = 0.82320643\n",
            "Iteration 8073, loss = 0.82317214\n",
            "Iteration 8074, loss = 0.82313736\n",
            "Iteration 8075, loss = 0.82310151\n",
            "Iteration 8076, loss = 0.82306468\n",
            "Iteration 8077, loss = 0.82302901\n",
            "Iteration 8078, loss = 0.82299399\n",
            "Iteration 8079, loss = 0.82295802\n",
            "Iteration 8080, loss = 0.82292234\n",
            "Iteration 8081, loss = 0.82288639\n",
            "Iteration 8082, loss = 0.82285216\n",
            "Iteration 8083, loss = 0.82281584\n",
            "Iteration 8084, loss = 0.82278264\n",
            "Iteration 8085, loss = 0.82274660\n",
            "Iteration 8086, loss = 0.82271279\n",
            "Iteration 8087, loss = 0.82267692\n",
            "Iteration 8088, loss = 0.82264210\n",
            "Iteration 8089, loss = 0.82260744\n",
            "Iteration 8090, loss = 0.82257165\n",
            "Iteration 8091, loss = 0.82253506\n",
            "Iteration 8092, loss = 0.82250394\n",
            "Iteration 8093, loss = 0.82246923\n",
            "Iteration 8094, loss = 0.82242960\n",
            "Iteration 8095, loss = 0.82239674\n",
            "Iteration 8096, loss = 0.82236150\n",
            "Iteration 8097, loss = 0.82232576\n",
            "Iteration 8098, loss = 0.82229086\n",
            "Iteration 8099, loss = 0.82225668\n",
            "Iteration 8100, loss = 0.82222166\n",
            "Iteration 8101, loss = 0.82218525\n",
            "Iteration 8102, loss = 0.82214989\n",
            "Iteration 8103, loss = 0.82211371\n",
            "Iteration 8104, loss = 0.82208122\n",
            "Iteration 8105, loss = 0.82204535\n",
            "Iteration 8106, loss = 0.82201152\n",
            "Iteration 8107, loss = 0.82197507\n",
            "Iteration 8108, loss = 0.82194170\n",
            "Iteration 8109, loss = 0.82190674\n",
            "Iteration 8110, loss = 0.82187247\n",
            "Iteration 8111, loss = 0.82183547\n",
            "Iteration 8112, loss = 0.82180119\n",
            "Iteration 8113, loss = 0.82176659\n",
            "Iteration 8114, loss = 0.82173083\n",
            "Iteration 8115, loss = 0.82169951\n",
            "Iteration 8116, loss = 0.82166350\n",
            "Iteration 8117, loss = 0.82162851\n",
            "Iteration 8118, loss = 0.82159221\n",
            "Iteration 8119, loss = 0.82155821\n",
            "Iteration 8120, loss = 0.82152513\n",
            "Iteration 8121, loss = 0.82149086\n",
            "Iteration 8122, loss = 0.82145496\n",
            "Iteration 8123, loss = 0.82141898\n",
            "Iteration 8124, loss = 0.82138262\n",
            "Iteration 8125, loss = 0.82135014\n",
            "Iteration 8126, loss = 0.82131360\n",
            "Iteration 8127, loss = 0.82127907\n",
            "Iteration 8128, loss = 0.82124661\n",
            "Iteration 8129, loss = 0.82121000\n",
            "Iteration 8130, loss = 0.82117563\n",
            "Iteration 8131, loss = 0.82113945\n",
            "Iteration 8132, loss = 0.82110511\n",
            "Iteration 8133, loss = 0.82107090\n",
            "Iteration 8134, loss = 0.82103480\n",
            "Iteration 8135, loss = 0.82100121\n",
            "Iteration 8136, loss = 0.82096747\n",
            "Iteration 8137, loss = 0.82093295\n",
            "Iteration 8138, loss = 0.82089677\n",
            "Iteration 8139, loss = 0.82086139\n",
            "Iteration 8140, loss = 0.82082481\n",
            "Iteration 8141, loss = 0.82079108\n",
            "Iteration 8142, loss = 0.82075738\n",
            "Iteration 8143, loss = 0.82072090\n",
            "Iteration 8144, loss = 0.82068932\n",
            "Iteration 8145, loss = 0.82065500\n",
            "Iteration 8146, loss = 0.82062035\n",
            "Iteration 8147, loss = 0.82058430\n",
            "Iteration 8148, loss = 0.82055119\n",
            "Iteration 8149, loss = 0.82051668\n",
            "Iteration 8150, loss = 0.82047995\n",
            "Iteration 8151, loss = 0.82044423\n",
            "Iteration 8152, loss = 0.82041040\n",
            "Iteration 8153, loss = 0.82037660\n",
            "Iteration 8154, loss = 0.82034103\n",
            "Iteration 8155, loss = 0.82030636\n",
            "Iteration 8156, loss = 0.82027096\n",
            "Iteration 8157, loss = 0.82023367\n",
            "Iteration 8158, loss = 0.82020180\n",
            "Iteration 8159, loss = 0.82016479\n",
            "Iteration 8160, loss = 0.82013154\n",
            "Iteration 8161, loss = 0.82009523\n",
            "Iteration 8162, loss = 0.82006110\n",
            "Iteration 8163, loss = 0.82002866\n",
            "Iteration 8164, loss = 0.81999244\n",
            "Iteration 8165, loss = 0.81995858\n",
            "Iteration 8166, loss = 0.81992511\n",
            "Iteration 8167, loss = 0.81989067\n",
            "Iteration 8168, loss = 0.81985525\n",
            "Iteration 8169, loss = 0.81982290\n",
            "Iteration 8170, loss = 0.81978701\n",
            "Iteration 8171, loss = 0.81975323\n",
            "Iteration 8172, loss = 0.81971948\n",
            "Iteration 8173, loss = 0.81968466\n",
            "Iteration 8174, loss = 0.81964993\n",
            "Iteration 8175, loss = 0.81961558\n",
            "Iteration 8176, loss = 0.81958185\n",
            "Iteration 8177, loss = 0.81954600\n",
            "Iteration 8178, loss = 0.81951231\n",
            "Iteration 8179, loss = 0.81947775\n",
            "Iteration 8180, loss = 0.81944420\n",
            "Iteration 8181, loss = 0.81940990\n",
            "Iteration 8182, loss = 0.81937502\n",
            "Iteration 8183, loss = 0.81934149\n",
            "Iteration 8184, loss = 0.81930688\n",
            "Iteration 8185, loss = 0.81927378\n",
            "Iteration 8186, loss = 0.81923883\n",
            "Iteration 8187, loss = 0.81920410\n",
            "Iteration 8188, loss = 0.81916964\n",
            "Iteration 8189, loss = 0.81913413\n",
            "Iteration 8190, loss = 0.81910109\n",
            "Iteration 8191, loss = 0.81906606\n",
            "Iteration 8192, loss = 0.81903197\n",
            "Iteration 8193, loss = 0.81899865\n",
            "Iteration 8194, loss = 0.81896498\n",
            "Iteration 8195, loss = 0.81892972\n",
            "Iteration 8196, loss = 0.81889655\n",
            "Iteration 8197, loss = 0.81886162\n",
            "Iteration 8198, loss = 0.81882813\n",
            "Iteration 8199, loss = 0.81879396\n",
            "Iteration 8200, loss = 0.81876007\n",
            "Iteration 8201, loss = 0.81872824\n",
            "Iteration 8202, loss = 0.81869351\n",
            "Iteration 8203, loss = 0.81865811\n",
            "Iteration 8204, loss = 0.81862553\n",
            "Iteration 8205, loss = 0.81859075\n",
            "Iteration 8206, loss = 0.81855181\n",
            "Iteration 8207, loss = 0.81851260\n",
            "Iteration 8208, loss = 0.81847674\n",
            "Iteration 8209, loss = 0.81843524\n",
            "Iteration 8210, loss = 0.81839348\n",
            "Iteration 8211, loss = 0.81835419\n",
            "Iteration 8212, loss = 0.81831303\n",
            "Iteration 8213, loss = 0.81827057\n",
            "Iteration 8214, loss = 0.81822658\n",
            "Iteration 8215, loss = 0.81818508\n",
            "Iteration 8216, loss = 0.81814300\n",
            "Iteration 8217, loss = 0.81809830\n",
            "Iteration 8218, loss = 0.81805702\n",
            "Iteration 8219, loss = 0.81801401\n",
            "Iteration 8220, loss = 0.81797029\n",
            "Iteration 8221, loss = 0.81792559\n",
            "Iteration 8222, loss = 0.81788211\n",
            "Iteration 8223, loss = 0.81783823\n",
            "Iteration 8224, loss = 0.81779410\n",
            "Iteration 8225, loss = 0.81775103\n",
            "Iteration 8226, loss = 0.81770642\n",
            "Iteration 8227, loss = 0.81766065\n",
            "Iteration 8228, loss = 0.81761911\n",
            "Iteration 8229, loss = 0.81757396\n",
            "Iteration 8230, loss = 0.81752948\n",
            "Iteration 8231, loss = 0.81748437\n",
            "Iteration 8232, loss = 0.81744106\n",
            "Iteration 8233, loss = 0.81739674\n",
            "Iteration 8234, loss = 0.81735208\n",
            "Iteration 8235, loss = 0.81730771\n",
            "Iteration 8236, loss = 0.81726395\n",
            "Iteration 8237, loss = 0.81721930\n",
            "Iteration 8238, loss = 0.81717570\n",
            "Iteration 8239, loss = 0.81712965\n",
            "Iteration 8240, loss = 0.81708845\n",
            "Iteration 8241, loss = 0.81704302\n",
            "Iteration 8242, loss = 0.81699762\n",
            "Iteration 8243, loss = 0.81695695\n",
            "Iteration 8244, loss = 0.81691231\n",
            "Iteration 8245, loss = 0.81686523\n",
            "Iteration 8246, loss = 0.81682197\n",
            "Iteration 8247, loss = 0.81678034\n",
            "Iteration 8248, loss = 0.81673635\n",
            "Iteration 8249, loss = 0.81669306\n",
            "Iteration 8250, loss = 0.81664741\n",
            "Iteration 8251, loss = 0.81660206\n",
            "Iteration 8252, loss = 0.81656063\n",
            "Iteration 8253, loss = 0.81651718\n",
            "Iteration 8254, loss = 0.81647211\n",
            "Iteration 8255, loss = 0.81642657\n",
            "Iteration 8256, loss = 0.81638370\n",
            "Iteration 8257, loss = 0.81633935\n",
            "Iteration 8258, loss = 0.81629461\n",
            "Iteration 8259, loss = 0.81625071\n",
            "Iteration 8260, loss = 0.81620578\n",
            "Iteration 8261, loss = 0.81616237\n",
            "Iteration 8262, loss = 0.81611918\n",
            "Iteration 8263, loss = 0.81607422\n",
            "Iteration 8264, loss = 0.81603116\n",
            "Iteration 8265, loss = 0.81598770\n",
            "Iteration 8266, loss = 0.81594493\n",
            "Iteration 8267, loss = 0.81590062\n",
            "Iteration 8268, loss = 0.81585778\n",
            "Iteration 8269, loss = 0.81581468\n",
            "Iteration 8270, loss = 0.81577062\n",
            "Iteration 8271, loss = 0.81572599\n",
            "Iteration 8272, loss = 0.81568395\n",
            "Iteration 8273, loss = 0.81564027\n",
            "Iteration 8274, loss = 0.81559675\n",
            "Iteration 8275, loss = 0.81555508\n",
            "Iteration 8276, loss = 0.81551333\n",
            "Iteration 8277, loss = 0.81547062\n",
            "Iteration 8278, loss = 0.81542647\n",
            "Iteration 8279, loss = 0.81538209\n",
            "Iteration 8280, loss = 0.81533767\n",
            "Iteration 8281, loss = 0.81529444\n",
            "Iteration 8282, loss = 0.81525075\n",
            "Iteration 8283, loss = 0.81520831\n",
            "Iteration 8284, loss = 0.81516482\n",
            "Iteration 8285, loss = 0.81512223\n",
            "Iteration 8286, loss = 0.81507896\n",
            "Iteration 8287, loss = 0.81503684\n",
            "Iteration 8288, loss = 0.81499339\n",
            "Iteration 8289, loss = 0.81495112\n",
            "Iteration 8290, loss = 0.81490851\n",
            "Iteration 8291, loss = 0.81486468\n",
            "Iteration 8292, loss = 0.81482180\n",
            "Iteration 8293, loss = 0.81478048\n",
            "Iteration 8294, loss = 0.81473844\n",
            "Iteration 8295, loss = 0.81469362\n",
            "Iteration 8296, loss = 0.81465042\n",
            "Iteration 8297, loss = 0.81460903\n",
            "Iteration 8298, loss = 0.81456662\n",
            "Iteration 8299, loss = 0.81452326\n",
            "Iteration 8300, loss = 0.81448177\n",
            "Iteration 8301, loss = 0.81443923\n",
            "Iteration 8302, loss = 0.81439748\n",
            "Iteration 8303, loss = 0.81435420\n",
            "Iteration 8304, loss = 0.81431182\n",
            "Iteration 8305, loss = 0.81426916\n",
            "Iteration 8306, loss = 0.81422774\n",
            "Iteration 8307, loss = 0.81418470\n",
            "Iteration 8308, loss = 0.81414212\n",
            "Iteration 8309, loss = 0.81409816\n",
            "Iteration 8310, loss = 0.81405685\n",
            "Iteration 8311, loss = 0.81401500\n",
            "Iteration 8312, loss = 0.81397267\n",
            "Iteration 8313, loss = 0.81392847\n",
            "Iteration 8314, loss = 0.81388611\n",
            "Iteration 8315, loss = 0.81384388\n",
            "Iteration 8316, loss = 0.81380189\n",
            "Iteration 8317, loss = 0.81375927\n",
            "Iteration 8318, loss = 0.81371744\n",
            "Iteration 8319, loss = 0.81367541\n",
            "Iteration 8320, loss = 0.81363172\n",
            "Iteration 8321, loss = 0.81359267\n",
            "Iteration 8322, loss = 0.81355004\n",
            "Iteration 8323, loss = 0.81350687\n",
            "Iteration 8324, loss = 0.81346586\n",
            "Iteration 8325, loss = 0.81342442\n",
            "Iteration 8326, loss = 0.81338169\n",
            "Iteration 8327, loss = 0.81334088\n",
            "Iteration 8328, loss = 0.81329838\n",
            "Iteration 8329, loss = 0.81325424\n",
            "Iteration 8330, loss = 0.81321329\n",
            "Iteration 8331, loss = 0.81317178\n",
            "Iteration 8332, loss = 0.81312906\n",
            "Iteration 8333, loss = 0.81308676\n",
            "Iteration 8334, loss = 0.81304572\n",
            "Iteration 8335, loss = 0.81300355\n",
            "Iteration 8336, loss = 0.81296211\n",
            "Iteration 8337, loss = 0.81291875\n",
            "Iteration 8338, loss = 0.81287750\n",
            "Iteration 8339, loss = 0.81283362\n",
            "Iteration 8340, loss = 0.81279401\n",
            "Iteration 8341, loss = 0.81275334\n",
            "Iteration 8342, loss = 0.81271271\n",
            "Iteration 8343, loss = 0.81267325\n",
            "Iteration 8344, loss = 0.81262888\n",
            "Iteration 8345, loss = 0.81258678\n",
            "Iteration 8346, loss = 0.81254606\n",
            "Iteration 8347, loss = 0.81250568\n",
            "Iteration 8348, loss = 0.81246298\n",
            "Iteration 8349, loss = 0.81242237\n",
            "Iteration 8350, loss = 0.81238174\n",
            "Iteration 8351, loss = 0.81233993\n",
            "Iteration 8352, loss = 0.81229678\n",
            "Iteration 8353, loss = 0.81225407\n",
            "Iteration 8354, loss = 0.81221338\n",
            "Iteration 8355, loss = 0.81217395\n",
            "Iteration 8356, loss = 0.81212981\n",
            "Iteration 8357, loss = 0.81208736\n",
            "Iteration 8358, loss = 0.81204674\n",
            "Iteration 8359, loss = 0.81200666\n",
            "Iteration 8360, loss = 0.81196654\n",
            "Iteration 8361, loss = 0.81192369\n",
            "Iteration 8362, loss = 0.81188299\n",
            "Iteration 8363, loss = 0.81184287\n",
            "Iteration 8364, loss = 0.81180092\n",
            "Iteration 8365, loss = 0.81176014\n",
            "Iteration 8366, loss = 0.81171866\n",
            "Iteration 8367, loss = 0.81167922\n",
            "Iteration 8368, loss = 0.81163843\n",
            "Iteration 8369, loss = 0.81159610\n",
            "Iteration 8370, loss = 0.81155434\n",
            "Iteration 8371, loss = 0.81151398\n",
            "Iteration 8372, loss = 0.81147289\n",
            "Iteration 8373, loss = 0.81143162\n",
            "Iteration 8374, loss = 0.81139099\n",
            "Iteration 8375, loss = 0.81135005\n",
            "Iteration 8376, loss = 0.81131053\n",
            "Iteration 8377, loss = 0.81126883\n",
            "Iteration 8378, loss = 0.81122680\n",
            "Iteration 8379, loss = 0.81118728\n",
            "Iteration 8380, loss = 0.81114664\n",
            "Iteration 8381, loss = 0.81110608\n",
            "Iteration 8382, loss = 0.81106599\n",
            "Iteration 8383, loss = 0.81102502\n",
            "Iteration 8384, loss = 0.81098493\n",
            "Iteration 8385, loss = 0.81094492\n",
            "Iteration 8386, loss = 0.81090501\n",
            "Iteration 8387, loss = 0.81086473\n",
            "Iteration 8388, loss = 0.81082407\n",
            "Iteration 8389, loss = 0.81078301\n",
            "Iteration 8390, loss = 0.81074357\n",
            "Iteration 8391, loss = 0.81070277\n",
            "Iteration 8392, loss = 0.81066127\n",
            "Iteration 8393, loss = 0.81062225\n",
            "Iteration 8394, loss = 0.81058211\n",
            "Iteration 8395, loss = 0.81054208\n",
            "Iteration 8396, loss = 0.81050245\n",
            "Iteration 8397, loss = 0.81046285\n",
            "Iteration 8398, loss = 0.81042348\n",
            "Iteration 8399, loss = 0.81038310\n",
            "Iteration 8400, loss = 0.81034243\n",
            "Iteration 8401, loss = 0.81030104\n",
            "Iteration 8402, loss = 0.81026062\n",
            "Iteration 8403, loss = 0.81022214\n",
            "Iteration 8404, loss = 0.81018433\n",
            "Iteration 8405, loss = 0.81014177\n",
            "Iteration 8406, loss = 0.81010096\n",
            "Iteration 8407, loss = 0.81006324\n",
            "Iteration 8408, loss = 0.81002365\n",
            "Iteration 8409, loss = 0.80998337\n",
            "Iteration 8410, loss = 0.80994256\n",
            "Iteration 8411, loss = 0.80990181\n",
            "Iteration 8412, loss = 0.80986411\n",
            "Iteration 8413, loss = 0.80982540\n",
            "Iteration 8414, loss = 0.80978492\n",
            "Iteration 8415, loss = 0.80974242\n",
            "Iteration 8416, loss = 0.80970460\n",
            "Iteration 8417, loss = 0.80966632\n",
            "Iteration 8418, loss = 0.80962639\n",
            "Iteration 8419, loss = 0.80958747\n",
            "Iteration 8420, loss = 0.80954711\n",
            "Iteration 8421, loss = 0.80950654\n",
            "Iteration 8422, loss = 0.80946520\n",
            "Iteration 8423, loss = 0.80942463\n",
            "Iteration 8424, loss = 0.80938593\n",
            "Iteration 8425, loss = 0.80934842\n",
            "Iteration 8426, loss = 0.80930709\n",
            "Iteration 8427, loss = 0.80926758\n",
            "Iteration 8428, loss = 0.80922612\n",
            "Iteration 8429, loss = 0.80918988\n",
            "Iteration 8430, loss = 0.80915225\n",
            "Iteration 8431, loss = 0.80911288\n",
            "Iteration 8432, loss = 0.80907281\n",
            "Iteration 8433, loss = 0.80903248\n",
            "Iteration 8434, loss = 0.80899208\n",
            "Iteration 8435, loss = 0.80895067\n",
            "Iteration 8436, loss = 0.80891038\n",
            "Iteration 8437, loss = 0.80887268\n",
            "Iteration 8438, loss = 0.80883241\n",
            "Iteration 8439, loss = 0.80879339\n",
            "Iteration 8440, loss = 0.80875356\n",
            "Iteration 8441, loss = 0.80871346\n",
            "Iteration 8442, loss = 0.80867440\n",
            "Iteration 8443, loss = 0.80863533\n",
            "Iteration 8444, loss = 0.80859674\n",
            "Iteration 8445, loss = 0.80855589\n",
            "Iteration 8446, loss = 0.80851719\n",
            "Iteration 8447, loss = 0.80847906\n",
            "Iteration 8448, loss = 0.80843773\n",
            "Iteration 8449, loss = 0.80839914\n",
            "Iteration 8450, loss = 0.80836161\n",
            "Iteration 8451, loss = 0.80832173\n",
            "Iteration 8452, loss = 0.80828220\n",
            "Iteration 8453, loss = 0.80824409\n",
            "Iteration 8454, loss = 0.80820667\n",
            "Iteration 8455, loss = 0.80816591\n",
            "Iteration 8456, loss = 0.80812528\n",
            "Iteration 8457, loss = 0.80808680\n",
            "Iteration 8458, loss = 0.80804786\n",
            "Iteration 8459, loss = 0.80800735\n",
            "Iteration 8460, loss = 0.80796810\n",
            "Iteration 8461, loss = 0.80792852\n",
            "Iteration 8462, loss = 0.80788984\n",
            "Iteration 8463, loss = 0.80785130\n",
            "Iteration 8464, loss = 0.80781177\n",
            "Iteration 8465, loss = 0.80777221\n",
            "Iteration 8466, loss = 0.80773433\n",
            "Iteration 8467, loss = 0.80769706\n",
            "Iteration 8468, loss = 0.80765880\n",
            "Iteration 8469, loss = 0.80761852\n",
            "Iteration 8470, loss = 0.80758062\n",
            "Iteration 8471, loss = 0.80754261\n",
            "Iteration 8472, loss = 0.80750359\n",
            "Iteration 8473, loss = 0.80746429\n",
            "Iteration 8474, loss = 0.80742411\n",
            "Iteration 8475, loss = 0.80738520\n",
            "Iteration 8476, loss = 0.80734747\n",
            "Iteration 8477, loss = 0.80730841\n",
            "Iteration 8478, loss = 0.80726846\n",
            "Iteration 8479, loss = 0.80723001\n",
            "Iteration 8480, loss = 0.80719045\n",
            "Iteration 8481, loss = 0.80715345\n",
            "Iteration 8482, loss = 0.80711352\n",
            "Iteration 8483, loss = 0.80707492\n",
            "Iteration 8484, loss = 0.80703523\n",
            "Iteration 8485, loss = 0.80699681\n",
            "Iteration 8486, loss = 0.80695785\n",
            "Iteration 8487, loss = 0.80691951\n",
            "Iteration 8488, loss = 0.80688095\n",
            "Iteration 8489, loss = 0.80684557\n",
            "Iteration 8490, loss = 0.80680336\n",
            "Iteration 8491, loss = 0.80676771\n",
            "Iteration 8492, loss = 0.80672845\n",
            "Iteration 8493, loss = 0.80668977\n",
            "Iteration 8494, loss = 0.80665201\n",
            "Iteration 8495, loss = 0.80661436\n",
            "Iteration 8496, loss = 0.80657527\n",
            "Iteration 8497, loss = 0.80653558\n",
            "Iteration 8498, loss = 0.80649657\n",
            "Iteration 8499, loss = 0.80645713\n",
            "Iteration 8500, loss = 0.80641883\n",
            "Iteration 8501, loss = 0.80638173\n",
            "Iteration 8502, loss = 0.80634477\n",
            "Iteration 8503, loss = 0.80630556\n",
            "Iteration 8504, loss = 0.80626613\n",
            "Iteration 8505, loss = 0.80622668\n",
            "Iteration 8506, loss = 0.80619034\n",
            "Iteration 8507, loss = 0.80615271\n",
            "Iteration 8508, loss = 0.80611349\n",
            "Iteration 8509, loss = 0.80607675\n",
            "Iteration 8510, loss = 0.80603729\n",
            "Iteration 8511, loss = 0.80599757\n",
            "Iteration 8512, loss = 0.80596061\n",
            "Iteration 8513, loss = 0.80592270\n",
            "Iteration 8514, loss = 0.80588346\n",
            "Iteration 8515, loss = 0.80584534\n",
            "Iteration 8516, loss = 0.80580676\n",
            "Iteration 8517, loss = 0.80576922\n",
            "Iteration 8518, loss = 0.80573065\n",
            "Iteration 8519, loss = 0.80569309\n",
            "Iteration 8520, loss = 0.80565532\n",
            "Iteration 8521, loss = 0.80561713\n",
            "Iteration 8522, loss = 0.80557841\n",
            "Iteration 8523, loss = 0.80553885\n",
            "Iteration 8524, loss = 0.80550029\n",
            "Iteration 8525, loss = 0.80546314\n",
            "Iteration 8526, loss = 0.80542586\n",
            "Iteration 8527, loss = 0.80538617\n",
            "Iteration 8528, loss = 0.80534752\n",
            "Iteration 8529, loss = 0.80531387\n",
            "Iteration 8530, loss = 0.80527277\n",
            "Iteration 8531, loss = 0.80523155\n",
            "Iteration 8532, loss = 0.80519799\n",
            "Iteration 8533, loss = 0.80515861\n",
            "Iteration 8534, loss = 0.80512002\n",
            "Iteration 8535, loss = 0.80508327\n",
            "Iteration 8536, loss = 0.80504600\n",
            "Iteration 8537, loss = 0.80500774\n",
            "Iteration 8538, loss = 0.80497151\n",
            "Iteration 8539, loss = 0.80493405\n",
            "Iteration 8540, loss = 0.80489479\n",
            "Iteration 8541, loss = 0.80485633\n",
            "Iteration 8542, loss = 0.80481882\n",
            "Iteration 8543, loss = 0.80478050\n",
            "Iteration 8544, loss = 0.80474130\n",
            "Iteration 8545, loss = 0.80470223\n",
            "Iteration 8546, loss = 0.80466605\n",
            "Iteration 8547, loss = 0.80462702\n",
            "Iteration 8548, loss = 0.80459309\n",
            "Iteration 8549, loss = 0.80455397\n",
            "Iteration 8550, loss = 0.80451371\n",
            "Iteration 8551, loss = 0.80447765\n",
            "Iteration 8552, loss = 0.80443818\n",
            "Iteration 8553, loss = 0.80440053\n",
            "Iteration 8554, loss = 0.80436334\n",
            "Iteration 8555, loss = 0.80432600\n",
            "Iteration 8556, loss = 0.80428789\n",
            "Iteration 8557, loss = 0.80425097\n",
            "Iteration 8558, loss = 0.80421271\n",
            "Iteration 8559, loss = 0.80417624\n",
            "Iteration 8560, loss = 0.80413854\n",
            "Iteration 8561, loss = 0.80409948\n",
            "Iteration 8562, loss = 0.80406198\n",
            "Iteration 8563, loss = 0.80402356\n",
            "Iteration 8564, loss = 0.80398584\n",
            "Iteration 8565, loss = 0.80394778\n",
            "Iteration 8566, loss = 0.80391150\n",
            "Iteration 8567, loss = 0.80387327\n",
            "Iteration 8568, loss = 0.80383617\n",
            "Iteration 8569, loss = 0.80379812\n",
            "Iteration 8570, loss = 0.80376178\n",
            "Iteration 8571, loss = 0.80372447\n",
            "Iteration 8572, loss = 0.80368937\n",
            "Iteration 8573, loss = 0.80365284\n",
            "Iteration 8574, loss = 0.80361347\n",
            "Iteration 8575, loss = 0.80357631\n",
            "Iteration 8576, loss = 0.80353958\n",
            "Iteration 8577, loss = 0.80350204\n",
            "Iteration 8578, loss = 0.80346367\n",
            "Iteration 8579, loss = 0.80342608\n",
            "Iteration 8580, loss = 0.80338875\n",
            "Iteration 8581, loss = 0.80335064\n",
            "Iteration 8582, loss = 0.80331383\n",
            "Iteration 8583, loss = 0.80327847\n",
            "Iteration 8584, loss = 0.80323925\n",
            "Iteration 8585, loss = 0.80320300\n",
            "Iteration 8586, loss = 0.80316536\n",
            "Iteration 8587, loss = 0.80312890\n",
            "Iteration 8588, loss = 0.80309312\n",
            "Iteration 8589, loss = 0.80305636\n",
            "Iteration 8590, loss = 0.80301867\n",
            "Iteration 8591, loss = 0.80298187\n",
            "Iteration 8592, loss = 0.80294489\n",
            "Iteration 8593, loss = 0.80290771\n",
            "Iteration 8594, loss = 0.80287066\n",
            "Iteration 8595, loss = 0.80283346\n",
            "Iteration 8596, loss = 0.80279565\n",
            "Iteration 8597, loss = 0.80275823\n",
            "Iteration 8598, loss = 0.80272040\n",
            "Iteration 8599, loss = 0.80268324\n",
            "Iteration 8600, loss = 0.80264686\n",
            "Iteration 8601, loss = 0.80260909\n",
            "Iteration 8602, loss = 0.80257330\n",
            "Iteration 8603, loss = 0.80253521\n",
            "Iteration 8604, loss = 0.80249875\n",
            "Iteration 8605, loss = 0.80246216\n",
            "Iteration 8606, loss = 0.80242341\n",
            "Iteration 8607, loss = 0.80238759\n",
            "Iteration 8608, loss = 0.80235173\n",
            "Iteration 8609, loss = 0.80231568\n",
            "Iteration 8610, loss = 0.80227674\n",
            "Iteration 8611, loss = 0.80224182\n",
            "Iteration 8612, loss = 0.80220666\n",
            "Iteration 8613, loss = 0.80217009\n",
            "Iteration 8614, loss = 0.80213292\n",
            "Iteration 8615, loss = 0.80209518\n",
            "Iteration 8616, loss = 0.80205843\n",
            "Iteration 8617, loss = 0.80202044\n",
            "Iteration 8618, loss = 0.80198434\n",
            "Iteration 8619, loss = 0.80194872\n",
            "Iteration 8620, loss = 0.80191126\n",
            "Iteration 8621, loss = 0.80187336\n",
            "Iteration 8622, loss = 0.80183700\n",
            "Iteration 8623, loss = 0.80180149\n",
            "Iteration 8624, loss = 0.80176399\n",
            "Iteration 8625, loss = 0.80172714\n",
            "Iteration 8626, loss = 0.80169141\n",
            "Iteration 8627, loss = 0.80165471\n",
            "Iteration 8628, loss = 0.80161855\n",
            "Iteration 8629, loss = 0.80158149\n",
            "Iteration 8630, loss = 0.80154378\n",
            "Iteration 8631, loss = 0.80150679\n",
            "Iteration 8632, loss = 0.80147077\n",
            "Iteration 8633, loss = 0.80143323\n",
            "Iteration 8634, loss = 0.80139690\n",
            "Iteration 8635, loss = 0.80135915\n",
            "Iteration 8636, loss = 0.80132333\n",
            "Iteration 8637, loss = 0.80128678\n",
            "Iteration 8638, loss = 0.80125146\n",
            "Iteration 8639, loss = 0.80121491\n",
            "Iteration 8640, loss = 0.80117867\n",
            "Iteration 8641, loss = 0.80114019\n",
            "Iteration 8642, loss = 0.80110390\n",
            "Iteration 8643, loss = 0.80106840\n",
            "Iteration 8644, loss = 0.80103299\n",
            "Iteration 8645, loss = 0.80099625\n",
            "Iteration 8646, loss = 0.80095881\n",
            "Iteration 8647, loss = 0.80092345\n",
            "Iteration 8648, loss = 0.80088626\n",
            "Iteration 8649, loss = 0.80085041\n",
            "Iteration 8650, loss = 0.80081316\n",
            "Iteration 8651, loss = 0.80077717\n",
            "Iteration 8652, loss = 0.80074097\n",
            "Iteration 8653, loss = 0.80070474\n",
            "Iteration 8654, loss = 0.80066720\n",
            "Iteration 8655, loss = 0.80063266\n",
            "Iteration 8656, loss = 0.80059984\n",
            "Iteration 8657, loss = 0.80055957\n",
            "Iteration 8658, loss = 0.80052603\n",
            "Iteration 8659, loss = 0.80048751\n",
            "Iteration 8660, loss = 0.80045121\n",
            "Iteration 8661, loss = 0.80041546\n",
            "Iteration 8662, loss = 0.80037973\n",
            "Iteration 8663, loss = 0.80034278\n",
            "Iteration 8664, loss = 0.80030684\n",
            "Iteration 8665, loss = 0.80027105\n",
            "Iteration 8666, loss = 0.80023458\n",
            "Iteration 8667, loss = 0.80019682\n",
            "Iteration 8668, loss = 0.80016272\n",
            "Iteration 8669, loss = 0.80012718\n",
            "Iteration 8670, loss = 0.80008890\n",
            "Iteration 8671, loss = 0.80005324\n",
            "Iteration 8672, loss = 0.80001845\n",
            "Iteration 8673, loss = 0.79998229\n",
            "Iteration 8674, loss = 0.79994581\n",
            "Iteration 8675, loss = 0.79990982\n",
            "Iteration 8676, loss = 0.79987347\n",
            "Iteration 8677, loss = 0.79983757\n",
            "Iteration 8678, loss = 0.79980364\n",
            "Iteration 8679, loss = 0.79976455\n",
            "Iteration 8680, loss = 0.79972928\n",
            "Iteration 8681, loss = 0.79969345\n",
            "Iteration 8682, loss = 0.79965767\n",
            "Iteration 8683, loss = 0.79962190\n",
            "Iteration 8684, loss = 0.79958581\n",
            "Iteration 8685, loss = 0.79954913\n",
            "Iteration 8686, loss = 0.79951582\n",
            "Iteration 8687, loss = 0.79947736\n",
            "Iteration 8688, loss = 0.79944179\n",
            "Iteration 8689, loss = 0.79940812\n",
            "Iteration 8690, loss = 0.79936956\n",
            "Iteration 8691, loss = 0.79933435\n",
            "Iteration 8692, loss = 0.79930016\n",
            "Iteration 8693, loss = 0.79926478\n",
            "Iteration 8694, loss = 0.79923003\n",
            "Iteration 8695, loss = 0.79919164\n",
            "Iteration 8696, loss = 0.79915803\n",
            "Iteration 8697, loss = 0.79912190\n",
            "Iteration 8698, loss = 0.79908472\n",
            "Iteration 8699, loss = 0.79904859\n",
            "Iteration 8700, loss = 0.79901333\n",
            "Iteration 8701, loss = 0.79897733\n",
            "Iteration 8702, loss = 0.79894048\n",
            "Iteration 8703, loss = 0.79890336\n",
            "Iteration 8704, loss = 0.79886775\n",
            "Iteration 8705, loss = 0.79883243\n",
            "Iteration 8706, loss = 0.79879597\n",
            "Iteration 8707, loss = 0.79876016\n",
            "Iteration 8708, loss = 0.79872354\n",
            "Iteration 8709, loss = 0.79868878\n",
            "Iteration 8710, loss = 0.79865271\n",
            "Iteration 8711, loss = 0.79861720\n",
            "Iteration 8712, loss = 0.79858499\n",
            "Iteration 8713, loss = 0.79854780\n",
            "Iteration 8714, loss = 0.79851229\n",
            "Iteration 8715, loss = 0.79847739\n",
            "Iteration 8716, loss = 0.79844146\n",
            "Iteration 8717, loss = 0.79840506\n",
            "Iteration 8718, loss = 0.79836756\n",
            "Iteration 8719, loss = 0.79833393\n",
            "Iteration 8720, loss = 0.79829854\n",
            "Iteration 8721, loss = 0.79826138\n",
            "Iteration 8722, loss = 0.79822365\n",
            "Iteration 8723, loss = 0.79818862\n",
            "Iteration 8724, loss = 0.79815271\n",
            "Iteration 8725, loss = 0.79811731\n",
            "Iteration 8726, loss = 0.79808251\n",
            "Iteration 8727, loss = 0.79804635\n",
            "Iteration 8728, loss = 0.79801182\n",
            "Iteration 8729, loss = 0.79797556\n",
            "Iteration 8730, loss = 0.79794101\n",
            "Iteration 8731, loss = 0.79790700\n",
            "Iteration 8732, loss = 0.79787184\n",
            "Iteration 8733, loss = 0.79783573\n",
            "Iteration 8734, loss = 0.79779894\n",
            "Iteration 8735, loss = 0.79776296\n",
            "Iteration 8736, loss = 0.79772747\n",
            "Iteration 8737, loss = 0.79769347\n",
            "Iteration 8738, loss = 0.79765682\n",
            "Iteration 8739, loss = 0.79762165\n",
            "Iteration 8740, loss = 0.79758601\n",
            "Iteration 8741, loss = 0.79755143\n",
            "Iteration 8742, loss = 0.79751642\n",
            "Iteration 8743, loss = 0.79748143\n",
            "Iteration 8744, loss = 0.79744558\n",
            "Iteration 8745, loss = 0.79741091\n",
            "Iteration 8746, loss = 0.79737522\n",
            "Iteration 8747, loss = 0.79734054\n",
            "Iteration 8748, loss = 0.79730592\n",
            "Iteration 8749, loss = 0.79727119\n",
            "Iteration 8750, loss = 0.79723570\n",
            "Iteration 8751, loss = 0.79719903\n",
            "Iteration 8752, loss = 0.79716561\n",
            "Iteration 8753, loss = 0.79713029\n",
            "Iteration 8754, loss = 0.79709502\n",
            "Iteration 8755, loss = 0.79705815\n",
            "Iteration 8756, loss = 0.79702608\n",
            "Iteration 8757, loss = 0.79699023\n",
            "Iteration 8758, loss = 0.79695561\n",
            "Iteration 8759, loss = 0.79692186\n",
            "Iteration 8760, loss = 0.79688753\n",
            "Iteration 8761, loss = 0.79685183\n",
            "Iteration 8762, loss = 0.79681633\n",
            "Iteration 8763, loss = 0.79678335\n",
            "Iteration 8764, loss = 0.79674661\n",
            "Iteration 8765, loss = 0.79671097\n",
            "Iteration 8766, loss = 0.79667690\n",
            "Iteration 8767, loss = 0.79664156\n",
            "Iteration 8768, loss = 0.79660536\n",
            "Iteration 8769, loss = 0.79656981\n",
            "Iteration 8770, loss = 0.79653380\n",
            "Iteration 8771, loss = 0.79649887\n",
            "Iteration 8772, loss = 0.79646941\n",
            "Iteration 8773, loss = 0.79643190\n",
            "Iteration 8774, loss = 0.79639663\n",
            "Iteration 8775, loss = 0.79636152\n",
            "Iteration 8776, loss = 0.79632507\n",
            "Iteration 8777, loss = 0.79629201\n",
            "Iteration 8778, loss = 0.79625918\n",
            "Iteration 8779, loss = 0.79622549\n",
            "Iteration 8780, loss = 0.79619192\n",
            "Iteration 8781, loss = 0.79615652\n",
            "Iteration 8782, loss = 0.79611974\n",
            "Iteration 8783, loss = 0.79608481\n",
            "Iteration 8784, loss = 0.79604871\n",
            "Iteration 8785, loss = 0.79601473\n",
            "Iteration 8786, loss = 0.79598054\n",
            "Iteration 8787, loss = 0.79594801\n",
            "Iteration 8788, loss = 0.79591074\n",
            "Iteration 8789, loss = 0.79587320\n",
            "Iteration 8790, loss = 0.79583857\n",
            "Iteration 8791, loss = 0.79580411\n",
            "Iteration 8792, loss = 0.79576914\n",
            "Iteration 8793, loss = 0.79573431\n",
            "Iteration 8794, loss = 0.79569939\n",
            "Iteration 8795, loss = 0.79566526\n",
            "Iteration 8796, loss = 0.79563080\n",
            "Iteration 8797, loss = 0.79559591\n",
            "Iteration 8798, loss = 0.79556070\n",
            "Iteration 8799, loss = 0.79552601\n",
            "Iteration 8800, loss = 0.79549069\n",
            "Iteration 8801, loss = 0.79545549\n",
            "Iteration 8802, loss = 0.79542002\n",
            "Iteration 8803, loss = 0.79538769\n",
            "Iteration 8804, loss = 0.79535206\n",
            "Iteration 8805, loss = 0.79531500\n",
            "Iteration 8806, loss = 0.79528376\n",
            "Iteration 8807, loss = 0.79525089\n",
            "Iteration 8808, loss = 0.79521637\n",
            "Iteration 8809, loss = 0.79518059\n",
            "Iteration 8810, loss = 0.79514643\n",
            "Iteration 8811, loss = 0.79511160\n",
            "Iteration 8812, loss = 0.79507815\n",
            "Iteration 8813, loss = 0.79504290\n",
            "Iteration 8814, loss = 0.79500819\n",
            "Iteration 8815, loss = 0.79497413\n",
            "Iteration 8816, loss = 0.79493876\n",
            "Iteration 8817, loss = 0.79490404\n",
            "Iteration 8818, loss = 0.79487192\n",
            "Iteration 8819, loss = 0.79483504\n",
            "Iteration 8820, loss = 0.79480100\n",
            "Iteration 8821, loss = 0.79476643\n",
            "Iteration 8822, loss = 0.79473268\n",
            "Iteration 8823, loss = 0.79469920\n",
            "Iteration 8824, loss = 0.79466403\n",
            "Iteration 8825, loss = 0.79463010\n",
            "Iteration 8826, loss = 0.79459551\n",
            "Iteration 8827, loss = 0.79456101\n",
            "Iteration 8828, loss = 0.79452766\n",
            "Iteration 8829, loss = 0.79449202\n",
            "Iteration 8830, loss = 0.79445849\n",
            "Iteration 8831, loss = 0.79442452\n",
            "Iteration 8832, loss = 0.79438947\n",
            "Iteration 8833, loss = 0.79435449\n",
            "Iteration 8834, loss = 0.79431852\n",
            "Iteration 8835, loss = 0.79428670\n",
            "Iteration 8836, loss = 0.79425129\n",
            "Iteration 8837, loss = 0.79421615\n",
            "Iteration 8838, loss = 0.79418379\n",
            "Iteration 8839, loss = 0.79414800\n",
            "Iteration 8840, loss = 0.79411345\n",
            "Iteration 8841, loss = 0.79408077\n",
            "Iteration 8842, loss = 0.79404592\n",
            "Iteration 8843, loss = 0.79401130\n",
            "Iteration 8844, loss = 0.79397749\n",
            "Iteration 8845, loss = 0.79394290\n",
            "Iteration 8846, loss = 0.79390812\n",
            "Iteration 8847, loss = 0.79387483\n",
            "Iteration 8848, loss = 0.79384097\n",
            "Iteration 8849, loss = 0.79380623\n",
            "Iteration 8850, loss = 0.79377205\n",
            "Iteration 8851, loss = 0.79373845\n",
            "Iteration 8852, loss = 0.79370373\n",
            "Iteration 8853, loss = 0.79366991\n",
            "Iteration 8854, loss = 0.79363634\n",
            "Iteration 8855, loss = 0.79360153\n",
            "Iteration 8856, loss = 0.79356889\n",
            "Iteration 8857, loss = 0.79353336\n",
            "Iteration 8858, loss = 0.79350148\n",
            "Iteration 8859, loss = 0.79346979\n",
            "Iteration 8860, loss = 0.79343440\n",
            "Iteration 8861, loss = 0.79339966\n",
            "Iteration 8862, loss = 0.79336759\n",
            "Iteration 8863, loss = 0.79333330\n",
            "Iteration 8864, loss = 0.79329684\n",
            "Iteration 8865, loss = 0.79326482\n",
            "Iteration 8866, loss = 0.79323152\n",
            "Iteration 8867, loss = 0.79319745\n",
            "Iteration 8868, loss = 0.79316258\n",
            "Iteration 8869, loss = 0.79312713\n",
            "Iteration 8870, loss = 0.79309416\n",
            "Iteration 8871, loss = 0.79306167\n",
            "Iteration 8872, loss = 0.79302681\n",
            "Iteration 8873, loss = 0.79299048\n",
            "Iteration 8874, loss = 0.79295736\n",
            "Iteration 8875, loss = 0.79292405\n",
            "Iteration 8876, loss = 0.79289034\n",
            "Iteration 8877, loss = 0.79285944\n",
            "Iteration 8878, loss = 0.79282412\n",
            "Iteration 8879, loss = 0.79279003\n",
            "Iteration 8880, loss = 0.79275663\n",
            "Iteration 8881, loss = 0.79272213\n",
            "Iteration 8882, loss = 0.79268961\n",
            "Iteration 8883, loss = 0.79265410\n",
            "Iteration 8884, loss = 0.79262064\n",
            "Iteration 8885, loss = 0.79258630\n",
            "Iteration 8886, loss = 0.79255576\n",
            "Iteration 8887, loss = 0.79251963\n",
            "Iteration 8888, loss = 0.79248756\n",
            "Iteration 8889, loss = 0.79245462\n",
            "Iteration 8890, loss = 0.79241922\n",
            "Iteration 8891, loss = 0.79238740\n",
            "Iteration 8892, loss = 0.79235482\n",
            "Iteration 8893, loss = 0.79232099\n",
            "Iteration 8894, loss = 0.79228679\n",
            "Iteration 8895, loss = 0.79225210\n",
            "Iteration 8896, loss = 0.79221835\n",
            "Iteration 8897, loss = 0.79218426\n",
            "Iteration 8898, loss = 0.79214726\n",
            "Iteration 8899, loss = 0.79211246\n",
            "Iteration 8900, loss = 0.79207786\n",
            "Iteration 8901, loss = 0.79204424\n",
            "Iteration 8902, loss = 0.79200734\n",
            "Iteration 8903, loss = 0.79197103\n",
            "Iteration 8904, loss = 0.79193737\n",
            "Iteration 8905, loss = 0.79190326\n",
            "Iteration 8906, loss = 0.79186843\n",
            "Iteration 8907, loss = 0.79183771\n",
            "Iteration 8908, loss = 0.79180009\n",
            "Iteration 8909, loss = 0.79176464\n",
            "Iteration 8910, loss = 0.79173161\n",
            "Iteration 8911, loss = 0.79169864\n",
            "Iteration 8912, loss = 0.79166200\n",
            "Iteration 8913, loss = 0.79162806\n",
            "Iteration 8914, loss = 0.79159366\n",
            "Iteration 8915, loss = 0.79155870\n",
            "Iteration 8916, loss = 0.79152731\n",
            "Iteration 8917, loss = 0.79149456\n",
            "Iteration 8918, loss = 0.79145765\n",
            "Iteration 8919, loss = 0.79142410\n",
            "Iteration 8920, loss = 0.79139186\n",
            "Iteration 8921, loss = 0.79135934\n",
            "Iteration 8922, loss = 0.79132549\n",
            "Iteration 8923, loss = 0.79129029\n",
            "Iteration 8924, loss = 0.79125609\n",
            "Iteration 8925, loss = 0.79122259\n",
            "Iteration 8926, loss = 0.79118935\n",
            "Iteration 8927, loss = 0.79115369\n",
            "Iteration 8928, loss = 0.79112049\n",
            "Iteration 8929, loss = 0.79108689\n",
            "Iteration 8930, loss = 0.79105229\n",
            "Iteration 8931, loss = 0.79101920\n",
            "Iteration 8932, loss = 0.79098445\n",
            "Iteration 8933, loss = 0.79094791\n",
            "Iteration 8934, loss = 0.79091420\n",
            "Iteration 8935, loss = 0.79087973\n",
            "Iteration 8936, loss = 0.79084807\n",
            "Iteration 8937, loss = 0.79081502\n",
            "Iteration 8938, loss = 0.79077952\n",
            "Iteration 8939, loss = 0.79074637\n",
            "Iteration 8940, loss = 0.79071363\n",
            "Iteration 8941, loss = 0.79067989\n",
            "Iteration 8942, loss = 0.79064573\n",
            "Iteration 8943, loss = 0.79061168\n",
            "Iteration 8944, loss = 0.79057646\n",
            "Iteration 8945, loss = 0.79054087\n",
            "Iteration 8946, loss = 0.79050580\n",
            "Iteration 8947, loss = 0.79047180\n",
            "Iteration 8948, loss = 0.79043627\n",
            "Iteration 8949, loss = 0.79040180\n",
            "Iteration 8950, loss = 0.79036944\n",
            "Iteration 8951, loss = 0.79033555\n",
            "Iteration 8952, loss = 0.79030109\n",
            "Iteration 8953, loss = 0.79026829\n",
            "Iteration 8954, loss = 0.79023424\n",
            "Iteration 8955, loss = 0.79020061\n",
            "Iteration 8956, loss = 0.79016645\n",
            "Iteration 8957, loss = 0.79013257\n",
            "Iteration 8958, loss = 0.79009942\n",
            "Iteration 8959, loss = 0.79006674\n",
            "Iteration 8960, loss = 0.79003174\n",
            "Iteration 8961, loss = 0.78999895\n",
            "Iteration 8962, loss = 0.78996587\n",
            "Iteration 8963, loss = 0.78993193\n",
            "Iteration 8964, loss = 0.78989924\n",
            "Iteration 8965, loss = 0.78986243\n",
            "Iteration 8966, loss = 0.78982848\n",
            "Iteration 8967, loss = 0.78979422\n",
            "Iteration 8968, loss = 0.78976060\n",
            "Iteration 8969, loss = 0.78972800\n",
            "Iteration 8970, loss = 0.78969466\n",
            "Iteration 8971, loss = 0.78965950\n",
            "Iteration 8972, loss = 0.78962870\n",
            "Iteration 8973, loss = 0.78959545\n",
            "Iteration 8974, loss = 0.78956058\n",
            "Iteration 8975, loss = 0.78952598\n",
            "Iteration 8976, loss = 0.78949364\n",
            "Iteration 8977, loss = 0.78946002\n",
            "Iteration 8978, loss = 0.78942639\n",
            "Iteration 8979, loss = 0.78939234\n",
            "Iteration 8980, loss = 0.78935971\n",
            "Iteration 8981, loss = 0.78932304\n",
            "Iteration 8982, loss = 0.78929305\n",
            "Iteration 8983, loss = 0.78926097\n",
            "Iteration 8984, loss = 0.78922634\n",
            "Iteration 8985, loss = 0.78918933\n",
            "Iteration 8986, loss = 0.78915728\n",
            "Iteration 8987, loss = 0.78912255\n",
            "Iteration 8988, loss = 0.78909064\n",
            "Iteration 8989, loss = 0.78905624\n",
            "Iteration 8990, loss = 0.78902328\n",
            "Iteration 8991, loss = 0.78898991\n",
            "Iteration 8992, loss = 0.78895586\n",
            "Iteration 8993, loss = 0.78892241\n",
            "Iteration 8994, loss = 0.78888833\n",
            "Iteration 8995, loss = 0.78885585\n",
            "Iteration 8996, loss = 0.78882345\n",
            "Iteration 8997, loss = 0.78878913\n",
            "Iteration 8998, loss = 0.78875458\n",
            "Iteration 8999, loss = 0.78872268\n",
            "Iteration 9000, loss = 0.78869050\n",
            "Iteration 9001, loss = 0.78865988\n",
            "Iteration 9002, loss = 0.78862603\n",
            "Iteration 9003, loss = 0.78859011\n",
            "Iteration 9004, loss = 0.78855686\n",
            "Iteration 9005, loss = 0.78852615\n",
            "Iteration 9006, loss = 0.78849319\n",
            "Iteration 9007, loss = 0.78845792\n",
            "Iteration 9008, loss = 0.78842457\n",
            "Iteration 9009, loss = 0.78839232\n",
            "Iteration 9010, loss = 0.78835876\n",
            "Iteration 9011, loss = 0.78832493\n",
            "Iteration 9012, loss = 0.78829242\n",
            "Iteration 9013, loss = 0.78825778\n",
            "Iteration 9014, loss = 0.78822753\n",
            "Iteration 9015, loss = 0.78819301\n",
            "Iteration 9016, loss = 0.78815736\n",
            "Iteration 9017, loss = 0.78812585\n",
            "Iteration 9018, loss = 0.78809267\n",
            "Iteration 9019, loss = 0.78805996\n",
            "Iteration 9020, loss = 0.78802598\n",
            "Iteration 9021, loss = 0.78799549\n",
            "Iteration 9022, loss = 0.78796159\n",
            "Iteration 9023, loss = 0.78792780\n",
            "Iteration 9024, loss = 0.78789619\n",
            "Iteration 9025, loss = 0.78786368\n",
            "Iteration 9026, loss = 0.78783024\n",
            "Iteration 9027, loss = 0.78779737\n",
            "Iteration 9028, loss = 0.78776280\n",
            "Iteration 9029, loss = 0.78773351\n",
            "Iteration 9030, loss = 0.78770037\n",
            "Iteration 9031, loss = 0.78766346\n",
            "Iteration 9032, loss = 0.78762971\n",
            "Iteration 9033, loss = 0.78759853\n",
            "Iteration 9034, loss = 0.78756896\n",
            "Iteration 9035, loss = 0.78753260\n",
            "Iteration 9036, loss = 0.78749923\n",
            "Iteration 9037, loss = 0.78746580\n",
            "Iteration 9038, loss = 0.78743584\n",
            "Iteration 9039, loss = 0.78740318\n",
            "Iteration 9040, loss = 0.78736822\n",
            "Iteration 9041, loss = 0.78733363\n",
            "Iteration 9042, loss = 0.78730113\n",
            "Iteration 9043, loss = 0.78727215\n",
            "Iteration 9044, loss = 0.78723590\n",
            "Iteration 9045, loss = 0.78720131\n",
            "Iteration 9046, loss = 0.78717326\n",
            "Iteration 9047, loss = 0.78714222\n",
            "Iteration 9048, loss = 0.78710873\n",
            "Iteration 9049, loss = 0.78707549\n",
            "Iteration 9050, loss = 0.78704373\n",
            "Iteration 9051, loss = 0.78701110\n",
            "Iteration 9052, loss = 0.78697750\n",
            "Iteration 9053, loss = 0.78694380\n",
            "Iteration 9054, loss = 0.78690981\n",
            "Iteration 9055, loss = 0.78687547\n",
            "Iteration 9056, loss = 0.78684539\n",
            "Iteration 9057, loss = 0.78681262\n",
            "Iteration 9058, loss = 0.78677778\n",
            "Iteration 9059, loss = 0.78674636\n",
            "Iteration 9060, loss = 0.78671669\n",
            "Iteration 9061, loss = 0.78668026\n",
            "Iteration 9062, loss = 0.78664961\n",
            "Iteration 9063, loss = 0.78662058\n",
            "Iteration 9064, loss = 0.78658751\n",
            "Iteration 9065, loss = 0.78655147\n",
            "Iteration 9066, loss = 0.78652081\n",
            "Iteration 9067, loss = 0.78648966\n",
            "Iteration 9068, loss = 0.78645763\n",
            "Iteration 9069, loss = 0.78642435\n",
            "Iteration 9070, loss = 0.78639037\n",
            "Iteration 9071, loss = 0.78635957\n",
            "Iteration 9072, loss = 0.78632703\n",
            "Iteration 9073, loss = 0.78629194\n",
            "Iteration 9074, loss = 0.78625801\n",
            "Iteration 9075, loss = 0.78622549\n",
            "Iteration 9076, loss = 0.78619661\n",
            "Iteration 9077, loss = 0.78616044\n",
            "Iteration 9078, loss = 0.78612797\n",
            "Iteration 9079, loss = 0.78609848\n",
            "Iteration 9080, loss = 0.78606595\n",
            "Iteration 9081, loss = 0.78603214\n",
            "Iteration 9082, loss = 0.78600156\n",
            "Iteration 9083, loss = 0.78597109\n",
            "Iteration 9084, loss = 0.78593951\n",
            "Iteration 9085, loss = 0.78590029\n",
            "Iteration 9086, loss = 0.78579052\n",
            "Iteration 9087, loss = 0.78565209\n",
            "Iteration 9088, loss = 0.78549698\n",
            "Iteration 9089, loss = 0.78533465\n",
            "Iteration 9090, loss = 0.78516369\n",
            "Iteration 9091, loss = 0.78498802\n",
            "Iteration 9092, loss = 0.78481266\n",
            "Iteration 9093, loss = 0.78463433\n",
            "Iteration 9094, loss = 0.78445726\n",
            "Iteration 9095, loss = 0.78427736\n",
            "Iteration 9096, loss = 0.78409998\n",
            "Iteration 9097, loss = 0.78392224\n",
            "Iteration 9098, loss = 0.78374802\n",
            "Iteration 9099, loss = 0.78357648\n",
            "Iteration 9100, loss = 0.78340758\n",
            "Iteration 9101, loss = 0.78324132\n",
            "Iteration 9102, loss = 0.78307824\n",
            "Iteration 9103, loss = 0.78291882\n",
            "Iteration 9104, loss = 0.78276403\n",
            "Iteration 9105, loss = 0.78264340\n",
            "Iteration 9106, loss = 0.78256716\n",
            "Iteration 9107, loss = 0.78249234\n",
            "Iteration 9108, loss = 0.78242395\n",
            "Iteration 9109, loss = 0.78235230\n",
            "Iteration 9110, loss = 0.78228543\n",
            "Iteration 9111, loss = 0.78222102\n",
            "Iteration 9112, loss = 0.78215820\n",
            "Iteration 9113, loss = 0.78209658\n",
            "Iteration 9114, loss = 0.78203647\n",
            "Iteration 9115, loss = 0.78197740\n",
            "Iteration 9116, loss = 0.78191899\n",
            "Iteration 9117, loss = 0.78186385\n",
            "Iteration 9118, loss = 0.78180717\n",
            "Iteration 9119, loss = 0.78175569\n",
            "Iteration 9120, loss = 0.78170209\n",
            "Iteration 9121, loss = 0.78164941\n",
            "Iteration 9122, loss = 0.78159789\n",
            "Iteration 9123, loss = 0.78155120\n",
            "Iteration 9124, loss = 0.78150188\n",
            "Iteration 9125, loss = 0.78145173\n",
            "Iteration 9126, loss = 0.78140373\n",
            "Iteration 9127, loss = 0.78135773\n",
            "Iteration 9128, loss = 0.78131150\n",
            "Iteration 9129, loss = 0.78126520\n",
            "Iteration 9130, loss = 0.78122080\n",
            "Iteration 9131, loss = 0.78117717\n",
            "Iteration 9132, loss = 0.78113308\n",
            "Iteration 9133, loss = 0.78108951\n",
            "Iteration 9134, loss = 0.78104580\n",
            "Iteration 9135, loss = 0.78100257\n",
            "Iteration 9136, loss = 0.78096230\n",
            "Iteration 9137, loss = 0.78091666\n",
            "Iteration 9138, loss = 0.78087602\n",
            "Iteration 9139, loss = 0.78083439\n",
            "Iteration 9140, loss = 0.78079454\n",
            "Iteration 9141, loss = 0.78075163\n",
            "Iteration 9142, loss = 0.78070798\n",
            "Iteration 9143, loss = 0.78066670\n",
            "Iteration 9144, loss = 0.78062618\n",
            "Iteration 9145, loss = 0.78058634\n",
            "Iteration 9146, loss = 0.78054579\n",
            "Iteration 9147, loss = 0.78050339\n",
            "Iteration 9148, loss = 0.78046290\n",
            "Iteration 9149, loss = 0.78042241\n",
            "Iteration 9150, loss = 0.78038032\n",
            "Iteration 9151, loss = 0.78034176\n",
            "Iteration 9152, loss = 0.78030299\n",
            "Iteration 9153, loss = 0.78026076\n",
            "Iteration 9154, loss = 0.78022116\n",
            "Iteration 9155, loss = 0.78018164\n",
            "Iteration 9156, loss = 0.78014289\n",
            "Iteration 9157, loss = 0.78010368\n",
            "Iteration 9158, loss = 0.78006536\n",
            "Iteration 9159, loss = 0.78002458\n",
            "Iteration 9160, loss = 0.77998749\n",
            "Iteration 9161, loss = 0.77994883\n",
            "Iteration 9162, loss = 0.77990896\n",
            "Iteration 9163, loss = 0.77986955\n",
            "Iteration 9164, loss = 0.77983042\n",
            "Iteration 9165, loss = 0.77979095\n",
            "Iteration 9166, loss = 0.77975094\n",
            "Iteration 9167, loss = 0.77971354\n",
            "Iteration 9168, loss = 0.77967760\n",
            "Iteration 9169, loss = 0.77963457\n",
            "Iteration 9170, loss = 0.77959688\n",
            "Iteration 9171, loss = 0.77955756\n",
            "Iteration 9172, loss = 0.77951814\n",
            "Iteration 9173, loss = 0.77947998\n",
            "Iteration 9174, loss = 0.77944297\n",
            "Iteration 9175, loss = 0.77940538\n",
            "Iteration 9176, loss = 0.77936668\n",
            "Iteration 9177, loss = 0.77932735\n",
            "Iteration 9178, loss = 0.77929176\n",
            "Iteration 9179, loss = 0.77925287\n",
            "Iteration 9180, loss = 0.77921298\n",
            "Iteration 9181, loss = 0.77917580\n",
            "Iteration 9182, loss = 0.77914151\n",
            "Iteration 9183, loss = 0.77910082\n",
            "Iteration 9184, loss = 0.77906202\n",
            "Iteration 9185, loss = 0.77902262\n",
            "Iteration 9186, loss = 0.77898394\n",
            "Iteration 9187, loss = 0.77894849\n",
            "Iteration 9188, loss = 0.77891179\n",
            "Iteration 9189, loss = 0.77887435\n",
            "Iteration 9190, loss = 0.77883398\n",
            "Iteration 9191, loss = 0.77879651\n",
            "Iteration 9192, loss = 0.77876006\n",
            "Iteration 9193, loss = 0.77872322\n",
            "Iteration 9194, loss = 0.77868526\n",
            "Iteration 9195, loss = 0.77864729\n",
            "Iteration 9196, loss = 0.77861027\n",
            "Iteration 9197, loss = 0.77857319\n",
            "Iteration 9198, loss = 0.77853641\n",
            "Iteration 9199, loss = 0.77850000\n",
            "Iteration 9200, loss = 0.77846178\n",
            "Iteration 9201, loss = 0.77842328\n",
            "Iteration 9202, loss = 0.77838704\n",
            "Iteration 9203, loss = 0.77834919\n",
            "Iteration 9204, loss = 0.77830969\n",
            "Iteration 9205, loss = 0.77827309\n",
            "Iteration 9206, loss = 0.77823521\n",
            "Iteration 9207, loss = 0.77819601\n",
            "Iteration 9208, loss = 0.77816223\n",
            "Iteration 9209, loss = 0.77812693\n",
            "Iteration 9210, loss = 0.77808795\n",
            "Iteration 9211, loss = 0.77804724\n",
            "Iteration 9212, loss = 0.77801082\n",
            "Iteration 9213, loss = 0.77797414\n",
            "Iteration 9214, loss = 0.77793687\n",
            "Iteration 9215, loss = 0.77790023\n",
            "Iteration 9216, loss = 0.77786301\n",
            "Iteration 9217, loss = 0.77782581\n",
            "Iteration 9218, loss = 0.77778903\n",
            "Iteration 9219, loss = 0.77775755\n",
            "Iteration 9220, loss = 0.77772083\n",
            "Iteration 9221, loss = 0.77768316\n",
            "Iteration 9222, loss = 0.77764625\n",
            "Iteration 9223, loss = 0.77761025\n",
            "Iteration 9224, loss = 0.77757535\n",
            "Iteration 9225, loss = 0.77753993\n",
            "Iteration 9226, loss = 0.77750284\n",
            "Iteration 9227, loss = 0.77746830\n",
            "Iteration 9228, loss = 0.77743426\n",
            "Iteration 9229, loss = 0.77739422\n",
            "Iteration 9230, loss = 0.77735873\n",
            "Iteration 9231, loss = 0.77732502\n",
            "Iteration 9232, loss = 0.77729041\n",
            "Iteration 9233, loss = 0.77725064\n",
            "Iteration 9234, loss = 0.77721613\n",
            "Iteration 9235, loss = 0.77717905\n",
            "Iteration 9236, loss = 0.77714320\n",
            "Iteration 9237, loss = 0.77711058\n",
            "Iteration 9238, loss = 0.77707111\n",
            "Iteration 9239, loss = 0.77703413\n",
            "Iteration 9240, loss = 0.77699765\n",
            "Iteration 9241, loss = 0.77696097\n",
            "Iteration 9242, loss = 0.77692593\n",
            "Iteration 9243, loss = 0.77688889\n",
            "Iteration 9244, loss = 0.77685322\n",
            "Iteration 9245, loss = 0.77682003\n",
            "Iteration 9246, loss = 0.77678424\n",
            "Iteration 9247, loss = 0.77674689\n",
            "Iteration 9248, loss = 0.77671404\n",
            "Iteration 9249, loss = 0.77667766\n",
            "Iteration 9250, loss = 0.77664250\n",
            "Iteration 9251, loss = 0.77660691\n",
            "Iteration 9252, loss = 0.77656974\n",
            "Iteration 9253, loss = 0.77653358\n",
            "Iteration 9254, loss = 0.77649771\n",
            "Iteration 9255, loss = 0.77646714\n",
            "Iteration 9256, loss = 0.77642838\n",
            "Iteration 9257, loss = 0.77639418\n",
            "Iteration 9258, loss = 0.77636022\n",
            "Iteration 9259, loss = 0.77632355\n",
            "Iteration 9260, loss = 0.77628991\n",
            "Iteration 9261, loss = 0.77625505\n",
            "Iteration 9262, loss = 0.77621965\n",
            "Iteration 9263, loss = 0.77618520\n",
            "Iteration 9264, loss = 0.77615044\n",
            "Iteration 9265, loss = 0.77611388\n",
            "Iteration 9266, loss = 0.77607854\n",
            "Iteration 9267, loss = 0.77604307\n",
            "Iteration 9268, loss = 0.77600831\n",
            "Iteration 9269, loss = 0.77597170\n",
            "Iteration 9270, loss = 0.77593684\n",
            "Iteration 9271, loss = 0.77590185\n",
            "Iteration 9272, loss = 0.77586779\n",
            "Iteration 9273, loss = 0.77583115\n",
            "Iteration 9274, loss = 0.77580233\n",
            "Iteration 9275, loss = 0.77576828\n",
            "Iteration 9276, loss = 0.77572656\n",
            "Iteration 9277, loss = 0.77569306\n",
            "Iteration 9278, loss = 0.77565757\n",
            "Iteration 9279, loss = 0.77562314\n",
            "Iteration 9280, loss = 0.77559002\n",
            "Iteration 9281, loss = 0.77555474\n",
            "Iteration 9282, loss = 0.77552044\n",
            "Iteration 9283, loss = 0.77548507\n",
            "Iteration 9284, loss = 0.77544950\n",
            "Iteration 9285, loss = 0.77541574\n",
            "Iteration 9286, loss = 0.77538076\n",
            "Iteration 9287, loss = 0.77534602\n",
            "Iteration 9288, loss = 0.77531258\n",
            "Iteration 9289, loss = 0.77527282\n",
            "Iteration 9290, loss = 0.77523436\n",
            "Iteration 9291, loss = 0.77520069\n",
            "Iteration 9292, loss = 0.77516638\n",
            "Iteration 9293, loss = 0.77512460\n",
            "Iteration 9294, loss = 0.77508851\n",
            "Iteration 9295, loss = 0.77505455\n",
            "Iteration 9296, loss = 0.77501776\n",
            "Iteration 9297, loss = 0.77497672\n",
            "Iteration 9298, loss = 0.77494123\n",
            "Iteration 9299, loss = 0.77490366\n",
            "Iteration 9300, loss = 0.77486294\n",
            "Iteration 9301, loss = 0.77482311\n",
            "Iteration 9302, loss = 0.77478478\n",
            "Iteration 9303, loss = 0.77474376\n",
            "Iteration 9304, loss = 0.77470745\n",
            "Iteration 9305, loss = 0.77466867\n",
            "Iteration 9306, loss = 0.77463192\n",
            "Iteration 9307, loss = 0.77459494\n",
            "Iteration 9308, loss = 0.77455528\n",
            "Iteration 9309, loss = 0.77451905\n",
            "Iteration 9310, loss = 0.77448348\n",
            "Iteration 9311, loss = 0.77443953\n",
            "Iteration 9312, loss = 0.77440221\n",
            "Iteration 9313, loss = 0.77436748\n",
            "Iteration 9314, loss = 0.77432723\n",
            "Iteration 9315, loss = 0.77428848\n",
            "Iteration 9316, loss = 0.77425240\n",
            "Iteration 9317, loss = 0.77421241\n",
            "Iteration 9318, loss = 0.77417614\n",
            "Iteration 9319, loss = 0.77413871\n",
            "Iteration 9320, loss = 0.77410085\n",
            "Iteration 9321, loss = 0.77406226\n",
            "Iteration 9322, loss = 0.77402386\n",
            "Iteration 9323, loss = 0.77398589\n",
            "Iteration 9324, loss = 0.77394822\n",
            "Iteration 9325, loss = 0.77390995\n",
            "Iteration 9326, loss = 0.77387289\n",
            "Iteration 9327, loss = 0.77383476\n",
            "Iteration 9328, loss = 0.77379854\n",
            "Iteration 9329, loss = 0.77376018\n",
            "Iteration 9330, loss = 0.77371749\n",
            "Iteration 9331, loss = 0.77367772\n",
            "Iteration 9332, loss = 0.77364416\n",
            "Iteration 9333, loss = 0.77360599\n",
            "Iteration 9334, loss = 0.77356398\n",
            "Iteration 9335, loss = 0.77353020\n",
            "Iteration 9336, loss = 0.77349328\n",
            "Iteration 9337, loss = 0.77345494\n",
            "Iteration 9338, loss = 0.77341613\n",
            "Iteration 9339, loss = 0.77337762\n",
            "Iteration 9340, loss = 0.77334103\n",
            "Iteration 9341, loss = 0.77330206\n",
            "Iteration 9342, loss = 0.77326356\n",
            "Iteration 9343, loss = 0.77323026\n",
            "Iteration 9344, loss = 0.77319390\n",
            "Iteration 9345, loss = 0.77315630\n",
            "Iteration 9346, loss = 0.77311565\n",
            "Iteration 9347, loss = 0.77307507\n",
            "Iteration 9348, loss = 0.77303571\n",
            "Iteration 9349, loss = 0.77300635\n",
            "Iteration 9350, loss = 0.77296871\n",
            "Iteration 9351, loss = 0.77292559\n",
            "Iteration 9352, loss = 0.77288919\n",
            "Iteration 9353, loss = 0.77285162\n",
            "Iteration 9354, loss = 0.77281296\n",
            "Iteration 9355, loss = 0.77277485\n",
            "Iteration 9356, loss = 0.77273630\n",
            "Iteration 9357, loss = 0.77269973\n",
            "Iteration 9358, loss = 0.77266135\n",
            "Iteration 9359, loss = 0.77262602\n",
            "Iteration 9360, loss = 0.77258717\n",
            "Iteration 9361, loss = 0.77255147\n",
            "Iteration 9362, loss = 0.77251443\n",
            "Iteration 9363, loss = 0.77247720\n",
            "Iteration 9364, loss = 0.77243873\n",
            "Iteration 9365, loss = 0.77240178\n",
            "Iteration 9366, loss = 0.77236373\n",
            "Iteration 9367, loss = 0.77232667\n",
            "Iteration 9368, loss = 0.77228905\n",
            "Iteration 9369, loss = 0.77225171\n",
            "Iteration 9370, loss = 0.77221754\n",
            "Iteration 9371, loss = 0.77217723\n",
            "Iteration 9372, loss = 0.77214179\n",
            "Iteration 9373, loss = 0.77210682\n",
            "Iteration 9374, loss = 0.77206859\n",
            "Iteration 9375, loss = 0.77202938\n",
            "Iteration 9376, loss = 0.77199438\n",
            "Iteration 9377, loss = 0.77195801\n",
            "Iteration 9378, loss = 0.77191807\n",
            "Iteration 9379, loss = 0.77188210\n",
            "Iteration 9380, loss = 0.77184469\n",
            "Iteration 9381, loss = 0.77181284\n",
            "Iteration 9382, loss = 0.77177547\n",
            "Iteration 9383, loss = 0.77173480\n",
            "Iteration 9384, loss = 0.77169851\n",
            "Iteration 9385, loss = 0.77166044\n",
            "Iteration 9386, loss = 0.77162301\n",
            "Iteration 9387, loss = 0.77158539\n",
            "Iteration 9388, loss = 0.77155041\n",
            "Iteration 9389, loss = 0.77151543\n",
            "Iteration 9390, loss = 0.77147704\n",
            "Iteration 9391, loss = 0.77144041\n",
            "Iteration 9392, loss = 0.77140583\n",
            "Iteration 9393, loss = 0.77136730\n",
            "Iteration 9394, loss = 0.77132961\n",
            "Iteration 9395, loss = 0.77129242\n",
            "Iteration 9396, loss = 0.77125859\n",
            "Iteration 9397, loss = 0.77122146\n",
            "Iteration 9398, loss = 0.77118226\n",
            "Iteration 9399, loss = 0.77114569\n",
            "Iteration 9400, loss = 0.77110831\n",
            "Iteration 9401, loss = 0.77107034\n",
            "Iteration 9402, loss = 0.77103528\n",
            "Iteration 9403, loss = 0.77099989\n",
            "Iteration 9404, loss = 0.77096386\n",
            "Iteration 9405, loss = 0.77092548\n",
            "Iteration 9406, loss = 0.77089111\n",
            "Iteration 9407, loss = 0.77085577\n",
            "Iteration 9408, loss = 0.77081789\n",
            "Iteration 9409, loss = 0.77078123\n",
            "Iteration 9410, loss = 0.77074759\n",
            "Iteration 9411, loss = 0.77070993\n",
            "Iteration 9412, loss = 0.77067028\n",
            "Iteration 9413, loss = 0.77063646\n",
            "Iteration 9414, loss = 0.77059677\n",
            "Iteration 9415, loss = 0.77056110\n",
            "Iteration 9416, loss = 0.77052337\n",
            "Iteration 9417, loss = 0.77048990\n",
            "Iteration 9418, loss = 0.77045320\n",
            "Iteration 9419, loss = 0.77041610\n",
            "Iteration 9420, loss = 0.77038017\n",
            "Iteration 9421, loss = 0.77034391\n",
            "Iteration 9422, loss = 0.77030726\n",
            "Iteration 9423, loss = 0.77026923\n",
            "Iteration 9424, loss = 0.77023747\n",
            "Iteration 9425, loss = 0.77020008\n",
            "Iteration 9426, loss = 0.77016252\n",
            "Iteration 9427, loss = 0.77013033\n",
            "Iteration 9428, loss = 0.77009456\n",
            "Iteration 9429, loss = 0.77005505\n",
            "Iteration 9430, loss = 0.77001923\n",
            "Iteration 9431, loss = 0.76999001\n",
            "Iteration 9432, loss = 0.76995222\n",
            "Iteration 9433, loss = 0.76991017\n",
            "Iteration 9434, loss = 0.76987518\n",
            "Iteration 9435, loss = 0.76984080\n",
            "Iteration 9436, loss = 0.76980306\n",
            "Iteration 9437, loss = 0.76976762\n",
            "Iteration 9438, loss = 0.76973181\n",
            "Iteration 9439, loss = 0.76969592\n",
            "Iteration 9440, loss = 0.76965941\n",
            "Iteration 9441, loss = 0.76962450\n",
            "Iteration 9442, loss = 0.76958698\n",
            "Iteration 9443, loss = 0.76954974\n",
            "Iteration 9444, loss = 0.76951549\n",
            "Iteration 9445, loss = 0.76947894\n",
            "Iteration 9446, loss = 0.76944346\n",
            "Iteration 9447, loss = 0.76940764\n",
            "Iteration 9448, loss = 0.76937367\n",
            "Iteration 9449, loss = 0.76933638\n",
            "Iteration 9450, loss = 0.76929855\n",
            "Iteration 9451, loss = 0.76926457\n",
            "Iteration 9452, loss = 0.76922950\n",
            "Iteration 9453, loss = 0.76919380\n",
            "Iteration 9454, loss = 0.76915644\n",
            "Iteration 9455, loss = 0.76912535\n",
            "Iteration 9456, loss = 0.76908653\n",
            "Iteration 9457, loss = 0.76905018\n",
            "Iteration 9458, loss = 0.76901568\n",
            "Iteration 9459, loss = 0.76897964\n",
            "Iteration 9460, loss = 0.76894252\n",
            "Iteration 9461, loss = 0.76890823\n",
            "Iteration 9462, loss = 0.76887099\n",
            "Iteration 9463, loss = 0.76883647\n",
            "Iteration 9464, loss = 0.76880116\n",
            "Iteration 9465, loss = 0.76876596\n",
            "Iteration 9466, loss = 0.76873231\n",
            "Iteration 9467, loss = 0.76869484\n",
            "Iteration 9468, loss = 0.76865896\n",
            "Iteration 9469, loss = 0.76862556\n",
            "Iteration 9470, loss = 0.76858949\n",
            "Iteration 9471, loss = 0.76855334\n",
            "Iteration 9472, loss = 0.76852015\n",
            "Iteration 9473, loss = 0.76848297\n",
            "Iteration 9474, loss = 0.76844888\n",
            "Iteration 9475, loss = 0.76841093\n",
            "Iteration 9476, loss = 0.76838128\n",
            "Iteration 9477, loss = 0.76834639\n",
            "Iteration 9478, loss = 0.76831047\n",
            "Iteration 9479, loss = 0.76827742\n",
            "Iteration 9480, loss = 0.76824224\n",
            "Iteration 9481, loss = 0.76820781\n",
            "Iteration 9482, loss = 0.76817310\n",
            "Iteration 9483, loss = 0.76813644\n",
            "Iteration 9484, loss = 0.76809862\n",
            "Iteration 9485, loss = 0.76806226\n",
            "Iteration 9486, loss = 0.76802836\n",
            "Iteration 9487, loss = 0.76799026\n",
            "Iteration 9488, loss = 0.76795631\n",
            "Iteration 9489, loss = 0.76792176\n",
            "Iteration 9490, loss = 0.76788606\n",
            "Iteration 9491, loss = 0.76785109\n",
            "Iteration 9492, loss = 0.76781570\n",
            "Iteration 9493, loss = 0.76777997\n",
            "Iteration 9494, loss = 0.76774361\n",
            "Iteration 9495, loss = 0.76771102\n",
            "Iteration 9496, loss = 0.76767716\n",
            "Iteration 9497, loss = 0.76764230\n",
            "Iteration 9498, loss = 0.76760479\n",
            "Iteration 9499, loss = 0.76757007\n",
            "Iteration 9500, loss = 0.76753647\n",
            "Iteration 9501, loss = 0.76750077\n",
            "Iteration 9502, loss = 0.76746771\n",
            "Iteration 9503, loss = 0.76743319\n",
            "Iteration 9504, loss = 0.76739639\n",
            "Iteration 9505, loss = 0.76736151\n",
            "Iteration 9506, loss = 0.76732943\n",
            "Iteration 9507, loss = 0.76728897\n",
            "Iteration 9508, loss = 0.76725782\n",
            "Iteration 9509, loss = 0.76722479\n",
            "Iteration 9510, loss = 0.76719073\n",
            "Iteration 9511, loss = 0.76715605\n",
            "Iteration 9512, loss = 0.76712158\n",
            "Iteration 9513, loss = 0.76708505\n",
            "Iteration 9514, loss = 0.76705090\n",
            "Iteration 9515, loss = 0.76701568\n",
            "Iteration 9516, loss = 0.76698109\n",
            "Iteration 9517, loss = 0.76694600\n",
            "Iteration 9518, loss = 0.76690995\n",
            "Iteration 9519, loss = 0.76687786\n",
            "Iteration 9520, loss = 0.76684402\n",
            "Iteration 9521, loss = 0.76680584\n",
            "Iteration 9522, loss = 0.76677154\n",
            "Iteration 9523, loss = 0.76673910\n",
            "Iteration 9524, loss = 0.76670292\n",
            "Iteration 9525, loss = 0.76666638\n",
            "Iteration 9526, loss = 0.76663094\n",
            "Iteration 9527, loss = 0.76659759\n",
            "Iteration 9528, loss = 0.76656535\n",
            "Iteration 9529, loss = 0.76652773\n",
            "Iteration 9530, loss = 0.76649392\n",
            "Iteration 9531, loss = 0.76645819\n",
            "Iteration 9532, loss = 0.76642656\n",
            "Iteration 9533, loss = 0.76639033\n",
            "Iteration 9534, loss = 0.76635900\n",
            "Iteration 9535, loss = 0.76632384\n",
            "Iteration 9536, loss = 0.76629173\n",
            "Iteration 9537, loss = 0.76625787\n",
            "Iteration 9538, loss = 0.76622222\n",
            "Iteration 9539, loss = 0.76619038\n",
            "Iteration 9540, loss = 0.76615139\n",
            "Iteration 9541, loss = 0.76611831\n",
            "Iteration 9542, loss = 0.76608514\n",
            "Iteration 9543, loss = 0.76604990\n",
            "Iteration 9544, loss = 0.76601270\n",
            "Iteration 9545, loss = 0.76598366\n",
            "Iteration 9546, loss = 0.76594559\n",
            "Iteration 9547, loss = 0.76591394\n",
            "Iteration 9548, loss = 0.76588168\n",
            "Iteration 9549, loss = 0.76584525\n",
            "Iteration 9550, loss = 0.76581426\n",
            "Iteration 9551, loss = 0.76578106\n",
            "Iteration 9552, loss = 0.76574720\n",
            "Iteration 9553, loss = 0.76571112\n",
            "Iteration 9554, loss = 0.76567421\n",
            "Iteration 9555, loss = 0.76563958\n",
            "Iteration 9556, loss = 0.76560657\n",
            "Iteration 9557, loss = 0.76556821\n",
            "Iteration 9558, loss = 0.76553886\n",
            "Iteration 9559, loss = 0.76550465\n",
            "Iteration 9560, loss = 0.76547186\n",
            "Iteration 9561, loss = 0.76544012\n",
            "Iteration 9562, loss = 0.76540538\n",
            "Iteration 9563, loss = 0.76536971\n",
            "Iteration 9564, loss = 0.76533666\n",
            "Iteration 9565, loss = 0.76530284\n",
            "Iteration 9566, loss = 0.76526757\n",
            "Iteration 9567, loss = 0.76523524\n",
            "Iteration 9568, loss = 0.76520374\n",
            "Iteration 9569, loss = 0.76516983\n",
            "Iteration 9570, loss = 0.76513368\n",
            "Iteration 9571, loss = 0.76510029\n",
            "Iteration 9572, loss = 0.76506635\n",
            "Iteration 9573, loss = 0.76503060\n",
            "Iteration 9574, loss = 0.76500488\n",
            "Iteration 9575, loss = 0.76496754\n",
            "Iteration 9576, loss = 0.76493219\n",
            "Iteration 9577, loss = 0.76490071\n",
            "Iteration 9578, loss = 0.76487031\n",
            "Iteration 9579, loss = 0.76483892\n",
            "Iteration 9580, loss = 0.76480438\n",
            "Iteration 9581, loss = 0.76476766\n",
            "Iteration 9582, loss = 0.76473028\n",
            "Iteration 9583, loss = 0.76470154\n",
            "Iteration 9584, loss = 0.76466989\n",
            "Iteration 9585, loss = 0.76463260\n",
            "Iteration 9586, loss = 0.76459511\n",
            "Iteration 9587, loss = 0.76456773\n",
            "Iteration 9588, loss = 0.76453222\n",
            "Iteration 9589, loss = 0.76449772\n",
            "Iteration 9590, loss = 0.76446408\n",
            "Iteration 9591, loss = 0.76443113\n",
            "Iteration 9592, loss = 0.76439713\n",
            "Iteration 9593, loss = 0.76436558\n",
            "Iteration 9594, loss = 0.76433183\n",
            "Iteration 9595, loss = 0.76429940\n",
            "Iteration 9596, loss = 0.76426669\n",
            "Iteration 9597, loss = 0.76423320\n",
            "Iteration 9598, loss = 0.76420152\n",
            "Iteration 9599, loss = 0.76416963\n",
            "Iteration 9600, loss = 0.76413382\n",
            "Iteration 9601, loss = 0.76409848\n",
            "Iteration 9602, loss = 0.76406549\n",
            "Iteration 9603, loss = 0.76403429\n",
            "Iteration 9604, loss = 0.76399953\n",
            "Iteration 9605, loss = 0.76396689\n",
            "Iteration 9606, loss = 0.76393446\n",
            "Iteration 9607, loss = 0.76390455\n",
            "Iteration 9608, loss = 0.76386760\n",
            "Iteration 9609, loss = 0.76383494\n",
            "Iteration 9610, loss = 0.76380497\n",
            "Iteration 9611, loss = 0.76377327\n",
            "Iteration 9612, loss = 0.76373566\n",
            "Iteration 9613, loss = 0.76370712\n",
            "Iteration 9614, loss = 0.76367463\n",
            "Iteration 9615, loss = 0.76364139\n",
            "Iteration 9616, loss = 0.76360774\n",
            "Iteration 9617, loss = 0.76357345\n",
            "Iteration 9618, loss = 0.76354106\n",
            "Iteration 9619, loss = 0.76350873\n",
            "Iteration 9620, loss = 0.76347495\n",
            "Iteration 9621, loss = 0.76344561\n",
            "Iteration 9622, loss = 0.76341296\n",
            "Iteration 9623, loss = 0.76337863\n",
            "Iteration 9624, loss = 0.76334518\n",
            "Iteration 9625, loss = 0.76331110\n",
            "Iteration 9626, loss = 0.76328101\n",
            "Iteration 9627, loss = 0.76324877\n",
            "Iteration 9628, loss = 0.76321298\n",
            "Iteration 9629, loss = 0.76318381\n",
            "Iteration 9630, loss = 0.76315164\n",
            "Iteration 9631, loss = 0.76311874\n",
            "Iteration 9632, loss = 0.76308619\n",
            "Iteration 9633, loss = 0.76305384\n",
            "Iteration 9634, loss = 0.76301845\n",
            "Iteration 9635, loss = 0.76298742\n",
            "Iteration 9636, loss = 0.76295844\n",
            "Iteration 9637, loss = 0.76292723\n",
            "Iteration 9638, loss = 0.76289640\n",
            "Iteration 9639, loss = 0.76286174\n",
            "Iteration 9640, loss = 0.76282595\n",
            "Iteration 9641, loss = 0.76279676\n",
            "Iteration 9642, loss = 0.76276396\n",
            "Iteration 9643, loss = 0.76273664\n",
            "Iteration 9644, loss = 0.76269722\n",
            "Iteration 9645, loss = 0.76266690\n",
            "Iteration 9646, loss = 0.76263511\n",
            "Iteration 9647, loss = 0.76260245\n",
            "Iteration 9648, loss = 0.76257093\n",
            "Iteration 9649, loss = 0.76253825\n",
            "Iteration 9650, loss = 0.76250734\n",
            "Iteration 9651, loss = 0.76247666\n",
            "Iteration 9652, loss = 0.76244246\n",
            "Iteration 9653, loss = 0.76241162\n",
            "Iteration 9654, loss = 0.76237900\n",
            "Iteration 9655, loss = 0.76234474\n",
            "Iteration 9656, loss = 0.76231916\n",
            "Iteration 9657, loss = 0.76228500\n",
            "Iteration 9658, loss = 0.76225066\n",
            "Iteration 9659, loss = 0.76221986\n",
            "Iteration 9660, loss = 0.76218762\n",
            "Iteration 9661, loss = 0.76215609\n",
            "Iteration 9662, loss = 0.76212418\n",
            "Iteration 9663, loss = 0.76209209\n",
            "Iteration 9664, loss = 0.76205949\n",
            "Iteration 9665, loss = 0.76202805\n",
            "Iteration 9666, loss = 0.76199960\n",
            "Iteration 9667, loss = 0.76196924\n",
            "Iteration 9668, loss = 0.76194025\n",
            "Iteration 9669, loss = 0.76190584\n",
            "Iteration 9670, loss = 0.76187227\n",
            "Iteration 9671, loss = 0.76183982\n",
            "Iteration 9672, loss = 0.76180486\n",
            "Iteration 9673, loss = 0.76177669\n",
            "Iteration 9674, loss = 0.76174513\n",
            "Iteration 9675, loss = 0.76171449\n",
            "Iteration 9676, loss = 0.76168214\n",
            "Iteration 9677, loss = 0.76164927\n",
            "Iteration 9678, loss = 0.76161776\n",
            "Iteration 9679, loss = 0.76158579\n",
            "Iteration 9680, loss = 0.76155323\n",
            "Iteration 9681, loss = 0.76152180\n",
            "Iteration 9682, loss = 0.76149105\n",
            "Iteration 9683, loss = 0.76146157\n",
            "Iteration 9684, loss = 0.76143416\n",
            "Iteration 9685, loss = 0.76140112\n",
            "Iteration 9686, loss = 0.76137005\n",
            "Iteration 9687, loss = 0.76133766\n",
            "Iteration 9688, loss = 0.76130316\n",
            "Iteration 9689, loss = 0.76127209\n",
            "Iteration 9690, loss = 0.76124143\n",
            "Iteration 9691, loss = 0.76120985\n",
            "Iteration 9692, loss = 0.76117903\n",
            "Iteration 9693, loss = 0.76114901\n",
            "Iteration 9694, loss = 0.76111580\n",
            "Iteration 9695, loss = 0.76108565\n",
            "Iteration 9696, loss = 0.76105624\n",
            "Iteration 9697, loss = 0.76102520\n",
            "Iteration 9698, loss = 0.76099473\n",
            "Iteration 9699, loss = 0.76096277\n",
            "Iteration 9700, loss = 0.76093075\n",
            "Iteration 9701, loss = 0.76090095\n",
            "Iteration 9702, loss = 0.76086787\n",
            "Iteration 9703, loss = 0.76083902\n",
            "Iteration 9704, loss = 0.76080664\n",
            "Iteration 9705, loss = 0.76077934\n",
            "Iteration 9706, loss = 0.76074829\n",
            "Iteration 9707, loss = 0.76071430\n",
            "Iteration 9708, loss = 0.76068713\n",
            "Iteration 9709, loss = 0.76065666\n",
            "Iteration 9710, loss = 0.76062716\n",
            "Iteration 9711, loss = 0.76059510\n",
            "Iteration 9712, loss = 0.76056333\n",
            "Iteration 9713, loss = 0.76053236\n",
            "Iteration 9714, loss = 0.76050140\n",
            "Iteration 9715, loss = 0.76047119\n",
            "Iteration 9716, loss = 0.76043953\n",
            "Iteration 9717, loss = 0.76041090\n",
            "Iteration 9718, loss = 0.76038169\n",
            "Iteration 9719, loss = 0.76034731\n",
            "Iteration 9720, loss = 0.76031548\n",
            "Iteration 9721, loss = 0.76028616\n",
            "Iteration 9722, loss = 0.76026119\n",
            "Iteration 9723, loss = 0.76022992\n",
            "Iteration 9724, loss = 0.76019562\n",
            "Iteration 9725, loss = 0.76016384\n",
            "Iteration 9726, loss = 0.76013068\n",
            "Iteration 9727, loss = 0.76010399\n",
            "Iteration 9728, loss = 0.76007479\n",
            "Iteration 9729, loss = 0.76004070\n",
            "Iteration 9730, loss = 0.76000877\n",
            "Iteration 9731, loss = 0.75998042\n",
            "Iteration 9732, loss = 0.75995042\n",
            "Iteration 9733, loss = 0.75991950\n",
            "Iteration 9734, loss = 0.75988944\n",
            "Iteration 9735, loss = 0.75985965\n",
            "Iteration 9736, loss = 0.75982683\n",
            "Iteration 9737, loss = 0.75979665\n",
            "Iteration 9738, loss = 0.75976894\n",
            "Iteration 9739, loss = 0.75973809\n",
            "Iteration 9740, loss = 0.75970629\n",
            "Iteration 9741, loss = 0.75967756\n",
            "Iteration 9742, loss = 0.75964720\n",
            "Iteration 9743, loss = 0.75961501\n",
            "Iteration 9744, loss = 0.75958649\n",
            "Iteration 9745, loss = 0.75955631\n",
            "Iteration 9746, loss = 0.75952682\n",
            "Iteration 9747, loss = 0.75949626\n",
            "Iteration 9748, loss = 0.75946544\n",
            "Iteration 9749, loss = 0.75943686\n",
            "Iteration 9750, loss = 0.75940259\n",
            "Iteration 9751, loss = 0.75937492\n",
            "Iteration 9752, loss = 0.75934521\n",
            "Iteration 9753, loss = 0.75931848\n",
            "Iteration 9754, loss = 0.75928913\n",
            "Iteration 9755, loss = 0.75925882\n",
            "Iteration 9756, loss = 0.75922678\n",
            "Iteration 9757, loss = 0.75919640\n",
            "Iteration 9758, loss = 0.75916667\n",
            "Iteration 9759, loss = 0.75913735\n",
            "Iteration 9760, loss = 0.75910677\n",
            "Iteration 9761, loss = 0.75907448\n",
            "Iteration 9762, loss = 0.75904299\n",
            "Iteration 9763, loss = 0.75901172\n",
            "Iteration 9764, loss = 0.75898716\n",
            "Iteration 9765, loss = 0.75895655\n",
            "Iteration 9766, loss = 0.75892468\n",
            "Iteration 9767, loss = 0.75889567\n",
            "Iteration 9768, loss = 0.75886831\n",
            "Iteration 9769, loss = 0.75883902\n",
            "Iteration 9770, loss = 0.75880719\n",
            "Iteration 9771, loss = 0.75877751\n",
            "Iteration 9772, loss = 0.75874991\n",
            "Iteration 9773, loss = 0.75871900\n",
            "Iteration 9774, loss = 0.75868614\n",
            "Iteration 9775, loss = 0.75866051\n",
            "Iteration 9776, loss = 0.75862967\n",
            "Iteration 9777, loss = 0.75859471\n",
            "Iteration 9778, loss = 0.75856418\n",
            "Iteration 9779, loss = 0.75853881\n",
            "Iteration 9780, loss = 0.75851032\n",
            "Iteration 9781, loss = 0.75847858\n",
            "Iteration 9782, loss = 0.75845001\n",
            "Iteration 9783, loss = 0.75842051\n",
            "Iteration 9784, loss = 0.75839070\n",
            "Iteration 9785, loss = 0.75836046\n",
            "Iteration 9786, loss = 0.75832979\n",
            "Iteration 9787, loss = 0.75830055\n",
            "Iteration 9788, loss = 0.75826928\n",
            "Iteration 9789, loss = 0.75824309\n",
            "Iteration 9790, loss = 0.75821338\n",
            "Iteration 9791, loss = 0.75818433\n",
            "Iteration 9792, loss = 0.75815343\n",
            "Iteration 9793, loss = 0.75812431\n",
            "Iteration 9794, loss = 0.75809558\n",
            "Iteration 9795, loss = 0.75806722\n",
            "Iteration 9796, loss = 0.75803557\n",
            "Iteration 9797, loss = 0.75800474\n",
            "Iteration 9798, loss = 0.75797518\n",
            "Iteration 9799, loss = 0.75794546\n",
            "Iteration 9800, loss = 0.75791694\n",
            "Iteration 9801, loss = 0.75789086\n",
            "Iteration 9802, loss = 0.75786157\n",
            "Iteration 9803, loss = 0.75782971\n",
            "Iteration 9804, loss = 0.75779984\n",
            "Iteration 9805, loss = 0.75777096\n",
            "Iteration 9806, loss = 0.75774270\n",
            "Iteration 9807, loss = 0.75771337\n",
            "Iteration 9808, loss = 0.75768072\n",
            "Iteration 9809, loss = 0.75765256\n",
            "Iteration 9810, loss = 0.75762438\n",
            "Iteration 9811, loss = 0.75759685\n",
            "Iteration 9812, loss = 0.75756616\n",
            "Iteration 9813, loss = 0.75753653\n",
            "Iteration 9814, loss = 0.75750765\n",
            "Iteration 9815, loss = 0.75747877\n",
            "Iteration 9816, loss = 0.75744969\n",
            "Iteration 9817, loss = 0.75741965\n",
            "Iteration 9818, loss = 0.75739039\n",
            "Iteration 9819, loss = 0.75735964\n",
            "Iteration 9820, loss = 0.75733096\n",
            "Iteration 9821, loss = 0.75730274\n",
            "Iteration 9822, loss = 0.75727195\n",
            "Iteration 9823, loss = 0.75724130\n",
            "Iteration 9824, loss = 0.75721768\n",
            "Iteration 9825, loss = 0.75718782\n",
            "Iteration 9826, loss = 0.75715585\n",
            "Iteration 9827, loss = 0.75712974\n",
            "Iteration 9828, loss = 0.75710301\n",
            "Iteration 9829, loss = 0.75707347\n",
            "Iteration 9830, loss = 0.75704330\n",
            "Iteration 9831, loss = 0.75701196\n",
            "Iteration 9832, loss = 0.75698137\n",
            "Iteration 9833, loss = 0.75695335\n",
            "Iteration 9834, loss = 0.75692286\n",
            "Iteration 9835, loss = 0.75689566\n",
            "Iteration 9836, loss = 0.75686428\n",
            "Iteration 9837, loss = 0.75683512\n",
            "Iteration 9838, loss = 0.75680472\n",
            "Iteration 9839, loss = 0.75678258\n",
            "Iteration 9840, loss = 0.75674872\n",
            "Iteration 9841, loss = 0.75671967\n",
            "Iteration 9842, loss = 0.75669227\n",
            "Iteration 9843, loss = 0.75666603\n",
            "Iteration 9844, loss = 0.75663540\n",
            "Iteration 9845, loss = 0.75660564\n",
            "Iteration 9846, loss = 0.75657730\n",
            "Iteration 9847, loss = 0.75655201\n",
            "Iteration 9848, loss = 0.75652031\n",
            "Iteration 9849, loss = 0.75649141\n",
            "Iteration 9850, loss = 0.75646533\n",
            "Iteration 9851, loss = 0.75643676\n",
            "Iteration 9852, loss = 0.75640690\n",
            "Iteration 9853, loss = 0.75637846\n",
            "Iteration 9854, loss = 0.75634942\n",
            "Iteration 9855, loss = 0.75632187\n",
            "Iteration 9856, loss = 0.75629240\n",
            "Iteration 9857, loss = 0.75626421\n",
            "Iteration 9858, loss = 0.75623474\n",
            "Iteration 9859, loss = 0.75620231\n",
            "Iteration 9860, loss = 0.75617131\n",
            "Iteration 9861, loss = 0.75614436\n",
            "Iteration 9862, loss = 0.75611534\n",
            "Iteration 9863, loss = 0.75608729\n",
            "Iteration 9864, loss = 0.75606073\n",
            "Iteration 9865, loss = 0.75603173\n",
            "Iteration 9866, loss = 0.75600178\n",
            "Iteration 9867, loss = 0.75597400\n",
            "Iteration 9868, loss = 0.75594571\n",
            "Iteration 9869, loss = 0.75591548\n",
            "Iteration 9870, loss = 0.75588836\n",
            "Iteration 9871, loss = 0.75586204\n",
            "Iteration 9872, loss = 0.75583341\n",
            "Iteration 9873, loss = 0.75580360\n",
            "Iteration 9874, loss = 0.75577213\n",
            "Iteration 9875, loss = 0.75575112\n",
            "Iteration 9876, loss = 0.75572132\n",
            "Iteration 9877, loss = 0.75569347\n",
            "Iteration 9878, loss = 0.75566313\n",
            "Iteration 9879, loss = 0.75563577\n",
            "Iteration 9880, loss = 0.75560903\n",
            "Iteration 9881, loss = 0.75558033\n",
            "Iteration 9882, loss = 0.75555140\n",
            "Iteration 9883, loss = 0.75552162\n",
            "Iteration 9884, loss = 0.75549070\n",
            "Iteration 9885, loss = 0.75546581\n",
            "Iteration 9886, loss = 0.75543785\n",
            "Iteration 9887, loss = 0.75541105\n",
            "Iteration 9888, loss = 0.75538054\n",
            "Iteration 9889, loss = 0.75535288\n",
            "Iteration 9890, loss = 0.75532718\n",
            "Iteration 9891, loss = 0.75529961\n",
            "Iteration 9892, loss = 0.75526827\n",
            "Iteration 9893, loss = 0.75523855\n",
            "Iteration 9894, loss = 0.75520897\n",
            "Iteration 9895, loss = 0.75518221\n",
            "Iteration 9896, loss = 0.75515303\n",
            "Iteration 9897, loss = 0.75512495\n",
            "Iteration 9898, loss = 0.75509494\n",
            "Iteration 9899, loss = 0.75506618\n",
            "Iteration 9900, loss = 0.75503810\n",
            "Iteration 9901, loss = 0.75501048\n",
            "Iteration 9902, loss = 0.75498383\n",
            "Iteration 9903, loss = 0.75495505\n",
            "Iteration 9904, loss = 0.75492712\n",
            "Iteration 9905, loss = 0.75490153\n",
            "Iteration 9906, loss = 0.75487366\n",
            "Iteration 9907, loss = 0.75484341\n",
            "Iteration 9908, loss = 0.75481275\n",
            "Iteration 9909, loss = 0.75478519\n",
            "Iteration 9910, loss = 0.75475384\n",
            "Iteration 9911, loss = 0.75472577\n",
            "Iteration 9912, loss = 0.75469930\n",
            "Iteration 9913, loss = 0.75467085\n",
            "Iteration 9914, loss = 0.75464407\n",
            "Iteration 9915, loss = 0.75461691\n",
            "Iteration 9916, loss = 0.75458953\n",
            "Iteration 9917, loss = 0.75456023\n",
            "Iteration 9918, loss = 0.75453319\n",
            "Iteration 9919, loss = 0.75450533\n",
            "Iteration 9920, loss = 0.75447513\n",
            "Iteration 9921, loss = 0.75444842\n",
            "Iteration 9922, loss = 0.75441875\n",
            "Iteration 9923, loss = 0.75439179\n",
            "Iteration 9924, loss = 0.75436309\n",
            "Iteration 9925, loss = 0.75433591\n",
            "Iteration 9926, loss = 0.75430807\n",
            "Iteration 9927, loss = 0.75428492\n",
            "Iteration 9928, loss = 0.75424953\n",
            "Iteration 9929, loss = 0.75422162\n",
            "Iteration 9930, loss = 0.75419462\n",
            "Iteration 9931, loss = 0.75416490\n",
            "Iteration 9932, loss = 0.75413981\n",
            "Iteration 9933, loss = 0.75411131\n",
            "Iteration 9934, loss = 0.75408406\n",
            "Iteration 9935, loss = 0.75405682\n",
            "Iteration 9936, loss = 0.75402854\n",
            "Iteration 9937, loss = 0.75399983\n",
            "Iteration 9938, loss = 0.75396916\n",
            "Iteration 9939, loss = 0.75394428\n",
            "Iteration 9940, loss = 0.75391561\n",
            "Iteration 9941, loss = 0.75388580\n",
            "Iteration 9942, loss = 0.75385926\n",
            "Iteration 9943, loss = 0.75383164\n",
            "Iteration 9944, loss = 0.75380312\n",
            "Iteration 9945, loss = 0.75377387\n",
            "Iteration 9946, loss = 0.75374951\n",
            "Iteration 9947, loss = 0.75371890\n",
            "Iteration 9948, loss = 0.75369293\n",
            "Iteration 9949, loss = 0.75366495\n",
            "Iteration 9950, loss = 0.75363895\n",
            "Iteration 9951, loss = 0.75361142\n",
            "Iteration 9952, loss = 0.75358134\n",
            "Iteration 9953, loss = 0.75355715\n",
            "Iteration 9954, loss = 0.75352492\n",
            "Iteration 9955, loss = 0.75349911\n",
            "Iteration 9956, loss = 0.75347282\n",
            "Iteration 9957, loss = 0.75344600\n",
            "Iteration 9958, loss = 0.75342043\n",
            "Iteration 9959, loss = 0.75339244\n",
            "Iteration 9960, loss = 0.75336192\n",
            "Iteration 9961, loss = 0.75333456\n",
            "Iteration 9962, loss = 0.75330733\n",
            "Iteration 9963, loss = 0.75327955\n",
            "Iteration 9964, loss = 0.75324784\n",
            "Iteration 9965, loss = 0.75322115\n",
            "Iteration 9966, loss = 0.75319507\n",
            "Iteration 9967, loss = 0.75316638\n",
            "Iteration 9968, loss = 0.75313771\n",
            "Iteration 9969, loss = 0.75310755\n",
            "Iteration 9970, loss = 0.75308292\n",
            "Iteration 9971, loss = 0.75305498\n",
            "Iteration 9972, loss = 0.75302767\n",
            "Iteration 9973, loss = 0.75299743\n",
            "Iteration 9974, loss = 0.75297022\n",
            "Iteration 9975, loss = 0.75294634\n",
            "Iteration 9976, loss = 0.75291753\n",
            "Iteration 9977, loss = 0.75288678\n",
            "Iteration 9978, loss = 0.75285748\n",
            "Iteration 9979, loss = 0.75283393\n",
            "Iteration 9980, loss = 0.75280407\n",
            "Iteration 9981, loss = 0.75277619\n",
            "Iteration 9982, loss = 0.75274956\n",
            "Iteration 9983, loss = 0.75272105\n",
            "Iteration 9984, loss = 0.75269732\n",
            "Iteration 9985, loss = 0.75267081\n",
            "Iteration 9986, loss = 0.75264037\n",
            "Iteration 9987, loss = 0.75261363\n",
            "Iteration 9988, loss = 0.75258728\n",
            "Iteration 9989, loss = 0.75255910\n",
            "Iteration 9990, loss = 0.75253162\n",
            "Iteration 9991, loss = 0.75250271\n",
            "Iteration 9992, loss = 0.75247393\n",
            "Iteration 9993, loss = 0.75244589\n",
            "Iteration 9994, loss = 0.75242047\n",
            "Iteration 9995, loss = 0.75239428\n",
            "Iteration 9996, loss = 0.75236878\n",
            "Iteration 9997, loss = 0.75234166\n",
            "Iteration 9998, loss = 0.75231323\n",
            "Iteration 9999, loss = 0.75228425\n",
            "Iteration 10000, loss = 0.75225787\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(2, 40), learning_rate_init=0.0001,\n",
              "              max_iter=10000, tol=1e-06, verbose=True)"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resultado da acurácia\n",
        "redeneural_4.score(X_test,Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0M7y3MIAo4G",
        "outputId": "f893e26f-e551-4679-a531-1970c49a2276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra\n",
        "# Três estados que mais compram\n",
        "\n",
        "df_extra = pd.read_csv('DATASET.csv',sep=';')\n",
        "\n"
      ],
      "metadata": {
        "id": "XObgdk-iAzX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_extra.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWM4gNGYEcn-",
        "outputId": "22f5e042-c511-43c6-c79c-487e3dbe75d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Estado', 'idade', 'qtd_item_compra', 'total_compras', 'tipo_pgto'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# agrupando os estados e somando o total de compras\n",
        "df_estados_mais_compram = df_extra.groupby(['Estado']).agg(qnt_compra=pd.NamedAgg('total_compras','sum')).reset_index()\n"
      ],
      "metadata": {
        "id": "rEbJW47aEYAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_estados_mais_compram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "XiNxICLGFVjb",
        "outputId": "f15de779-bc5d-422c-90d8-da7b215c6b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Estado  qnt_compra\n",
              "0      AC      195.00\n",
              "1      AL       78.00\n",
              "2      AM     1671.00\n",
              "3      BA       48.00\n",
              "4      DF     2124.00\n",
              "5      ES      469.90\n",
              "6      GO     1742.00\n",
              "7      MG     2541.60\n",
              "8      MS     1709.00\n",
              "9      MT       68.00\n",
              "10     PA     2021.36\n",
              "11     PB     1879.00\n",
              "12     PE      630.00\n",
              "13     PI       25.00\n",
              "14     PR      362.00\n",
              "15     RJ     1511.37\n",
              "16     RO       91.00\n",
              "17     RR       80.00\n",
              "18     RS     2054.00\n",
              "19     SC     2567.00\n",
              "20     SE      198.00\n",
              "21     SP     9965.90\n",
              "22     TO      495.00"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7af2b6f-d970-4759-92ea-478b478bd756\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Estado</th>\n",
              "      <th>qnt_compra</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AC</td>\n",
              "      <td>195.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AL</td>\n",
              "      <td>78.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AM</td>\n",
              "      <td>1671.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BA</td>\n",
              "      <td>48.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DF</td>\n",
              "      <td>2124.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ES</td>\n",
              "      <td>469.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GO</td>\n",
              "      <td>1742.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MG</td>\n",
              "      <td>2541.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MS</td>\n",
              "      <td>1709.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>MT</td>\n",
              "      <td>68.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>PA</td>\n",
              "      <td>2021.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>PB</td>\n",
              "      <td>1879.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>PE</td>\n",
              "      <td>630.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>PI</td>\n",
              "      <td>25.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>PR</td>\n",
              "      <td>362.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>RJ</td>\n",
              "      <td>1511.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>RO</td>\n",
              "      <td>91.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>RR</td>\n",
              "      <td>80.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>RS</td>\n",
              "      <td>2054.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>SC</td>\n",
              "      <td>2567.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>SE</td>\n",
              "      <td>198.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>SP</td>\n",
              "      <td>9965.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>TO</td>\n",
              "      <td>495.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7af2b6f-d970-4759-92ea-478b478bd756')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c7af2b6f-d970-4759-92ea-478b478bd756 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c7af2b6f-d970-4759-92ea-478b478bd756');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordenando os 3 com maiores números de compras\n",
        "\n",
        "df_estados_ordenados = df_estados_mais_compram.sort_values(['qnt_compra'], ascending=False)\n",
        "print(df_estados_ordenados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRrAtDVWFHcj",
        "outputId": "d36490b8-d05d-4064-8577-776c2ce72af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Estado  qnt_compra\n",
            "21     SP     9965.90\n",
            "19     SC     2567.00\n",
            "7      MG     2541.60\n",
            "4      DF     2124.00\n",
            "18     RS     2054.00\n",
            "10     PA     2021.36\n",
            "11     PB     1879.00\n",
            "6      GO     1742.00\n",
            "8      MS     1709.00\n",
            "2      AM     1671.00\n",
            "15     RJ     1511.37\n",
            "12     PE      630.00\n",
            "22     TO      495.00\n",
            "5      ES      469.90\n",
            "14     PR      362.00\n",
            "20     SE      198.00\n",
            "0      AC      195.00\n",
            "16     RO       91.00\n",
            "17     RR       80.00\n",
            "1      AL       78.00\n",
            "9      MT       68.00\n",
            "3      BA       48.00\n",
            "13     PI       25.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Três estados que mais compram\n",
        "df_resposta = df_estados_ordenados[['Estado','qnt_compra']].head(3)\n",
        "print(df_resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ-DNXa-E1qS",
        "outputId": "b470952f-43ce-416b-924d-0c71d684ebb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Estado  qnt_compra\n",
            "21     SP      9965.9\n",
            "19     SC      2567.0\n",
            "7      MG      2541.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Método preferido\n",
        "\n",
        "df_metodos_pagamento_preferido = df_extra.groupby(['tipo_pgto']).agg(qnt_tipo_pagamento=pd.NamedAgg('tipo_pgto','count')).reset_index()"
      ],
      "metadata": {
        "id": "25PD4sb6GFoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordenando\n",
        "df_principal_metodo = df_metodos_pagamento_preferido.sort_values(['qnt_tipo_pagamento'], ascending=False)\n",
        "df_principal_metodo.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "ZsqD1fC8HWW8",
        "outputId": "66bbacfb-04ed-46ba-b390-a074f3a9682b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  tipo_pgto  qnt_tipo_pagamento\n",
              "1    CARTÃO                  51"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-784b73b4-3a8e-4dcb-9b07-debb90c56bb0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tipo_pgto</th>\n",
              "      <th>qnt_tipo_pagamento</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CARTÃO</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-784b73b4-3a8e-4dcb-9b07-debb90c56bb0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-784b73b4-3a8e-4dcb-9b07-debb90c56bb0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-784b73b4-3a8e-4dcb-9b07-debb90c56bb0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Média de idade\n",
        "df_extra['idade'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AuDHYydHrkV",
        "outputId": "f62c32b1-1648-473d-e027-90c7e872eec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.73469387755102"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Maior venda e o método de pagamento\n",
        "\n",
        "df_maior_venda = df_extra.sort_values(['total_compras'], ascending=False)\n",
        "print(df_maior_venda[['total_compras','tipo_pgto']].head(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b4t7XAQIIAO",
        "outputId": "0317f4a3-f97b-44bd-855d-b02959103b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    total_compras tipo_pgto\n",
            "46         1800.9    CARTÃO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "K5LLbx0_IvTs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}